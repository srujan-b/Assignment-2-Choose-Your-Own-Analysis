{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srujan-b/Assignment-2-Choose-Your-Own-Analysis/blob/main/merged.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51kyjylRd2GA",
        "outputId": "22b6ba4c-16ee-4544-b522-9ad46f02ed1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBUIgy8np_fu",
        "outputId": "e5b7340b-2e6f-4b83-f4eb-6ff2ea550eae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ColabNotebooks/videos_imra\n"
          ]
        }
      ],
      "source": [
        "cd '/content/drive/MyDrive/ColabNotebooks/videos_imra/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOXVEN8ld3V5"
      },
      "source": [
        "## ConsensusModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OOXlrfMdkjl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class SegmentConsensus(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input_tensor, consensus_type, dim=1):\n",
        "        if consensus_type == 'avg':\n",
        "            output = input_tensor.mean(dim=dim, keepdim=True)\n",
        "        elif consensus_type == 'identity':\n",
        "            output = input_tensor\n",
        "        else:\n",
        "            output = None\n",
        "        ctx.save_for_backward(input_tensor)\n",
        "        ctx.consensus_type = consensus_type\n",
        "        ctx.dim = dim\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input_tensor, = ctx.saved_tensors\n",
        "        consensus_type = ctx.consensus_type\n",
        "        dim = ctx.dim\n",
        "        if consensus_type == 'avg':\n",
        "            grad_in = grad_output.expand(input_tensor.size()) / float(input_tensor.size(dim))\n",
        "        elif consensus_type == 'identity':\n",
        "            grad_in = grad_output\n",
        "        else:\n",
        "            grad_in = None\n",
        "        return grad_in, None, None\n",
        "\n",
        "class ConsensusModule(torch.nn.Module):\n",
        "    def __init__(self, consensus_type, dim=1):\n",
        "        super(ConsensusModule, self).__init__()\n",
        "        self.consensus_type = consensus_type if consensus_type != 'rnn' else 'identity'\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, input):\n",
        "        return SegmentConsensus.apply(input, self.consensus_type, self.dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eDLqwsN0XS6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "yEpftr14XTSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from sys import stderr\n",
        "\n",
        "\n",
        "def log(file, msg):\n",
        "    \"\"\"Log a message.\n",
        "\n",
        "    :param file: File object to which the message will be written.\n",
        "    :param msg:  Message to log (str).\n",
        "    \"\"\"\n",
        "    print(time.strftime(\"[%d.%m.%Y %H:%M:%S]: \"), msg, file=stderr)\n",
        "    file.write(time.strftime(\"[%d.%m.%Y %H:%M:%S]: \") + msg + os.linesep)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n"
      ],
      "metadata": {
        "id": "Zc6vVZRmXVFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_orkXCGd7fS"
      },
      "source": [
        "## Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZiGIh8weCyU"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import random\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import numbers\n",
        "import math\n",
        "import torch\n",
        "\n",
        "\n",
        "class GroupRandomCrop(object):\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        w, h = img_group[0].size\n",
        "        th, tw = self.size\n",
        "\n",
        "        out_images = list()\n",
        "\n",
        "        x1 = random.randint(0, w - tw)\n",
        "        y1 = random.randint(0, h - th)\n",
        "\n",
        "        for img in img_group:\n",
        "            assert(img.size[0] == w and img.size[1] == h)\n",
        "            if w == tw and h == th:\n",
        "                out_images.append(img)\n",
        "            else:\n",
        "                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n",
        "\n",
        "        return out_images\n",
        "\n",
        "\n",
        "class GroupCenterCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.worker = torchvision.transforms.CenterCrop(size)\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        return [self.worker(img) for img in img_group]\n",
        "\n",
        "\n",
        "class GroupRandomHorizontalFlip(object):\n",
        "    \"\"\"Randomly horizontally flips the given PIL.Image with a probability of 0.5\"\"\"\n",
        "    def __init__(self, is_flow=False):\n",
        "        self.is_flow = is_flow\n",
        "\n",
        "    def __call__(self, img_group, is_flow=False):\n",
        "        v = random.random()\n",
        "        if v < 0.5:\n",
        "            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n",
        "            if self.is_flow:\n",
        "                for i in range(0, len(ret), 2):\n",
        "                    ret[i] = ImageOps.invert(ret[i])  # invert flow pixel values when flipping\n",
        "            return ret\n",
        "        else:\n",
        "            return img_group\n",
        "\n",
        "\n",
        "class GroupNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        rep_mean = self.mean * (tensor.size()[0]//len(self.mean))\n",
        "        rep_std = self.std * (tensor.size()[0]//len(self.std))\n",
        "\n",
        "        # TODO: make efficient\n",
        "        for t, m, s in zip(tensor, rep_mean, rep_std):\n",
        "            t.sub_(m).div_(s)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class GroupScale(object):\n",
        "    \"\"\" Rescales the input PIL.Image to the given 'size'.\n",
        "    'size' will be the size of the smaller edge.\n",
        "    For example, if height > width, then image will be\n",
        "    rescaled to (size * height / width, size)\n",
        "    size: size of the smaller edge\n",
        "    interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
        "        self.worker = torchvision.transforms.Resize(size, interpolation)\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        return [self.worker(img) for img in img_group]\n",
        "\n",
        "\n",
        "class GroupOverSample(object):\n",
        "    \"\"\"Optionally scale, then for each of five crop positions (fixed offsets): crop all images and append them to\n",
        "    the resulting list, also append their flipped versions\"\"\"\n",
        "    def __init__(self, crop_size, scale_size=None):\n",
        "        self.crop_size = crop_size if not isinstance(crop_size, int) else (crop_size, crop_size)\n",
        "\n",
        "        if scale_size is not None:\n",
        "            self.scale_worker = GroupScale(scale_size)\n",
        "        else:\n",
        "            self.scale_worker = None\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        if self.scale_worker is not None:\n",
        "            img_group = self.scale_worker(img_group)\n",
        "\n",
        "        image_w, image_h = img_group[0].size\n",
        "        crop_w, crop_h = self.crop_size\n",
        "\n",
        "        offsets = GroupMultiScaleCrop.fill_fix_offset(False, image_w, image_h, crop_w, crop_h)\n",
        "        oversample_group = list()\n",
        "        for o_w, o_h in offsets:\n",
        "            normal_group = list()\n",
        "            flip_group = list()\n",
        "            for i, img in enumerate(img_group):\n",
        "                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n",
        "                normal_group.append(crop)\n",
        "                flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "                if img.mode == 'L' and i % 2 == 0:\n",
        "                    flip_group.append(ImageOps.invert(flip_crop))\n",
        "                else:\n",
        "                    flip_group.append(flip_crop)\n",
        "\n",
        "            oversample_group.extend(normal_group)\n",
        "            oversample_group.extend(flip_group)\n",
        "        return oversample_group\n",
        "\n",
        "\n",
        "class GroupMultiScaleCrop(object):\n",
        "    \"\"\"Crop then resize. Crop size is determined randomly based on scales & max_distort. Crop position is determined\n",
        "    randomly or may be a random one of several fixed choices\"\"\"\n",
        "    def __init__(self, input_size, scales=None, max_distort=1, fix_crop=True, more_fix_crop=True):\n",
        "        self.scales = scales if scales is not None else [1, .875, .75, .66]\n",
        "        self.max_distort = max_distort\n",
        "        self.fix_crop = fix_crop\n",
        "        self.more_fix_crop = more_fix_crop\n",
        "        self.input_size = input_size if not isinstance(input_size, int) else [input_size, input_size]\n",
        "        self.interpolation = Image.BILINEAR\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        im_size = img_group[0].size\n",
        "\n",
        "        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size(im_size)\n",
        "        crop_img_group = [img.crop((offset_w, offset_h, offset_w + crop_w, offset_h + crop_h)) for img in img_group]\n",
        "        ret_img_group = [img.resize((self.input_size[0], self.input_size[1]), self.interpolation)\n",
        "                         for img in crop_img_group]\n",
        "        return ret_img_group\n",
        "\n",
        "    def _sample_crop_size(self, im_size):\n",
        "        image_w, image_h = im_size[0], im_size[1]\n",
        "\n",
        "        # find a crop size\n",
        "        base_size = min(image_w, image_h)\n",
        "        crop_sizes = [int(base_size * x) for x in self.scales]\n",
        "        crop_h = [self.input_size[1] if abs(x - self.input_size[1]) < 3 else x for x in crop_sizes]\n",
        "        crop_w = [self.input_size[0] if abs(x - self.input_size[0]) < 3 else x for x in crop_sizes]\n",
        "\n",
        "        pairs = []\n",
        "        for i, h in enumerate(crop_h):\n",
        "            for j, w in enumerate(crop_w):\n",
        "                if abs(i - j) <= self.max_distort:\n",
        "                    pairs.append((w, h))\n",
        "\n",
        "        crop_pair = random.choice(pairs)\n",
        "        if not self.fix_crop:\n",
        "            w_offset = random.randint(0, image_w - crop_pair[0])\n",
        "            h_offset = random.randint(0, image_h - crop_pair[1])\n",
        "        else:\n",
        "            w_offset, h_offset = self._sample_fix_offset(image_w, image_h, crop_pair[0], crop_pair[1])\n",
        "\n",
        "        return crop_pair[0], crop_pair[1], w_offset, h_offset\n",
        "\n",
        "    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):\n",
        "        offsets = self.fill_fix_offset(self.more_fix_crop, image_w, image_h, crop_w, crop_h)\n",
        "        return random.choice(offsets)\n",
        "\n",
        "    @staticmethod\n",
        "    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):\n",
        "        \"\"\"Choices of cropping an image of the given size (crop_w, crop_h) from the original image\"\"\"\n",
        "        w_step = (image_w - crop_w) // 4\n",
        "        h_step = (image_h - crop_h) // 4\n",
        "\n",
        "        ret = list()\n",
        "        ret.append((0, 0))  # upper left\n",
        "        ret.append((4 * w_step, 0))  # upper right\n",
        "        ret.append((0, 4 * h_step))  # lower left\n",
        "        ret.append((4 * w_step, 4 * h_step))  # lower right\n",
        "        ret.append((2 * w_step, 2 * h_step))  # center\n",
        "\n",
        "        if more_fix_crop:\n",
        "            ret.append((0, 2 * h_step))  # center left\n",
        "            ret.append((4 * w_step, 2 * h_step))  # center right\n",
        "            ret.append((2 * w_step, 4 * h_step))  # lower center\n",
        "            ret.append((2 * w_step, 0 * h_step))  # upper center\n",
        "\n",
        "            ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n",
        "            ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n",
        "            ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n",
        "            ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "class GroupRandomSizedCrop(object):\n",
        "    \"\"\"Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the original size\n",
        "    and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio (then resize)\n",
        "    This is popularly used to train the Inception networks\n",
        "    size: size of the smaller edge\n",
        "    interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        for attempt in range(10):\n",
        "            area = img_group[0].size[0] * img_group[0].size[1]\n",
        "            target_area = random.uniform(0.08, 1.0) * area\n",
        "            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n",
        "\n",
        "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
        "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
        "\n",
        "            if random.random() < 0.5:\n",
        "                w, h = h, w\n",
        "\n",
        "            if w <= img_group[0].size[0] and h <= img_group[0].size[1]:\n",
        "                x1 = random.randint(0, img_group[0].size[0] - w)\n",
        "                y1 = random.randint(0, img_group[0].size[1] - h)\n",
        "                found = True\n",
        "                break\n",
        "        else:\n",
        "            found = False\n",
        "            x1 = 0\n",
        "            y1 = 0\n",
        "\n",
        "        if found:\n",
        "            out_group = list()\n",
        "            for img in img_group:\n",
        "                img = img.crop((x1, y1, x1 + w, y1 + h))\n",
        "                assert(img.size == (w, h))\n",
        "                out_group.append(img.resize((self.size, self.size), self.interpolation))\n",
        "            return out_group\n",
        "        else:\n",
        "            # Fallback\n",
        "            scale = GroupScale(self.size, interpolation=self.interpolation)\n",
        "            crop = GroupRandomCrop(self.size)\n",
        "            return crop(scale(img_group))\n",
        "\n",
        "\n",
        "class Stack(object):\n",
        "\n",
        "    def __init__(self, roll=False):\n",
        "        self.roll = roll\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        if img_group[0].mode == 'L':\n",
        "            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n",
        "        elif img_group[0].mode == 'RGB':\n",
        "            if self.roll:\n",
        "                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n",
        "            else:\n",
        "                return np.concatenate(img_group, axis=2)\n",
        "\n",
        "\n",
        "class ToTorchFormatTensor(object):\n",
        "    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n",
        "    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \"\"\"\n",
        "    def __init__(self, div=True):\n",
        "        self.div = div\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        if isinstance(pic, np.ndarray):\n",
        "            # handle numpy array\n",
        "            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n",
        "        else:\n",
        "            # handle PIL Image\n",
        "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
        "            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n",
        "            # put it from HWC to CHW format\n",
        "            # yikes, this transpose takes 80% of the loading time/CPU\n",
        "            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
        "        return img.float().div(255) if self.div else img.float()\n",
        "\n",
        "\n",
        "class IdentityTransform(object):\n",
        "\n",
        "    def __call__(self, data):\n",
        "        return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-MC5P3_IubC"
      },
      "source": [
        "## Bin Inception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NCErS36JSOX"
      },
      "source": [
        "### layer_factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPLjiPueJVN4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "LAYER_BUILDER_DICT=dict()\n",
        "\n",
        "\n",
        "def parse_expr(expr):\n",
        "    parts = expr.split('<=')\n",
        "    return parts[0].split(','), parts[1], parts[2].split(',')\n",
        "\n",
        "\n",
        "def get_basic_layer(info, channels=None, conv_bias=False):\n",
        "    id = info['id']\n",
        "    attr = info['attrs'] if 'attrs' in info else list()\n",
        "\n",
        "    out, op, in_vars = parse_expr(info['expr'])\n",
        "    assert(len(out) == 1)\n",
        "    assert(len(in_vars) == 1)\n",
        "    mod, out_channel, = LAYER_BUILDER_DICT[op](attr, channels, conv_bias)\n",
        "\n",
        "    return id, out[0], mod, out_channel, in_vars[0]\n",
        "\n",
        "\n",
        "def build_conv(attr, channels=None, conv_bias=False):\n",
        "    out_channels = attr['num_output']\n",
        "    ks = attr['kernel_size'] if 'kernel_size' in attr else (attr['kernel_h'], attr['kernel_w'])\n",
        "    if 'pad' in attr or 'pad_w' in attr and 'pad_h' in attr:\n",
        "        padding = attr['pad'] if 'pad' in attr else (attr['pad_h'], attr['pad_w'])\n",
        "    else:\n",
        "        padding = 0\n",
        "    if 'stride' in attr or 'stride_w' in attr and 'stride_h' in attr:\n",
        "        stride = attr['stride'] if 'stride' in attr else (attr['stride_h'], attr['stride_w'])\n",
        "    else:\n",
        "        stride = 1\n",
        "\n",
        "    conv = nn.Conv2d(channels, out_channels, ks, stride, padding, bias=conv_bias)\n",
        "\n",
        "    return conv, out_channels\n",
        "\n",
        "\n",
        "def build_pooling(attr, channels=None, conv_bias=False):\n",
        "    method = attr['mode']\n",
        "    pad = attr['pad'] if 'pad' in attr else 0\n",
        "    if method == 'max':\n",
        "        pool = nn.MaxPool2d(attr['kernel_size'], attr['stride'], pad,\n",
        "                            ceil_mode=True) # all Caffe pooling use ceil model\n",
        "    elif method == 'ave':\n",
        "        pool = nn.AvgPool2d(attr['kernel_size'], attr['stride'], pad,\n",
        "                            ceil_mode=True)  # all Caffe pooling use ceil model\n",
        "    else:\n",
        "        raise ValueError(\"Unknown pooling method: {}\".format(method))\n",
        "\n",
        "    return pool, channels\n",
        "\n",
        "\n",
        "def build_relu(attr, channels=None, conv_bias=False):\n",
        "    return nn.ReLU(inplace=True), channels\n",
        "\n",
        "\n",
        "def build_bn(attr, channels=None, conv_bias=False):\n",
        "    return nn.BatchNorm2d(channels, momentum=0.1), channels\n",
        "\n",
        "\n",
        "def build_linear(attr, channels=None, conv_bias=False):\n",
        "    return nn.Linear(channels, attr['num_output']), channels\n",
        "\n",
        "\n",
        "def build_dropout(attr, channels=None, conv_bias=False):\n",
        "    return nn.Dropout(p=attr['dropout_ratio']), channels\n",
        "\n",
        "\n",
        "LAYER_BUILDER_DICT['Convolution'] = build_conv\n",
        "\n",
        "LAYER_BUILDER_DICT['Pooling'] = build_pooling\n",
        "\n",
        "LAYER_BUILDER_DICT['ReLU'] = build_relu\n",
        "\n",
        "LAYER_BUILDER_DICT['Dropout'] = build_dropout\n",
        "\n",
        "LAYER_BUILDER_DICT['BN'] = build_bn\n",
        "\n",
        "LAYER_BUILDER_DICT['InnerProduct'] = build_linear\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ8Zn2OMJDE2"
      },
      "source": [
        "### pytorch_load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyjo7NYJja8T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "\n",
        "class BNInception(nn.Module):\n",
        "    def __init__(self, model_path='model_zoo/bninception/bn_inception.yaml', num_classes=101,\n",
        "                       weight_url='https://yjxiong.blob.core.windows.net/models/bn_inception-9f5701afb96c8044.pth'):\n",
        "        super(BNInception, self).__init__()\n",
        "\n",
        "        with open('/content/drive/MyDrive/ColabNotebooks/videos_imra/bninception/inceptionv3.yaml', 'r') as file:\n",
        "          manifest = yaml.full_load(file)\n",
        "\n",
        "        layers = manifest['layers']\n",
        "\n",
        "        self._channel_dict = dict()\n",
        "\n",
        "        self._op_list = list()\n",
        "        for l in layers:\n",
        "            out_var, op, in_var = parse_expr(l['expr'])\n",
        "            if op != 'Concat':\n",
        "                id, out_name, module, out_channel, in_name = get_basic_layer(l,\n",
        "                                                                3 if len(self._channel_dict) == 0 else self._channel_dict[in_var[0]],\n",
        "                                                                             conv_bias=True)\n",
        "\n",
        "                self._channel_dict[out_name] = out_channel\n",
        "                setattr(self, id, module)\n",
        "                self._op_list.append((id, op, out_name, in_name))\n",
        "            else:\n",
        "                self._op_list.append((id, op, out_var[0], in_var))\n",
        "                channel = sum([self._channel_dict[x] for x in in_var])\n",
        "                self._channel_dict[out_var[0]] = channel\n",
        "\n",
        "        if weight_url is not None:\n",
        "            self.load_state_dict(torch.utils.model_zoo.load_url(weight_url))\n",
        "\n",
        "    def forward(self, input):\n",
        "        data_dict = dict()\n",
        "        data_dict[self._op_list[0][-1]] = input\n",
        "\n",
        "        def get_hook(name):\n",
        "\n",
        "            def hook(m, grad_in, grad_out):\n",
        "                print(name, grad_out[0].data.abs().mean())\n",
        "\n",
        "            return hook\n",
        "        for op in self._op_list:\n",
        "            if op[1] != 'Concat' and op[1] != 'InnerProduct':\n",
        "                data_dict[op[2]] = getattr(self, op[0])(data_dict[op[-1]])\n",
        "                # getattr(self, op[0]).register_backward_hook(get_hook(op[0]))\n",
        "            elif op[1] == 'InnerProduct':\n",
        "                x = data_dict[op[-1]]\n",
        "                data_dict[op[2]] = getattr(self, op[0])(x.view(x.size(0), -1))\n",
        "            else:\n",
        "                try:\n",
        "                    data_dict[op[2]] = torch.cat(tuple(data_dict[x] for x in op[-1]), 1)\n",
        "                except:\n",
        "                    for x in op[-1]:\n",
        "                        print(x,data_dict[x].size())\n",
        "                    raise\n",
        "        return data_dict[self._op_list[-1][2]]\n",
        "\n",
        "\n",
        "class InceptionV3(BNInception):\n",
        "    def __init__(self, model_path=None, num_classes=101,\n",
        "                 weight_url='https://yjxiong.blob.core.windows.net/models/inceptionv3-cuhk-0e09b300b493bc74c.pth'):\n",
        "        super(InceptionV3, self).__init__(model_path=model_path, weight_url=weight_url, num_classes=num_classes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4OhDC3RN1OX"
      },
      "source": [
        "### Python I 3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi4bXaxkJj4G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class MaxPool3dSamePadding(nn.MaxPool3d):\n",
        "\n",
        "    def compute_pad(self, dim, s):\n",
        "        if s % self.stride[dim] == 0:\n",
        "            return max(self.kernel_size[dim] - self.stride[dim], 0)\n",
        "        else:\n",
        "            return max(self.kernel_size[dim] - (s % self.stride[dim]), 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute 'same' padding\n",
        "        (batch, channel, t, h, w) = x.size()\n",
        "        #print t,h,w\n",
        "        out_t = np.ceil(float(t) / float(self.stride[0]))\n",
        "        out_h = np.ceil(float(h) / float(self.stride[1]))\n",
        "        out_w = np.ceil(float(w) / float(self.stride[2]))\n",
        "        #print out_t, out_h, out_w\n",
        "        pad_t = self.compute_pad(0, t)\n",
        "        pad_h = self.compute_pad(1, h)\n",
        "        pad_w = self.compute_pad(2, w)\n",
        "        #print pad_t, pad_h, pad_w\n",
        "\n",
        "        pad_t_f = pad_t // 2\n",
        "        pad_t_b = pad_t - pad_t_f\n",
        "        pad_h_f = pad_h // 2\n",
        "        pad_h_b = pad_h - pad_h_f\n",
        "        pad_w_f = pad_w // 2\n",
        "        pad_w_b = pad_w - pad_w_f\n",
        "\n",
        "        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n",
        "        #print x.size()\n",
        "        #print pad\n",
        "        x = F.pad(x, pad)\n",
        "        return super(MaxPool3dSamePadding, self).forward(x)\n",
        "\n",
        "\n",
        "class Unit3D(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels,\n",
        "                 output_channels,\n",
        "                 kernel_shape=(1, 1, 1),\n",
        "                 stride=(1, 1, 1),\n",
        "                 padding=0,\n",
        "                 activation_fn=F.relu,\n",
        "                 use_batch_norm=True,\n",
        "                 use_bias=False,\n",
        "                 name='unit_3d'):\n",
        "\n",
        "        \"\"\"Initializes Unit3D module.\"\"\"\n",
        "        super(Unit3D, self).__init__()\n",
        "\n",
        "        self._output_channels = output_channels\n",
        "        self._kernel_shape = kernel_shape\n",
        "        self._stride = stride\n",
        "        self._use_batch_norm = use_batch_norm\n",
        "        self._activation_fn = activation_fn\n",
        "        self._use_bias = use_bias\n",
        "        self.name = name\n",
        "        self.padding = padding\n",
        "\n",
        "        self.conv3d = nn.Conv3d(in_channels=in_channels,\n",
        "                                out_channels=self._output_channels,\n",
        "                                kernel_size=self._kernel_shape,\n",
        "                                stride=self._stride,\n",
        "                                padding=0, # we always want padding to be 0 here. We will dynamically pad based on input size in forward function\n",
        "                                bias=self._use_bias)\n",
        "\n",
        "        if self._use_batch_norm:\n",
        "            self.bn = nn.BatchNorm3d(self._output_channels, eps=0.001, momentum=0.01)\n",
        "\n",
        "    def compute_pad(self, dim, s):\n",
        "        if s % self._stride[dim] == 0:\n",
        "            return max(self._kernel_shape[dim] - self._stride[dim], 0)\n",
        "        else:\n",
        "            return max(self._kernel_shape[dim] - (s % self._stride[dim]), 0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute 'same' padding\n",
        "        (batch, channel, t, h, w) = x.size()\n",
        "        #print t,h,w\n",
        "        out_t = np.ceil(float(t) / float(self._stride[0]))\n",
        "        out_h = np.ceil(float(h) / float(self._stride[1]))\n",
        "        out_w = np.ceil(float(w) / float(self._stride[2]))\n",
        "        #print out_t, out_h, out_w\n",
        "        pad_t = self.compute_pad(0, t)\n",
        "        pad_h = self.compute_pad(1, h)\n",
        "        pad_w = self.compute_pad(2, w)\n",
        "        #print pad_t, pad_h, pad_w\n",
        "\n",
        "        pad_t_f = pad_t // 2\n",
        "        pad_t_b = pad_t - pad_t_f\n",
        "        pad_h_f = pad_h // 2\n",
        "        pad_h_b = pad_h - pad_h_f\n",
        "        pad_w_f = pad_w // 2\n",
        "        pad_w_b = pad_w - pad_w_f\n",
        "\n",
        "        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n",
        "        #print x.size()\n",
        "        #print pad\n",
        "        x = F.pad(x, pad)\n",
        "        #print x.size()\n",
        "\n",
        "        x = self.conv3d(x)\n",
        "        if self._use_batch_norm:\n",
        "            x = self.bn(x)\n",
        "        if self._activation_fn is not None:\n",
        "            x = self._activation_fn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, name):\n",
        "        super(InceptionModule, self).__init__()\n",
        "\n",
        "        self.b0 = Unit3D(in_channels=in_channels, output_channels=out_channels[0], kernel_shape=[1, 1, 1], padding=0,\n",
        "                         name=name+'/Branch_0/Conv3d_0a_1x1')\n",
        "        self.b1a = Unit3D(in_channels=in_channels, output_channels=out_channels[1], kernel_shape=[1, 1, 1], padding=0,\n",
        "                          name=name+'/Branch_1/Conv3d_0a_1x1')\n",
        "        self.b1b = Unit3D(in_channels=out_channels[1], output_channels=out_channels[2], kernel_shape=[3, 3, 3],\n",
        "                          name=name+'/Branch_1/Conv3d_0b_3x3')\n",
        "        self.b2a = Unit3D(in_channels=in_channels, output_channels=out_channels[3], kernel_shape=[1, 1, 1], padding=0,\n",
        "                          name=name+'/Branch_2/Conv3d_0a_1x1')\n",
        "        self.b2b = Unit3D(in_channels=out_channels[3], output_channels=out_channels[4], kernel_shape=[3, 3, 3],\n",
        "                          name=name+'/Branch_2/Conv3d_0b_3x3')\n",
        "        self.b3a = MaxPool3dSamePadding(kernel_size=[3, 3, 3],\n",
        "                                stride=(1, 1, 1), padding=0)\n",
        "        self.b3b = Unit3D(in_channels=in_channels, output_channels=out_channels[5], kernel_shape=[1, 1, 1], padding=0,\n",
        "                          name=name+'/Branch_3/Conv3d_0b_1x1')\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, x):\n",
        "        b0 = self.b0(x)\n",
        "        b1 = self.b1b(self.b1a(x))\n",
        "        b2 = self.b2b(self.b2a(x))\n",
        "        b3 = self.b3b(self.b3a(x))\n",
        "        return torch.cat([b0,b1,b2,b3], dim=1)\n",
        "\n",
        "\n",
        "class InceptionI3d(nn.Module):\n",
        "    \"\"\"Inception-v1 I3D architecture.\n",
        "    The model is introduced in:\n",
        "        Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\n",
        "        Joao Carreira, Andrew Zisserman\n",
        "        https://arxiv.org/pdf/1705.07750v1.pdf.\n",
        "    See also the Inception architecture, introduced in:\n",
        "        Going deeper with convolutions\n",
        "        Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n",
        "        Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n",
        "        http://arxiv.org/pdf/1409.4842v1.pdf.\n",
        "    \"\"\"\n",
        "\n",
        "    # Endpoints of the model in order. During construction, all the endpoints up\n",
        "    # to a designated `final_endpoint` are returned in a dictionary as the\n",
        "    # second return value.\n",
        "    VALID_ENDPOINTS = (\n",
        "        'Conv3d_1a_7x7',\n",
        "        'MaxPool3d_2a_3x3',\n",
        "        'Conv3d_2b_1x1',\n",
        "        'Conv3d_2c_3x3',\n",
        "        'MaxPool3d_3a_3x3',\n",
        "        'Mixed_3b',\n",
        "        'Mixed_3c',\n",
        "        'MaxPool3d_4a_3x3',\n",
        "        'Mixed_4b',\n",
        "        'Mixed_4c',\n",
        "        'Mixed_4d',\n",
        "        'Mixed_4e',\n",
        "        'Mixed_4f',\n",
        "        'MaxPool3d_5a_2x2',\n",
        "        'Mixed_5b',\n",
        "        'Mixed_5c',\n",
        "        'Logits',\n",
        "        'Predictions',\n",
        "    )\n",
        "\n",
        "    def __init__(self, num_classes=400, spatial_squeeze=True,\n",
        "                 final_endpoint='Logits', name='inception_i3d', in_channels=3, dropout_keep_prob=0.5):\n",
        "        \"\"\"Initializes I3D model instance.\n",
        "        Args:\n",
        "          num_classes: The number of outputs in the logit layer (default 400, which\n",
        "              matches the Kinetics dataset).\n",
        "          spatial_squeeze: Whether to squeeze the spatial dimensions for the logits\n",
        "              before returning (default True).\n",
        "          final_endpoint: The model contains many possible endpoints.\n",
        "              `final_endpoint` specifies the last endpoint for the model to be built\n",
        "              up to. In addition to the output at `final_endpoint`, all the outputs\n",
        "              at endpoints up to `final_endpoint` will also be returned, in a\n",
        "              dictionary. `final_endpoint` must be one of\n",
        "              InceptionI3d.VALID_ENDPOINTS (default 'Logits').\n",
        "          name: A string (optional). The name of this module.\n",
        "        Raises:\n",
        "          ValueError: if `final_endpoint` is not recognized.\n",
        "        \"\"\"\n",
        "\n",
        "        if final_endpoint not in self.VALID_ENDPOINTS:\n",
        "            raise ValueError('Unknown final endpoint %s' % final_endpoint)\n",
        "\n",
        "        super(InceptionI3d, self).__init__()\n",
        "        self._num_classes = num_classes\n",
        "        self._spatial_squeeze = spatial_squeeze\n",
        "        self._final_endpoint = final_endpoint\n",
        "        self.logits = None\n",
        "\n",
        "        if self._final_endpoint not in self.VALID_ENDPOINTS:\n",
        "            raise ValueError('Unknown final endpoint %s' % self._final_endpoint)\n",
        "\n",
        "        self.end_points = {}\n",
        "        end_point = 'Conv3d_1a_7x7'\n",
        "        self.end_points[end_point] = Unit3D(in_channels=in_channels, output_channels=64, kernel_shape=[7, 7, 7],\n",
        "                                            stride=(2, 2, 2), padding=(3,3,3),  name=name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_2a_3x3'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Conv3d_2b_1x1'\n",
        "        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=64, kernel_shape=[1, 1, 1], padding=0,\n",
        "                                       name=name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Conv3d_2c_3x3'\n",
        "        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=192, kernel_shape=[3, 3, 3], padding=1,\n",
        "                                       name=name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_3a_3x3'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_3b'\n",
        "        self.end_points[end_point] = InceptionModule(192, [64,96,128,16,32,32], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_3c'\n",
        "        self.end_points[end_point] = InceptionModule(256, [128,128,192,32,96,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_4a_3x3'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(2, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4b'\n",
        "        self.end_points[end_point] = InceptionModule(128+192+96+64, [192,96,208,16,48,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4c'\n",
        "        self.end_points[end_point] = InceptionModule(192+208+48+64, [160,112,224,24,64,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4d'\n",
        "        self.end_points[end_point] = InceptionModule(160+224+64+64, [128,128,256,24,64,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4e'\n",
        "        self.end_points[end_point] = InceptionModule(128+256+64+64, [112,144,288,32,64,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4f'\n",
        "        self.end_points[end_point] = InceptionModule(112+288+64+64, [256,160,320,32,128,128], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_5a_2x2'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(2, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_5b'\n",
        "        self.end_points[end_point] = InceptionModule(256+320+128+128, [256,160,320,32,128,128], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_5c'\n",
        "        self.end_points[end_point] = InceptionModule(256+320+128+128, [384,192,384,48,128,128], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Logits'\n",
        "        self.avg_pool = nn.AvgPool3d(kernel_size=[2, 7, 7],\n",
        "                                     stride=(1, 1, 1))\n",
        "        self.dropout = nn.Dropout(dropout_keep_prob)\n",
        "        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n",
        "                             kernel_shape=[1, 1, 1],\n",
        "                             padding=0,\n",
        "                             activation_fn=None,\n",
        "                             use_batch_norm=False,\n",
        "                             use_bias=True,\n",
        "                             name='logits')\n",
        "\n",
        "        self.build()\n",
        "\n",
        "\n",
        "    def replace_logits(self, num_classes):\n",
        "        self._num_classes = num_classes\n",
        "        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n",
        "                             kernel_shape=[1, 1, 1],\n",
        "                             padding=0,\n",
        "                             activation_fn=None,\n",
        "                             use_batch_norm=False,\n",
        "                             use_bias=True,\n",
        "                             name='logits')\n",
        "\n",
        "    def set_dropout(self, dropout):\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def build(self):\n",
        "        for k in self.end_points.keys():\n",
        "            self.add_module(k, self.end_points[k])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for end_point in self.VALID_ENDPOINTS:\n",
        "            if end_point in self.end_points:\n",
        "                x = self._modules[end_point](x) # use _modules to work with dataparallel\n",
        "\n",
        "        x = self.logits(self.dropout(self.avg_pool(x)))\n",
        "        if self._spatial_squeeze:\n",
        "            logits = x.squeeze(3).squeeze(3)  # tensor B x C x T\n",
        "\n",
        "        # avgpooling along temporal dimension\n",
        "        logits = torch.mean(logits, 2)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        for end_point in self.VALID_ENDPOINTS:\n",
        "            if end_point in self.end_points:\n",
        "                x = self._modules[end_point](x)\n",
        "        return self.avg_pool(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPaF2gv8JHnb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5XnBJLNfq_c"
      },
      "source": [
        "## Model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGM0UMdFftJq"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn.init import normal, constant\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "class TSN(nn.Module):\n",
        "    def __init__(self, num_class, num_segments, modality,\n",
        "                 base_model='resnet101', new_length=None,\n",
        "                 consensus_type='avg', before_softmax=True,\n",
        "                 dropout=0.8, partial_bn=True, use_three_input_channels=False, pretrained_model=None):\n",
        "        super(TSN, self).__init__()\n",
        "        self.arch = base_model\n",
        "        self.modality = modality\n",
        "        self.num_segments = num_segments\n",
        "        self.reshape = True\n",
        "        self.before_softmax = before_softmax\n",
        "        self.dropout = dropout\n",
        "        self.consensus_type = consensus_type\n",
        "        if not before_softmax and consensus_type != 'avg':\n",
        "            raise ValueError(\"Only avg consensus can be used after Softmax\")\n",
        "\n",
        "        if new_length is None:\n",
        "            self.new_length = 1 if modality == \"RGB\" else 5\n",
        "        else:\n",
        "            self.new_length = new_length  # number of consecutive frames contained in a snippet\n",
        "\n",
        "        self.use_three_input_channels = use_three_input_channels\n",
        "\n",
        "        print((\"\"\"Initializing TSN with base model: {}.\n",
        "                TSN Configurations:\n",
        "                input_modality:     {}\n",
        "                num_segments:       {}\n",
        "                new_length:         {}\n",
        "                consensus_module:   {}\n",
        "                dropout_ratio:      {}\n",
        "        \"\"\".format(base_model, self.modality, self.num_segments, self.new_length, consensus_type, self.dropout)))\n",
        "\n",
        "        self._prepare_base_model(base_model, pretrained_model)\n",
        "\n",
        "        if not self.is_3D_architecture:\n",
        "            if base_model != 'Pretrained-Inception-v3':\n",
        "                self._prepare_tsn(num_class)\n",
        "\n",
        "                if self.modality == 'Flow':\n",
        "                    print(\"Converting the ImageNet model to a flow init model\")\n",
        "                    self.base_model = self._construct_flow_model(self.base_model)\n",
        "                    print(\"Done. Flow model ready...\")\n",
        "                elif self.modality == 'RGBDiff':\n",
        "                    print(\"Converting the ImageNet model to RGB+Diff init model\")\n",
        "                    self.base_model = self._construct_diff_model(self.base_model)\n",
        "                    print(\"Done. RGBDiff model ready.\")\n",
        "            else:\n",
        "                if self.modality == 'Flow':\n",
        "                    print(\"Converting the ImageNet model to a flow init model\")\n",
        "                    self.base_model = self._construct_flow_model(self.base_model)\n",
        "                    print(\"Done. Flow model ready...\")\n",
        "                elif self.modality == 'RGBDiff':\n",
        "                    print(\"Converting the ImageNet model to RGB+Diff init model\")\n",
        "                    self.base_model = self._construct_diff_model(self.base_model)\n",
        "                    print(\"Done. RGBDiff model ready.\")\n",
        "\n",
        "                if pretrained_model is not None:\n",
        "                    print('loading pretrained model weights from {}'.format(pretrained_model))\n",
        "                    state_dict = torch.load(pretrained_model)\n",
        "                    for k, v in state_dict.items():\n",
        "                        state_dict[k] = torch.squeeze(v, dim=0)\n",
        "                    self.base_model.load_state_dict(state_dict)\n",
        "                self._prepare_tsn(num_class)\n",
        "        else:\n",
        "            self._prepare_tsn(num_class)\n",
        "\n",
        "        self.consensus = ConsensusModule(consensus_type)\n",
        "\n",
        "        if not self.before_softmax:\n",
        "            self.softmax = nn.Softmax()\n",
        "\n",
        "        self._enable_pbn = partial_bn\n",
        "        if partial_bn:\n",
        "            self.partialBN(True)\n",
        "\n",
        "    def _prepare_tsn(self, num_class):\n",
        "        if self.arch == 'Inception3D':\n",
        "            self.base_model.set_dropout(self.dropout)\n",
        "            self.base_model.replace_logits(num_class)\n",
        "            self.new_fc = None\n",
        "        else:\n",
        "            if self.arch == 'Pretrained-Inception-v3':\n",
        "\n",
        "                setattr(self.base_model, 'top_cls_drop', nn.Dropout(p=self.dropout))\n",
        "                feature_dim = getattr(self.base_model, self.base_model.last_layer_name).in_features\n",
        "                setattr(self.base_model, self.base_model.last_layer_name, nn.Linear(feature_dim, num_class))\n",
        "                self.new_fc = None\n",
        "            elif self.arch == 'alexnet':\n",
        "                feature_dim = self.base_model.classifier_layers[self.base_model.last_fc_key].in_features\n",
        "                self.base_model.classifier_layers[self.base_model.last_fc_key] = nn.Dropout(p=self.dropout)\n",
        "                self.new_fc = nn.Linear(feature_dim, num_class)\n",
        "            else:\n",
        "                feature_dim = getattr(self.base_model, self.base_model.last_layer_name).in_features\n",
        "                if self.dropout == 0:\n",
        "                    setattr(self.base_model, self.base_model.last_layer_name, nn.Linear(feature_dim, num_class))\n",
        "                    self.new_fc = None\n",
        "                else:\n",
        "                    setattr(self.base_model, self.base_model.last_layer_name, nn.Dropout(p=self.dropout))\n",
        "                    self.new_fc = nn.Linear(feature_dim, num_class)\n",
        "\n",
        "            std = 0.001\n",
        "            if self.new_fc is None:\n",
        "                normal(getattr(self.base_model, self.base_model.last_layer_name).weight, 0, std)\n",
        "                constant(getattr(self.base_model, self.base_model.last_layer_name).bias, 0)\n",
        "            else:\n",
        "                normal(self.new_fc.weight, 0, std)\n",
        "                constant(self.new_fc.bias, 0)\n",
        "\n",
        "    def _prepare_base_model(self, base_model, pretrained_model=None):\n",
        "        if base_model == 'Inception3D':\n",
        "            if self.modality == 'RGB' or self.use_three_input_channels:\n",
        "                self.base_model = InceptionI3d(num_classes=400, in_channels=3,\n",
        "                                               dropout_keep_prob=self.dropout)\n",
        "            else:\n",
        "                assert (self.modality == 'Flow')\n",
        "                self.base_model = InceptionI3d(num_classes=400, in_channels=2,\n",
        "                                               dropout_keep_prob=self.dropout)\n",
        "\n",
        "            if pretrained_model is not None:\n",
        "                print('loading pretrained model weights from {}'.format(pretrained_model))\n",
        "                state_dict = torch.load(pretrained_model)\n",
        "                self.base_model.load_state_dict(state_dict)\n",
        "\n",
        "            self.input_size = 224\n",
        "            self.input_mean = [0.485, 0.456, 0.406]\n",
        "            self.input_std = [0.229, 0.224, 0.225]\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [0.5]\n",
        "                self.input_std = [np.mean(self.input_std)]\n",
        "        elif base_model == 'Pretrained-Inception-v3':\n",
        "          model_path = '/content/drive/MyDrive/ColabNotebooks/videos_imra/bninception/inceptionv3.yaml'\n",
        "          if os.path.isfile(model_path):\n",
        "            print('Model Path:', model_path)\n",
        "            self.base_model = InceptionV3(model_path=model_path, weight_url=None)\n",
        "          else:\n",
        "            print('Model file does not exist:', model_path)\n",
        "\n",
        "            self.base_model.last_layer_name = 'fc_action'\n",
        "            self.input_size = 299\n",
        "            self.input_mean = [0.5]\n",
        "            self.input_std = [0.5]\n",
        "        elif base_model == '3D-Resnet-34':\n",
        "            import resnet\n",
        "            shortcut_type = 'A'\n",
        "            sample_size = 112\n",
        "            sample_duration = 16\n",
        "\n",
        "            self.base_model = resnet.resnet34(\n",
        "                num_classes=400,\n",
        "                shortcut_type=shortcut_type,\n",
        "                sample_size=sample_size,\n",
        "                sample_duration=sample_duration)\n",
        "            self.base_model.last_layer_name = 'fc'\n",
        "            self.input_size = train_opts.sample_size\n",
        "            self.input_mean = [0.485, 0.456, 0.406]\n",
        "            self.input_std = [0.229, 0.224, 0.225]\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [0.5]\n",
        "                self.input_std = [np.mean(self.input_std)]\n",
        "\n",
        "            if pretrained_model is not None:\n",
        "                print('loading pretrained model weights from {}'.format(pretrained_model))\n",
        "                pretrain = torch.load(pretrained_model)\n",
        "                assert pretrain['arch'] == \"resnet-34\"\n",
        "                base_dict = {'.'.join(k.split('.')[1:]): v for k, v in list(pretrain['state_dict'].items())}\n",
        "                self.base_model.load_state_dict(base_dict)\n",
        "        elif base_model == \"alexnet\":\n",
        "            self.base_model = getattr(torchvision.models, base_model)(True)\n",
        "            self.base_model.last_layer_name = None\n",
        "            self.base_model.classifier_layers = getattr(getattr(self.base_model, '_modules')['classifier'], '_modules')\n",
        "            self.base_model.last_fc_key = '6'\n",
        "\n",
        "            self.input_size = 224\n",
        "            self.input_mean = [0.485, 0.456, 0.406]\n",
        "            self.input_std = [0.229, 0.224, 0.225]\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [0.5]\n",
        "                self.input_std = [np.mean(self.input_std)]\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                self.input_mean = [0.485, 0.456, 0.406] + [0] * 3 * self.new_length\n",
        "                self.input_std = self.input_std + [np.mean(self.input_std) * 2] * 3 * self.new_length\n",
        "        elif 'resnet' in base_model or 'vgg' in base_model:\n",
        "            self.base_model = getattr(torchvision.models, base_model)(True)\n",
        "            self.base_model.last_layer_name = 'fc'\n",
        "            self.input_size = 224\n",
        "            self.input_mean = [0.485, 0.456, 0.406]\n",
        "            self.input_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [0.5]\n",
        "                self.input_std = [np.mean(self.input_std)]\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                self.input_mean = [0.485, 0.456, 0.406] + [0] * 3 * self.new_length\n",
        "                self.input_std = self.input_std + [np.mean(self.input_std) * 2] * 3 * self.new_length\n",
        "        elif base_model == 'BNInception':\n",
        "            import tf_model_zoo  # clone tf_model_zoo repository for this to work!\n",
        "                                 #  (see original repository at https://github.com/yjxiong/tsn-pytorch)\n",
        "            self.base_model = getattr(tf_model_zoo, base_model)()\n",
        "            self.base_model.last_layer_name = 'fc'\n",
        "            self.input_size = 224\n",
        "            self.input_mean = [104, 117, 128]\n",
        "            self.input_std = [1]\n",
        "\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [128]\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                self.input_mean = self.input_mean * (1 + self.new_length)\n",
        "\n",
        "        elif 'inception' in base_model:\n",
        "            print(base_model)\n",
        "            import tf_model_zoo\n",
        "            self.base_model = getattr(tf_model_zoo, base_model)()\n",
        "            self.base_model.last_layer_name = 'classif'\n",
        "            self.input_size = 299\n",
        "            self.input_mean = [0.5]\n",
        "            self.input_std = [0.5]\n",
        "        else:\n",
        "            raise ValueError('Unknown base model: {}'.format(base_model))\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        \"\"\"\n",
        "        Override the default train() to freeze the BN parameters\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        super(TSN, self).train(mode)\n",
        "        count = 0\n",
        "        if self._enable_pbn:\n",
        "            # print(\"Freezing BatchNorm2D except the first one.\")\n",
        "            for m in self.base_model.modules():\n",
        "                if isinstance(m, nn.BatchNorm2d):\n",
        "                    count += 1\n",
        "                    if count >= (2 if self._enable_pbn else 1):\n",
        "                        m.eval()\n",
        "\n",
        "                        # shutdown update in frozen mode\n",
        "                        m.weight.requires_grad = False\n",
        "                        m.bias.requires_grad = False\n",
        "\n",
        "    def partialBN(self, enable):\n",
        "        self._enable_pbn = enable\n",
        "\n",
        "    def get_optim_policies(self):\n",
        "        first_conv_weight = []\n",
        "        first_conv_bias = []\n",
        "        normal_weight = []\n",
        "        normal_bias = []\n",
        "        bn = []\n",
        "\n",
        "        conv_cnt = 0\n",
        "        bn_cnt = 0\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Conv1d):\n",
        "                ps = list(m.parameters())\n",
        "                conv_cnt += 1\n",
        "                if conv_cnt == 1:\n",
        "                    first_conv_weight.append(ps[0])\n",
        "                    if len(ps) == 2:\n",
        "                        first_conv_bias.append(ps[1])\n",
        "                else:\n",
        "                    normal_weight.append(ps[0])\n",
        "                    if len(ps) == 2:\n",
        "                        normal_bias.append(ps[1])\n",
        "            elif isinstance(m, torch.nn.Linear):\n",
        "                ps = list(m.parameters())\n",
        "                normal_weight.append(ps[0])\n",
        "                if len(ps) == 2:\n",
        "                    normal_bias.append(ps[1])\n",
        "\n",
        "            elif isinstance(m, torch.nn.BatchNorm1d):\n",
        "                bn.extend(list(m.parameters()))\n",
        "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
        "                bn_cnt += 1\n",
        "                # later BN's are frozen\n",
        "                if not self._enable_pbn or bn_cnt == 1:\n",
        "                    bn.extend(list(m.parameters()))\n",
        "            elif len(m._modules) == 0:\n",
        "                if len(list(m.parameters())) > 0:\n",
        "                    raise ValueError(\"New atomic module type: {}. Need to give it a learning policy\".format(type(m)))\n",
        "\n",
        "        return [\n",
        "            {'params': first_conv_weight, 'lr_mult': 5 if self.modality == 'Flow' else 1, 'decay_mult': 1,\n",
        "             'name': \"first_conv_weight\"},\n",
        "            {'params': first_conv_bias, 'lr_mult': 10 if self.modality == 'Flow' else 2, 'decay_mult': 0,\n",
        "             'name': \"first_conv_bias\"},\n",
        "            {'params': normal_weight, 'lr_mult': 1, 'decay_mult': 1,\n",
        "             'name': \"normal_weight\"},\n",
        "            {'params': normal_bias, 'lr_mult': 2, 'decay_mult': 0,\n",
        "             'name': \"normal_bias\"},\n",
        "            {'params': bn, 'lr_mult': 1, 'decay_mult': 0,\n",
        "             'name': \"BN scale/shift\"},\n",
        "        ]\n",
        "\n",
        "    def forward(self, input):\n",
        "        sample_len = (3 if self.modality == \"RGB\" else 2) * self.new_length\n",
        "\n",
        "        if self.modality == 'RGBDiff':\n",
        "            sample_len = 3 * self.new_length\n",
        "            input = self._get_diff(input)\n",
        "\n",
        "        if self.is_3D_architecture:\n",
        "            input = input.view((-1,) + input.size()[-4:])\n",
        "        else:\n",
        "            input = input.view((-1, sample_len) + input.size()[-2:])\n",
        "\n",
        "        base_out = self.base_model(input)\n",
        "\n",
        "        if self.new_fc is not None:\n",
        "            base_out = self.new_fc(base_out)\n",
        "\n",
        "        if not self.before_softmax:\n",
        "            base_out = self.softmax(base_out)\n",
        "        if self.reshape:\n",
        "            base_out = base_out.view((-1, self.num_segments) + base_out.size()[1:])\n",
        "\n",
        "        output = self.consensus(base_out)\n",
        "        return output.squeeze(1)\n",
        "\n",
        "    def _get_diff(self, input, keep_rgb=False):\n",
        "        input_c = 3 if self.modality in [\"RGB\", \"RGBDiff\"] else 2\n",
        "        input_view = input.view((-1, self.num_segments, self.new_length + 1, input_c,) + input.size()[2:])\n",
        "        if keep_rgb:\n",
        "            new_data = input_view.clone()\n",
        "        else:\n",
        "            new_data = input_view[:, :, 1:, :, :, :].clone()\n",
        "\n",
        "        for x in reversed(list(range(1, self.new_length + 1))):\n",
        "            if keep_rgb:\n",
        "                new_data[:, :, x, :, :, :] = input_view[:, :, x, :, :, :] - input_view[:, :, x - 1, :, :, :]\n",
        "            else:\n",
        "                new_data[:, :, x - 1, :, :, :] = input_view[:, :, x, :, :, :] - input_view[:, :, x - 1, :, :, :]\n",
        "\n",
        "        return new_data\n",
        "\n",
        "    def _construct_flow_model(self, base_model):\n",
        "        # modify the convolution layers\n",
        "        # Torch models are usually defined in a hierarchical way.\n",
        "        # nn.modules.children() return all sub modules in a DFS manner\n",
        "        modules = list(self.base_model.modules())\n",
        "        first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n",
        "        conv_layer = modules[first_conv_idx]\n",
        "        container = modules[first_conv_idx - 1]\n",
        "\n",
        "        # modify parameters, assume the first blob contains the convolution kernels\n",
        "        params = [x.clone() for x in conv_layer.parameters()]\n",
        "        kernel_size = params[0].size()\n",
        "        new_kernel_size = kernel_size[:1] + (2 * self.new_length, ) + kernel_size[2:]\n",
        "        new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n",
        "\n",
        "        new_conv = nn.Conv2d(2 * self.new_length, conv_layer.out_channels,\n",
        "                             conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n",
        "                             bias=True if len(params) == 2 else False)\n",
        "        new_conv.weight.data = new_kernels\n",
        "        if len(params) == 2:\n",
        "            new_conv.bias.data = params[1].data # add bias if neccessary\n",
        "        layer_name = list(container.state_dict().keys())[0][:-7] # remove .weight suffix to get the layer name\n",
        "\n",
        "        # replace the first convlution layer\n",
        "        setattr(container, layer_name, new_conv)\n",
        "        return base_model\n",
        "\n",
        "    def _construct_diff_model(self, base_model, keep_rgb=False):\n",
        "        # modify the convolution layers\n",
        "        # Torch models are usually defined in a hierarchical way.\n",
        "        # nn.modules.children() return all sub modules in a DFS manner\n",
        "        modules = list(self.base_model.modules())\n",
        "        first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n",
        "        conv_layer = modules[first_conv_idx]\n",
        "        container = modules[first_conv_idx - 1]\n",
        "\n",
        "        # modify parameters, assume the first blob contains the convolution kernels\n",
        "        params = [x.clone() for x in conv_layer.parameters()]\n",
        "        kernel_size = params[0].size()\n",
        "        if not keep_rgb:\n",
        "            new_kernel_size = kernel_size[:1] + (3 * self.new_length,) + kernel_size[2:]\n",
        "            new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n",
        "        else:\n",
        "            new_kernel_size = kernel_size[:1] + (3 * self.new_length,) + kernel_size[2:]\n",
        "            new_kernels = torch.cat((params[0].data, params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()),\n",
        "                                    1)\n",
        "            new_kernel_size = kernel_size[:1] + (3 + 3 * self.new_length,) + kernel_size[2:]\n",
        "\n",
        "        new_conv = nn.Conv2d(new_kernel_size[1], conv_layer.out_channels,\n",
        "                             conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n",
        "                             bias=True if len(params) == 2 else False)\n",
        "        new_conv.weight.data = new_kernels\n",
        "        if len(params) == 2:\n",
        "            new_conv.bias.data = params[1].data  # add bias if neccessary\n",
        "        layer_name = list(container.state_dict().keys())[0][:-7]  # remove .weight suffix to get the layer name\n",
        "\n",
        "        # replace the first convolution layer\n",
        "        setattr(container, layer_name, new_conv)\n",
        "        return base_model\n",
        "\n",
        "    @property\n",
        "    def crop_size(self):\n",
        "        return self.input_size\n",
        "\n",
        "    @property\n",
        "    def scale_size(self):\n",
        "        return self.input_size * 256 // 224\n",
        "\n",
        "    @property\n",
        "    def is_3D_architecture(self):\n",
        "        return \"3d\" in self.arch or \"3D\" in self.arch\n",
        "\n",
        "    def get_augmentation(self, do_horizontal_flip=True):\n",
        "        if do_horizontal_flip:\n",
        "            if self.modality == 'RGB':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75, .66]),\n",
        "                                                       GroupRandomHorizontalFlip(is_flow=False)])\n",
        "            elif self.modality == 'Flow':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75]),\n",
        "                                                       GroupRandomHorizontalFlip(is_flow=True)])\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75]),\n",
        "                                                       GroupRandomHorizontalFlip(is_flow=False)])\n",
        "        else:\n",
        "            if self.modality == 'RGB':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75, .66])])\n",
        "            elif self.modality == 'Flow':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75])])\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75])])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62jFMZ1HQavl"
      },
      "source": [
        "# Data Loder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EESrAtUICBCr"
      },
      "source": [
        "## Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8zrkeePB8qB"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import random\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import numbers\n",
        "import math\n",
        "import torch\n",
        "\n",
        "\n",
        "class GroupRandomCrop(object):\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        w, h = img_group[0].size\n",
        "        th, tw = self.size\n",
        "\n",
        "        out_images = list()\n",
        "\n",
        "        x1 = random.randint(0, w - tw)\n",
        "        y1 = random.randint(0, h - th)\n",
        "\n",
        "        for img in img_group:\n",
        "            assert(img.size[0] == w and img.size[1] == h)\n",
        "            if w == tw and h == th:\n",
        "                out_images.append(img)\n",
        "            else:\n",
        "                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n",
        "\n",
        "        return out_images\n",
        "\n",
        "\n",
        "class GroupCenterCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.worker = torchvision.transforms.CenterCrop(size)\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        return [self.worker(img) for img in img_group]\n",
        "\n",
        "\n",
        "class GroupRandomHorizontalFlip(object):\n",
        "    \"\"\"Randomly horizontally flips the given PIL.Image with a probability of 0.5\"\"\"\n",
        "    def __init__(self, is_flow=False):\n",
        "        self.is_flow = is_flow\n",
        "\n",
        "    def __call__(self, img_group, is_flow=False):\n",
        "        v = random.random()\n",
        "        if v < 0.5:\n",
        "            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n",
        "            if self.is_flow:\n",
        "                for i in range(0, len(ret), 2):\n",
        "                    ret[i] = ImageOps.invert(ret[i])  # invert flow pixel values when flipping\n",
        "            return ret\n",
        "        else:\n",
        "            return img_group\n",
        "\n",
        "\n",
        "class GroupNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        rep_mean = self.mean * (tensor.size()[0]//len(self.mean))\n",
        "        rep_std = self.std * (tensor.size()[0]//len(self.std))\n",
        "\n",
        "        # TODO: make efficient\n",
        "        for t, m, s in zip(tensor, rep_mean, rep_std):\n",
        "            t.sub_(m).div_(s)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class GroupScale(object):\n",
        "    \"\"\" Rescales the input PIL.Image to the given 'size'.\n",
        "    'size' will be the size of the smaller edge.\n",
        "    For example, if height > width, then image will be\n",
        "    rescaled to (size * height / width, size)\n",
        "    size: size of the smaller edge\n",
        "    interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
        "        self.worker = torchvision.transforms.Resize(size, interpolation)\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        return [self.worker(img) for img in img_group]\n",
        "\n",
        "\n",
        "class GroupOverSample(object):\n",
        "    \"\"\"Optionally scale, then for each of five crop positions (fixed offsets): crop all images and append them to\n",
        "    the resulting list, also append their flipped versions\"\"\"\n",
        "    def __init__(self, crop_size, scale_size=None):\n",
        "        self.crop_size = crop_size if not isinstance(crop_size, int) else (crop_size, crop_size)\n",
        "\n",
        "        if scale_size is not None:\n",
        "            self.scale_worker = GroupScale(scale_size)\n",
        "        else:\n",
        "            self.scale_worker = None\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        if self.scale_worker is not None:\n",
        "            img_group = self.scale_worker(img_group)\n",
        "\n",
        "        image_w, image_h = img_group[0].size\n",
        "        crop_w, crop_h = self.crop_size\n",
        "\n",
        "        offsets = GroupMultiScaleCrop.fill_fix_offset(False, image_w, image_h, crop_w, crop_h)\n",
        "        oversample_group = list()\n",
        "        for o_w, o_h in offsets:\n",
        "            normal_group = list()\n",
        "            flip_group = list()\n",
        "            for i, img in enumerate(img_group):\n",
        "                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n",
        "                normal_group.append(crop)\n",
        "                flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "                if img.mode == 'L' and i % 2 == 0:\n",
        "                    flip_group.append(ImageOps.invert(flip_crop))\n",
        "                else:\n",
        "                    flip_group.append(flip_crop)\n",
        "\n",
        "            oversample_group.extend(normal_group)\n",
        "            oversample_group.extend(flip_group)\n",
        "        return oversample_group\n",
        "\n",
        "\n",
        "class GroupMultiScaleCrop(object):\n",
        "    \"\"\"Crop then resize. Crop size is determined randomly based on scales & max_distort. Crop position is determined\n",
        "    randomly or may be a random one of several fixed choices\"\"\"\n",
        "    def __init__(self, input_size, scales=None, max_distort=1, fix_crop=True, more_fix_crop=True):\n",
        "        self.scales = scales if scales is not None else [1, .875, .75, .66]\n",
        "        self.max_distort = max_distort\n",
        "        self.fix_crop = fix_crop\n",
        "        self.more_fix_crop = more_fix_crop\n",
        "        self.input_size = input_size if not isinstance(input_size, int) else [input_size, input_size]\n",
        "        self.interpolation = Image.BILINEAR\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        im_size = img_group[0].size\n",
        "\n",
        "        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size(im_size)\n",
        "        crop_img_group = [img.crop((offset_w, offset_h, offset_w + crop_w, offset_h + crop_h)) for img in img_group]\n",
        "        ret_img_group = [img.resize((self.input_size[0], self.input_size[1]), self.interpolation)\n",
        "                         for img in crop_img_group]\n",
        "        return ret_img_group\n",
        "\n",
        "    def _sample_crop_size(self, im_size):\n",
        "        image_w, image_h = im_size[0], im_size[1]\n",
        "\n",
        "        # find a crop size\n",
        "        base_size = min(image_w, image_h)\n",
        "        crop_sizes = [int(base_size * x) for x in self.scales]\n",
        "        crop_h = [self.input_size[1] if abs(x - self.input_size[1]) < 3 else x for x in crop_sizes]\n",
        "        crop_w = [self.input_size[0] if abs(x - self.input_size[0]) < 3 else x for x in crop_sizes]\n",
        "\n",
        "        pairs = []\n",
        "        for i, h in enumerate(crop_h):\n",
        "            for j, w in enumerate(crop_w):\n",
        "                if abs(i - j) <= self.max_distort:\n",
        "                    pairs.append((w, h))\n",
        "\n",
        "        crop_pair = random.choice(pairs)\n",
        "        if not self.fix_crop:\n",
        "            w_offset = random.randint(0, image_w - crop_pair[0])\n",
        "            h_offset = random.randint(0, image_h - crop_pair[1])\n",
        "        else:\n",
        "            w_offset, h_offset = self._sample_fix_offset(image_w, image_h, crop_pair[0], crop_pair[1])\n",
        "\n",
        "        return crop_pair[0], crop_pair[1], w_offset, h_offset\n",
        "\n",
        "    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):\n",
        "        offsets = self.fill_fix_offset(self.more_fix_crop, image_w, image_h, crop_w, crop_h)\n",
        "        return random.choice(offsets)\n",
        "\n",
        "    @staticmethod\n",
        "    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):\n",
        "        \"\"\"Choices of cropping an image of the given size (crop_w, crop_h) from the original image\"\"\"\n",
        "        w_step = (image_w - crop_w) // 4\n",
        "        h_step = (image_h - crop_h) // 4\n",
        "\n",
        "        ret = list()\n",
        "        ret.append((0, 0))  # upper left\n",
        "        ret.append((4 * w_step, 0))  # upper right\n",
        "        ret.append((0, 4 * h_step))  # lower left\n",
        "        ret.append((4 * w_step, 4 * h_step))  # lower right\n",
        "        ret.append((2 * w_step, 2 * h_step))  # center\n",
        "\n",
        "        if more_fix_crop:\n",
        "            ret.append((0, 2 * h_step))  # center left\n",
        "            ret.append((4 * w_step, 2 * h_step))  # center right\n",
        "            ret.append((2 * w_step, 4 * h_step))  # lower center\n",
        "            ret.append((2 * w_step, 0 * h_step))  # upper center\n",
        "\n",
        "            ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n",
        "            ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n",
        "            ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n",
        "            ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "class GroupRandomSizedCrop(object):\n",
        "    \"\"\"Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the original size\n",
        "    and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio (then resize)\n",
        "    This is popularly used to train the Inception networks\n",
        "    size: size of the smaller edge\n",
        "    interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        for attempt in range(10):\n",
        "            area = img_group[0].size[0] * img_group[0].size[1]\n",
        "            target_area = random.uniform(0.08, 1.0) * area\n",
        "            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n",
        "\n",
        "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
        "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
        "\n",
        "            if random.random() < 0.5:\n",
        "                w, h = h, w\n",
        "\n",
        "            if w <= img_group[0].size[0] and h <= img_group[0].size[1]:\n",
        "                x1 = random.randint(0, img_group[0].size[0] - w)\n",
        "                y1 = random.randint(0, img_group[0].size[1] - h)\n",
        "                found = True\n",
        "                break\n",
        "        else:\n",
        "            found = False\n",
        "            x1 = 0\n",
        "            y1 = 0\n",
        "\n",
        "        if found:\n",
        "            out_group = list()\n",
        "            for img in img_group:\n",
        "                img = img.crop((x1, y1, x1 + w, y1 + h))\n",
        "                assert(img.size == (w, h))\n",
        "                out_group.append(img.resize((self.size, self.size), self.interpolation))\n",
        "            return out_group\n",
        "        else:\n",
        "            # Fallback\n",
        "            scale = GroupScale(self.size, interpolation=self.interpolation)\n",
        "            crop = GroupRandomCrop(self.size)\n",
        "            return crop(scale(img_group))\n",
        "\n",
        "\n",
        "class Stack(object):\n",
        "\n",
        "    def __init__(self, roll=False):\n",
        "        self.roll = roll\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        if img_group[0].mode == 'L':\n",
        "            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n",
        "        elif img_group[0].mode == 'RGB':\n",
        "            if self.roll:\n",
        "                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n",
        "            else:\n",
        "                return np.concatenate(img_group, axis=2)\n",
        "\n",
        "\n",
        "class ToTorchFormatTensor(object):\n",
        "    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n",
        "    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \"\"\"\n",
        "    def __init__(self, div=True):\n",
        "        self.div = div\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        if isinstance(pic, np.ndarray):\n",
        "            # handle numpy array\n",
        "            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n",
        "        else:\n",
        "            # handle PIL Image\n",
        "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
        "            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n",
        "            # put it from HWC to CHW format\n",
        "            # yikes, this transpose takes 80% of the loading time/CPU\n",
        "            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
        "        return img.float().div(255) if self.div else img.float()\n",
        "\n",
        "\n",
        "class IdentityTransform(object):\n",
        "\n",
        "    def __call__(self, data):\n",
        "        return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqZLND_2CGSE"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp3yGpi1-xny"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "import torch\n",
        "from torchvision.transforms import ToTensor, Compose\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "\n",
        "\n",
        "class VideoRecord(object):\n",
        "    def __init__(self, row):\n",
        "        self._data = row\n",
        "        self.frame_count = 0\n",
        "\n",
        "    @property\n",
        "    def trial(self):\n",
        "        return self._data[0]\n",
        "\n",
        "    @property\n",
        "    def num_frames(self):  # number of frames if sampled at full temporal resolution (30 fps)\n",
        "        return int(self._data[1])\n",
        "\n",
        "    @property\n",
        "    def root_path(self):\n",
        "      return self._data[2]\n",
        "\n",
        "    @property\n",
        "    def label(self):\n",
        "        return int(self._data[3])\n",
        "\n",
        "class TSNDataSet(data.Dataset):\n",
        "    def __init__(self, list_of_list_files,\n",
        "                 num_segments=3, new_length=1, modality='RGB',\n",
        "                 image_tmpl='img_{:05d}.jpg', transform=None, normalize=None,\n",
        "                 random_shift=True, test_mode=False,\n",
        "                 video_sampling_step=3,\n",
        "                 return_3D_tensor=False, return_three_channels=False,\n",
        "                 preload_to_RAM=False, return_trial_id=False):\n",
        "\n",
        "\n",
        "        self.list_of_list_files = list_of_list_files\n",
        "        self.num_segments = num_segments\n",
        "        self.new_length = new_length  # number of consecutive frames contained in a snippet\n",
        "        self.modality = modality\n",
        "        self.image_tmpl = image_tmpl\n",
        "        self.transform = transform\n",
        "        self.normalize = normalize\n",
        "        self.random_shift = random_shift\n",
        "        self.test_mode = test_mode\n",
        "\n",
        "        self.video_sampling_step = video_sampling_step\n",
        "        self.return_3D_tensor = return_3D_tensor\n",
        "        self.return_three_channels = return_three_channels\n",
        "        self.preload_to_RAM = preload_to_RAM\n",
        "        self.return_trial_id = return_trial_id\n",
        "\n",
        "        if self.modality == 'RGBDiff':\n",
        "            self.new_length += 1# Diff needs one more image to calculate diff\n",
        "\n",
        "        self._parse_list_files()\n",
        "\n",
        "        if self.preload_to_RAM:\n",
        "            self._preload_images()\n",
        "\n",
        "    def _load_image(self, directory, idx):\n",
        "        if self.modality == 'RGB' or self.modality == 'RGBDiff':\n",
        "            return [Image.open(os.path.join(directory, self.image_tmpl.format(idx + 1))).convert('RGB')]\n",
        "            # extracted images are numbered from 1 to N (instead of 0 to N-1)\n",
        "        elif self.modality == 'Flow':\n",
        "            x_img = Image.open(os.path.join(directory, self.image_tmpl.format('x', idx + 1))).convert('L')\n",
        "            y_img = Image.open(os.path.join(directory, self.image_tmpl.format('y', idx + 1))).convert('L')\n",
        "\n",
        "            return [x_img, y_img]\n",
        "\n",
        "    def _parse_list_files(self):\n",
        "        self.video_list = []\n",
        "        for list_file in self.list_of_list_files:\n",
        "            video_list = [VideoRecord(x.strip().split(',')) for x in open(list_file)]\n",
        "            self.video_list += video_list\n",
        "        for record in self.video_list:\n",
        "            frame_count = record.num_frames // self.video_sampling_step\n",
        "            try:\n",
        "                # check whether last frame is there (sometimes gets lost during the extraction process)\n",
        "                self._load_image(os.path.join(record.root_path), frame_count - 1)\n",
        "            except FileNotFoundError:\n",
        "                frame_count = frame_count - 1\n",
        "            record.frame_count = frame_count\n",
        "\n",
        "    def _preload_images(self):\n",
        "        self.image_data = {}\n",
        "        for record in self.video_list:\n",
        "            print(\"Loading images for {}...\".format(record.trial))\n",
        "            images = []\n",
        "            img_dir = os.path.join(record.root_path)\n",
        "            for p in range(0, record.frame_count):\n",
        "                images.extend(self._load_image(img_dir, p))\n",
        "            self.image_data[record.trial] = images\n",
        "\n",
        "    def _sample_indices(self, record):\n",
        "        \"\"\"\n",
        "\n",
        "        :param record: VideoRecord\n",
        "        :return: list\n",
        "        \"\"\"\n",
        "        average_duration = (record.frame_count - self.new_length + 1) // self.num_segments\n",
        "        if average_duration > 0:\n",
        "            offsets = np.multiply(list(range(self.num_segments)), average_duration) + randint(average_duration, size=self.num_segments)\n",
        "        elif record.frame_count > self.num_segments:\n",
        "            offsets = np.sort(randint(record.frame_count - self.new_length + 1, size=self.num_segments))\n",
        "        else:\n",
        "            offsets = np.zeros((self.num_segments,))\n",
        "        return offsets\n",
        "\n",
        "    def _get_val_indices(self, record):\n",
        "        if record.frame_count > self.num_segments + self.new_length - 1:\n",
        "            tick = (record.frame_count - self.new_length + 1) / float(self.num_segments)\n",
        "            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n",
        "        else:\n",
        "            offsets = np.zeros((self.num_segments,))\n",
        "        return offsets\n",
        "\n",
        "    def _get_test_indices(self, record):\n",
        "        tick = (record.frame_count - self.new_length + 1) / float(self.num_segments)\n",
        "        offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n",
        "        return offsets\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        record = self.video_list[index]\n",
        "\n",
        "        if not self.test_mode:\n",
        "            segment_indices = self._sample_indices(record) if self.random_shift else self._get_val_indices(record)\n",
        "        else:\n",
        "            segment_indices = self._get_test_indices(record)\n",
        "\n",
        "        return self.get(record, segment_indices)\n",
        "\n",
        "    def _get_snippet(self, record, seg_ind):\n",
        "        snippet = list()\n",
        "        p = int(seg_ind)\n",
        "        for _ in range(self.new_length):\n",
        "            if self.preload_to_RAM:\n",
        "                if self.modality == 'RGB' or self.modality == 'RGBDiff':\n",
        "                    seg_imgs = self.image_data[record.trial][p: p + 1]\n",
        "                elif self.modality == 'Flow':\n",
        "                    idx = p * 2\n",
        "                    seg_imgs = self.image_data[record.trial][idx: idx + 2]\n",
        "            else:\n",
        "                img_dir = os.path.join(record.root_path)\n",
        "                seg_imgs = self._load_image(img_dir, p)\n",
        "            snippet.extend(seg_imgs)\n",
        "            if p < (record.frame_count - 1):\n",
        "                p += 1\n",
        "        return snippet\n",
        "\n",
        "    def get(self, record, indices):\n",
        "\n",
        "        images = list()\n",
        "        for seg_ind in indices:\n",
        "            images.extend(self._get_snippet(record, seg_ind))\n",
        "\n",
        "        if self.return_3D_tensor:\n",
        "            images = self.transform(images)\n",
        "            images = [ToTensor()(img) for img in images]\n",
        "            if self.modality == 'RGB':\n",
        "                images = torch.stack(images, 0)\n",
        "            elif self.modality == 'Flow':\n",
        "                _images = []\n",
        "                if self.return_three_channels:\n",
        "                    for i in range(len(images) // 2):\n",
        "                        image_dummy = (images[i] + images[i + 1]) / 2\n",
        "                        _images.append(torch.cat([images[i], images[i + 1], image_dummy], 0))\n",
        "                else:\n",
        "                    for i in range(len(images) // 2):\n",
        "                        _images.append(torch.cat([images[i], images[i + 1]], 0))\n",
        "                images = torch.stack(_images, 0)\n",
        "            images = self.normalize(images)\n",
        "            images = images.view(((-1, self.new_length) + images.size()[-3:]))\n",
        "            images = images.permute(0, 2, 1, 3, 4)\n",
        "            process_data = images\n",
        "        else:\n",
        "            transform = Compose([\n",
        "                self.transform,\n",
        "                Stack(roll=False),\n",
        "                ToTensor(),\n",
        "                self.normalize,\n",
        "            ])\n",
        "            process_data = transform(images)\n",
        "\n",
        "        target = record.label\n",
        "\n",
        "        if self.return_trial_id:\n",
        "            trial_id = record.trial.split('_')[-2]\n",
        "            print(\"Trial ID:\", trial_id)\n",
        "            return trial_id, process_data, target\n",
        "        else:\n",
        "            return process_data, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "consensus_type = 'avg'\n",
        "model = TSN(2, 10, 'RGB', base_model='Inception3D', new_length=64,\n",
        "            consensus_type='avg', before_softmax=True, dropout=0.7, partial_bn=False,\n",
        "            use_three_input_channels=False, pretrained_model='/content/drive/MyDrive/ColabNotebooks/videos_imra/rgb_imagenet.pt')\n",
        "\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "# Define your transformations\n",
        "normalize = GroupNormalize(model.input_mean, model.input_std)\n",
        "train_augmentation = model.get_augmentation(True)"
      ],
      "metadata": {
        "id": "Nlo7pV3F0fgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file = '/content/drive/MyDrive/ColabNotebooks/videos_imra/video_results.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Get the paths from the third column\n",
        "paths = df.iloc[:, 2]\n",
        "\n",
        "# Loop through the paths and count the files in each folder\n",
        "for index, path in enumerate(paths):\n",
        "    folder_path = os.path.join('/content/drive/MyDrive/ColabNotebooks/videos_imra', path)\n",
        "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
        "        file_count = len(os.listdir(folder_path))\n",
        "        print(f\"Folder: {folder_path}, File Count: {file_count}\")\n",
        "        # Update the value in the second column (at index 2)\n",
        "        df.iat[index, 1] = file_count\n",
        "    else:\n",
        "        print(f\"Folder does not exist: {folder_path}\")\n",
        "\n",
        "# Save the updated DataFrame to the CSV file\n",
        "df.to_csv(csv_file, index=False)"
      ],
      "metadata": {
        "id": "B6U46ynUDUmL",
        "outputId": "700d526a-cc96-474b-f52d-b485cf98cbfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/intreputedSuturing/Novice/frames/Novice_dV_006_interupted, File Count: 1627\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/Suturing/Novice/frames/Novice_dv_007_suture, File Count: 2135\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/intreputedSuturing/Novice/frames/Novice_dV_007_interupted, File Count: 2006\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/Suturing/Expert/frames/Expert_dV_010_suture, File Count: 776\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/intreputedSuturing/Expert/frames/Expert_dV_010_interupted, File Count: 955\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/Suturing/Novice/frames/Novice_dV_012_suture, File Count: 1447\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/intreputedSuturing/Novice/frames/Novice_dV_012_interupted, File Count: 1164\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/Suturing/Novice/frames/Novice_dV_012_suture, File Count: 1447\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/intreputedSuturing/Novice/frames/Novice_dV_012_interupted, File Count: 1164\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/Suturing/Novice/frames/Novice_dV_020_suture, File Count: 1308\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/intreputedSuturing/Novice/frames/Novice_dV_020_interupted, File Count: 2256\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/Suturing/Novice/frames/Novice_dV_022_suture, File Count: 2415\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/intreputedSuturing/Novice/frames/Novice_dV_022_interupted, File Count: 2332\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/Suturing/Novice/frames/Novice_dV_024_suture, File Count: 1577\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/intreputedSuturing/Novice/frames/Novice_dV_024_interupted, File Count: 1863\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/Suturing/Novice/frames/Novice_dV_025_suture, File Count: 1877\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/intreputedSuturing/Novice/frames/Novice_dV_025_interupted, File Count: 2371\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/Suturing/Novice/frames/Novice_dV_027_suture, File Count: 1436\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/intreputedSuturing/Novice/frames/Novice_dV_027_interupted, File Count: 2031\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/Suturing/Novice/frames/Novice_dV_031_suture, File Count: 2164\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/Suturing/Expert/frames/Expert_dV_051_suture, File Count: 2977\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/intreputedSuturing/Expert/frames/Expert_dV_2_051_interupted, File Count: 2188\n",
            "Folder: /content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/intreputedSuturing/Expert/frames/Expert_dV_1_051_interupted, File Count: 2669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "train_lists = ['/content/drive/MyDrive/ColabNotebooks/videos_imra/video_results.csv']\n",
        "\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "\n",
        "\n",
        "\n",
        "train_set = TSNDataSet(\n",
        "    list_of_list_files=train_lists,\n",
        "    num_segments=10,\n",
        "    new_length=64,\n",
        "    modality='RGB',\n",
        "    image_tmpl='frame_{:05d}.jpg',\n",
        "    transform=train_augmentation,\n",
        "    normalize=normalize,\n",
        "    random_shift=True,\n",
        "    test_mode=False,\n",
        "    video_sampling_step=3,\n",
        "    return_3D_tensor=True,\n",
        "    return_three_channels=False,\n",
        "    preload_to_RAM=True\n",
        ")\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Save the train_set object to a file\n",
        "with open('train_set.pkl', 'wb') as file:\n",
        "    pickle.dump(train_set, file)\n",
        "    print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hnVJyhJbaQHK",
        "outputId": "ae6ed640-c5de-4685-a7cc-ecede46e4d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing TSN with base model: Inception3D.\n",
            "                TSN Configurations:\n",
            "                input_modality:     RGB\n",
            "                num_segments:       10\n",
            "                new_length:         64\n",
            "                consensus_module:   avg\n",
            "                dropout_ratio:      0.7\n",
            "        \n",
            "loading pretrained model weights from /content/drive/MyDrive/ColabNotebooks/videos_imra/rgb_imagenet.pt\n",
            "Loading images for Novice_dv_006_suture...\n",
            "Loading images for Novice_dV_006_interupted...\n",
            "Loading images for Novice_dv_007_suture...\n",
            "Loading images for Novice_dV_007_interupted...\n",
            "Loading images for Expert_dV_010_suture...\n",
            "Loading images for Expert_dV_010_interupted...\n",
            "Loading images for Novice_dV_012_suture...\n",
            "Loading images for Novice_dV_012_interupted...\n",
            "Loading images for Novice_dV_012_suture...\n",
            "Loading images for Novice_dV_012_interupted...\n",
            "Loading images for Novice_dV_020_suture...\n",
            "Loading images for Novice_dV_020_interupted...\n",
            "Loading images for Novice_dV_022_suture...\n",
            "Loading images for Novice_dV_022_interupted...\n",
            "Loading images for Novice_dV_024_suture...\n",
            "Loading images for Novice_dV_024_interupted...\n",
            "Loading images for Novice_dV_025_suture...\n",
            "Loading images for Novice_dV_025_interupted...\n",
            "Loading images for Novice_dV_027_suture...\n",
            "Loading images for Novice_dV_027_interupted...\n",
            "Loading images for Novice_dV_031_suture...\n",
            "Loading images for Expert_dV_051_suture...\n",
            "Loading images for Expert_dV_2_051_interupted...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0f9a939734b9>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtrain_augmentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m train_set = TSNDataSet(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mlist_of_list_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_lists\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mnum_segments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-60409eac3585>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, list_of_list_files, num_segments, new_length, modality, image_tmpl, transform, normalize, random_shift, test_mode, video_sampling_step, return_3D_tensor, return_three_channels, preload_to_RAM, return_trial_id)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreload_to_RAM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-60409eac3585>\u001b[0m in \u001b[0;36m_preload_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mimg_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-60409eac3585>\u001b[0m in \u001b[0;36m_load_image\u001b[0;34m(self, directory, idx)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'RGB'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'RGBDiff'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_tmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;31m# extracted images are numbered from 1 to N (instead of 0 to N-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Flow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ColabNotebooks/videos_imra/pixel_reduced/videos_imra/intreputedSuturing/Expert/frames/Expert_dV_2_051_interupted/frame_02188.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load the train_set object from the file\n",
        "with open('train_set.pkl', 'rb') as file:\n",
        "    train_set = pickle.load(file)"
      ],
      "metadata": {
        "id": "-q4ANIZT4WFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr01c_DBIZnk",
        "outputId": "5ac45e21-08a7-4507-c956-41596a44593b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train loss: 0.6026 Train acc: 1.000\n",
            "Epoch 1: Train loss: 0.5895 Train acc: 1.000\n",
            "Epoch 2: Train loss: 0.5704 Train acc: 1.000\n",
            "Epoch 3: Train loss: 0.5847 Train acc: 1.000\n",
            "Epoch 4: Train loss: 0.5850 Train acc: 1.000\n",
            "Epoch 5: Train loss: 0.5612 Train acc: 1.000\n",
            "Epoch 6: Train loss: 0.5540 Train acc: 1.000\n",
            "Epoch 7: Train loss: 0.5558 Train acc: 1.000\n",
            "Epoch 8: Train loss: 0.5706 Train acc: 1.000\n",
            "Epoch 9: Train loss: 0.5416 Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_9.pth.tar\n",
            "Epoch 10: Train loss: 0.5586 Train acc: 1.000\n",
            "Epoch 11: Train loss: 0.5546 Train acc: 1.000\n",
            "Epoch 12: Train loss: 0.5507 Train acc: 1.000\n",
            "Epoch 13: Train loss: 0.5509 Train acc: 1.000\n",
            "Epoch 14: Train loss: 0.5504 Train acc: 1.000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "\n",
        "def run(model, train_set):\n",
        "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=40, pin_memory=True)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.00001)\n",
        "\n",
        "    for epoch in range(0, 120):\n",
        "        train_loss = AverageMeter()\n",
        "        train_acc = AverageMeter()\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _output = torch.sigmoid(output)\n",
        "            train_loss.update(loss.item(), target.size(0))\n",
        "            _, predicted = torch.max(_output, 1)\n",
        "            acc = (predicted == target).sum().item() / target.size(0)\n",
        "            train_acc.update(acc, target.size(0))\n",
        "\n",
        "\n",
        "        print(\"Epoch {}: Train loss: {:.4f} Train acc: {:.3f}\".format(epoch, train_loss.avg, train_acc.avg))\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 120 - 1:\n",
        "            name = \"model_\" + str(epoch)\n",
        "            model_file = os.path.join('/content/drive/MyDrive/ColabNotebooks/videos_imra/models', name + \".pth.tar\")\n",
        "            state = {\n",
        "                'epoch': epoch + 1,\n",
        "                'arch': 1,\n",
        "                'state_dict': model.state_dict(),\n",
        "            }\n",
        "            torch.save(state, model_file)\n",
        "            print(\"Saved model to \" + model_file)\n",
        "\n",
        "\n",
        "run(model, train_set)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5oVqYpU-5HWC",
        "outputId": "61f63905-9d7c-4c56-bb30-148383767c7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.4504, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.4404, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.4371, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.4431, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.4327, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.4249, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.4275, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.4378, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.4294, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.4175, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.4146, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.4056, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.4177, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.4267, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.4133, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.3962, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.3930, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.4377, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.3808, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.3747, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.4108, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.3735, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.3970, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.3761, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.3972, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.4198, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.3760, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.3729, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.3964, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.3542, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.3574, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.3742, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.3748, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.3609, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.3648, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.3422, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.3341, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.3656, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.3512, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.3534, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.3535, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.3321, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.3497, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.3338, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.3354, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.3410, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.3216, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.3498, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.3274, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.3243, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.3549, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.2984, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.3040, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.3040, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.3210, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.3069, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.3193, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.2932, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.2908, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.3185, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_0.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.3090, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.3243, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.3122, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.3176, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.3128, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.2897, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.3016, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.2947, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.3024, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.2855, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.2909, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.2800, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.2864, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.2676, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.2832, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.2718, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.2667, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.2925, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.2783, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.2749, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.3077, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.2654, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.2654, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.2579, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.2603, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.2669, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.2408, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.2520, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.2568, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.2560, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.2512, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.2614, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.2450, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.2514, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.2441, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.2335, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.2325, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.2408, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.2502, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.2353, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.2288, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.2158, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.2176, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.2162, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.2429, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.2116, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.2206, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.2227, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.2104, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.2185, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.2194, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.1956, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.2017, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.2021, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.2202, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.1890, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.1826, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.2053, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.1989, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.1911, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_1.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.1950, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.1866, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.2062, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.1972, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.1917, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.1773, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.1847, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.1817, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.2070, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.1925, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.1942, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.1932, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.1881, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.1697, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.1680, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.1667, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.1740, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.1875, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.1659, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.1849, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.1688, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.1623, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.1681, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.1635, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.1509, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.1634, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.1660, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.1763, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.1577, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.1528, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.1530, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.1423, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.1522, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.1500, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.1508, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.1394, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.1613, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.1448, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.1561, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.1555, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.1392, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.1321, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.1344, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.1361, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.1408, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.1344, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.1252, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.1254, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.1394, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.1328, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.1251, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.1272, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.1316, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.1267, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.1270, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.1337, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.1253, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.1211, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.1305, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.1108, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_2.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 2.0869, Train acc: 0.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 2.0064, Train acc: 0.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 2.0871, Train acc: 0.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 1.9698, Train acc: 0.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 1.9524, Train acc: 0.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 1.9717, Train acc: 0.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 1.9835, Train acc: 0.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 1.9094, Train acc: 0.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 1.9638, Train acc: 0.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 1.9976, Train acc: 0.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 1.9232, Train acc: 0.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 1.9523, Train acc: 0.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 1.9783, Train acc: 0.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 1.8787, Train acc: 0.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 1.9291, Train acc: 0.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 1.8636, Train acc: 0.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 1.9337, Train acc: 0.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 1.8249, Train acc: 0.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 1.8943, Train acc: 0.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 1.7940, Train acc: 0.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 1.8445, Train acc: 0.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 1.8514, Train acc: 0.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 1.7745, Train acc: 0.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 1.8223, Train acc: 0.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 1.8456, Train acc: 0.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 1.7654, Train acc: 0.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 1.7689, Train acc: 0.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 1.8105, Train acc: 0.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 1.8377, Train acc: 0.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 1.8098, Train acc: 0.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 1.7665, Train acc: 0.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 1.7279, Train acc: 0.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 1.7775, Train acc: 0.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 1.8223, Train acc: 0.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 1.7447, Train acc: 0.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 1.6616, Train acc: 0.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 1.6865, Train acc: 0.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 1.6387, Train acc: 0.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 1.7411, Train acc: 0.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 1.7355, Train acc: 0.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 1.7757, Train acc: 0.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 1.6695, Train acc: 0.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 1.6634, Train acc: 0.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 1.6403, Train acc: 0.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 1.6190, Train acc: 0.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 1.6588, Train acc: 0.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 1.6315, Train acc: 0.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 1.6106, Train acc: 0.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 1.6236, Train acc: 0.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 1.6076, Train acc: 0.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 1.6605, Train acc: 0.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 1.5679, Train acc: 0.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 1.5829, Train acc: 0.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 1.6050, Train acc: 0.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 1.5688, Train acc: 0.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 1.6049, Train acc: 0.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 1.5724, Train acc: 0.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 1.6138, Train acc: 0.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 1.5963, Train acc: 0.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 1.5485, Train acc: 0.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_3.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 1.5203, Train acc: 0.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 1.6650, Train acc: 0.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 1.6256, Train acc: 0.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 1.5409, Train acc: 0.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 1.5976, Train acc: 0.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 1.5023, Train acc: 0.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 1.5121, Train acc: 0.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 1.5487, Train acc: 0.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 1.5805, Train acc: 0.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 1.4933, Train acc: 0.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 1.4825, Train acc: 0.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 1.4705, Train acc: 0.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 1.4960, Train acc: 0.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 1.5020, Train acc: 0.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 1.4864, Train acc: 0.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 1.4914, Train acc: 0.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 1.4896, Train acc: 0.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 1.4255, Train acc: 0.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 1.4843, Train acc: 0.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 1.3928, Train acc: 0.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 1.4548, Train acc: 0.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 1.4306, Train acc: 0.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 1.4148, Train acc: 0.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 1.4178, Train acc: 0.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 1.4497, Train acc: 0.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 1.3851, Train acc: 0.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 1.3534, Train acc: 0.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 1.3782, Train acc: 0.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 1.3111, Train acc: 0.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 1.3480, Train acc: 0.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 1.3496, Train acc: 0.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 1.3599, Train acc: 0.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 1.3234, Train acc: 0.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 1.3518, Train acc: 0.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 1.3093, Train acc: 0.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 1.3105, Train acc: 0.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 1.2826, Train acc: 0.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 1.2897, Train acc: 0.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 1.2381, Train acc: 0.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 1.3237, Train acc: 0.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 1.2728, Train acc: 0.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 1.2614, Train acc: 0.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 1.2816, Train acc: 0.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 1.2891, Train acc: 0.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 1.2579, Train acc: 0.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 1.2232, Train acc: 0.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 1.2255, Train acc: 0.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 1.2778, Train acc: 0.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 1.1565, Train acc: 0.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 1.2166, Train acc: 0.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 1.2240, Train acc: 0.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 1.1789, Train acc: 0.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 1.1665, Train acc: 0.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 1.1532, Train acc: 0.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 1.1616, Train acc: 0.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 1.1656, Train acc: 0.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 1.1413, Train acc: 0.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 1.2066, Train acc: 0.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 1.1608, Train acc: 0.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 1.0822, Train acc: 0.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_4.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.3727, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.3582, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.3460, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.3465, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.3565, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.3768, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.3537, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.3396, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.3577, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.3515, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.3397, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.3390, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.3280, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.3372, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.3465, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.3417, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.3262, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.3273, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.3045, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.3213, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.3093, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.3075, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.2986, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.2910, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.3111, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.3006, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.3049, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.3008, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.2868, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.2818, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.2923, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.2962, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.2811, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.2764, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.2866, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.2739, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.3040, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.2631, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.2658, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.2549, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.2636, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.2677, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.2612, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.2550, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.2580, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.2453, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.2509, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.2513, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.2297, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.2466, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.2494, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.2427, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.2303, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.2155, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.2136, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.2286, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.2258, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.2495, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.2254, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.2145, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_5.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.2321, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.2335, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.2329, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.2399, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.2099, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.2195, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.2137, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.2203, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.1975, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.1996, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.2257, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.2239, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.1985, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.2060, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.1888, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.1982, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.2036, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.1978, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.1906, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.1927, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.1995, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.1893, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.1928, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.1900, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.1633, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.1800, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.1947, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.1851, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.1736, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.1754, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.1756, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.1687, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.1780, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.1630, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.1800, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.1580, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.1601, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.1704, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.1627, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.1569, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.1560, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.1690, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.1595, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.1493, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.1553, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.1564, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.1580, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.1569, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.1522, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.1549, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.1398, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.1456, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.1329, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.1345, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.1415, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.1454, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.1393, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.1534, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.1484, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.1342, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_6.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.1302, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.1393, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.1460, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.1268, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.1193, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.1323, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.1223, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.1299, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.1232, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.1199, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.1178, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.1177, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.1170, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.1226, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.1149, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.1046, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.1153, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.1132, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.1112, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.1128, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.1080, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.1063, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.1091, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.1098, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.1131, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.1103, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.1097, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.1163, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.1056, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.1060, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.1022, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.1027, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0989, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0993, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.1194, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.1050, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.1012, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0910, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.1049, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0915, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0973, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0962, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0871, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0844, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0933, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0936, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0944, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0889, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0831, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0915, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0857, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0888, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0820, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0913, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0873, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0927, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0822, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0852, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0872, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0823, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_7.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0935, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0865, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0832, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0799, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0919, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0872, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0826, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0870, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0786, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0778, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0797, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0804, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0786, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0782, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0770, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0819, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0784, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0718, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0847, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0739, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0763, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0670, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0688, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0672, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0731, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0727, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0656, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0776, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0704, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0639, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0696, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0618, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0772, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0704, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0691, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0609, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0684, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0654, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0708, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0690, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0617, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0575, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0655, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0577, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0614, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0634, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0619, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0601, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0563, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0595, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0579, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0606, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0583, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0562, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0599, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0530, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0536, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0596, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0556, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0542, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_8.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0615, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0618, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0589, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0583, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0549, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0585, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0619, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0536, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0566, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0522, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0599, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0492, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0536, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0530, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0490, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0522, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0530, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0508, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0514, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0500, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0523, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0498, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0510, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0495, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0450, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0457, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0443, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0484, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0483, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0490, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0512, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0459, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0495, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0427, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0518, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0416, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0449, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0377, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0456, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0467, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0417, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0433, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0395, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0445, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0465, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0439, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0478, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0463, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0359, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0407, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0413, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0396, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0369, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0368, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0359, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0366, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0384, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0374, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0355, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0344, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_9.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0405, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0390, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0364, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0401, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0383, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0399, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0356, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0356, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0353, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0374, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0327, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0374, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0361, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0347, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0327, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0329, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0346, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0345, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0362, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0303, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0331, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0308, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0329, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0313, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0315, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0292, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0317, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0310, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0303, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0284, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0296, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0295, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0316, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0306, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0302, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0295, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0309, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0263, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0268, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0308, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0288, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0296, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0270, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0299, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0264, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0285, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0279, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0263, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0248, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0246, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0274, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0283, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0277, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0254, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0275, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0240, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0279, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0271, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0256, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0248, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_10.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0281, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0283, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0279, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0344, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0305, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0258, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0286, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0300, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0272, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0279, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0269, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0254, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0321, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0271, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0259, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0324, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0261, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0257, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0270, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0313, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0249, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0233, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0236, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0245, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0252, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0239, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0251, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0300, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0232, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0231, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0240, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0213, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0211, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0215, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0242, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0244, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0227, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0220, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0257, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0223, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0214, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0212, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0213, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0211, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0183, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0244, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0221, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0224, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0209, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0205, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0189, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0196, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0197, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0203, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0193, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0197, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0178, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0184, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0185, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0194, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_11.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0202, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0223, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0223, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0196, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0184, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0193, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0255, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0211, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0197, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0191, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0204, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0182, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0184, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0176, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0173, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0176, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0189, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0174, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0182, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0188, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0154, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0192, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0168, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0205, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0174, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0175, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0167, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0170, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0167, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0193, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0153, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0155, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0174, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0152, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0177, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0169, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0157, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0157, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0164, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0155, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0168, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0158, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0147, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0149, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0152, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0168, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0146, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0146, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0182, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0173, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0148, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0135, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0141, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0145, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0146, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0136, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0132, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0140, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0136, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0138, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_12.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0141, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0176, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0155, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0156, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0148, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0147, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0133, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0134, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0131, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0130, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0132, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0127, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0124, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0135, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0139, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0123, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0122, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0126, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0140, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0122, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0126, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0112, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0127, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0121, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0125, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0120, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0121, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0115, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0119, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0124, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0101, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0102, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0101, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0110, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0104, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0104, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0108, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0107, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0120, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0120, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0107, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0097, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0095, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0101, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0110, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0106, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0107, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0107, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0095, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0106, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0106, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0109, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0094, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0100, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0095, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0091, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0107, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0085, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0106, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0100, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_13.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0100, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0098, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0087, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0101, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0103, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0093, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0097, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0093, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0089, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0099, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0097, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0098, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0089, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0093, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0093, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0087, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0086, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0085, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0080, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0097, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0084, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0097, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0075, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0072, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0081, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0074, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0080, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0081, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0084, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0082, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0072, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0082, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0076, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0071, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0087, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0084, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0067, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0071, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0087, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0071, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0072, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0074, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0066, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0082, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0072, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0072, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0072, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0074, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0078, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0061, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0066, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0083, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0072, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0073, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0067, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0072, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0067, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0067, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0073, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0066, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_14.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0078, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0071, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0066, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0073, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0064, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0073, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0082, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0066, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0064, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0067, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0062, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0061, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0067, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0070, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0064, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0060, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0065, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0058, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0061, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0062, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0058, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0057, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0060, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0059, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0065, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0060, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0063, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0073, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0063, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0053, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0054, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0055, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0063, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0053, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0057, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0056, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0051, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0057, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0052, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0050, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0058, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0058, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0051, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0052, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0054, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0051, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0051, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0051, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0056, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0046, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0053, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0052, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0049, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0053, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0052, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0051, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0045, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0048, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0048, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0051, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_15.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0050, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0052, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0046, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0044, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0042, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0047, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0047, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0045, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0041, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0052, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0046, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0039, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0043, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0042, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0042, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0041, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0038, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0042, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0042, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0042, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0036, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0038, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0039, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0039, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0036, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0039, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0037, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0039, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0035, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0040, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0036, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0037, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0043, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0035, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0036, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0035, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0032, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0035, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0034, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0039, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0036, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0034, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0034, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0036, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0031, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0035, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0031, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0033, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0036, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0031, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0033, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0032, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0033, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0029, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0034, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0033, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0034, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0030, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0032, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0033, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_16.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0039, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0034, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0036, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0036, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0039, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0032, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0031, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0034, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0032, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0034, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0031, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0033, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0038, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0031, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0034, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0033, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0028, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0029, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0029, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0032, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0031, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0035, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0028, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0032, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0028, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0031, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0031, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0031, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0027, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0028, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0029, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0033, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0029, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0032, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0029, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0026, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0032, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0027, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0031, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0027, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0024, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0027, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0028, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0025, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0027, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0030, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0025, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0026, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0021, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0026, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0029, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0027, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0022, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0024, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0027, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0024, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0024, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0025, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0024, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0021, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_17.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0027, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0024, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0027, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0027, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0025, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0026, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0025, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0025, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0026, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0023, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0023, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0026, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0027, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0024, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0025, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0023, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0022, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0023, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0023, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0022, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0024, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0022, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0019, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0023, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0020, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0020, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0022, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0022, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0020, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0023, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0021, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0021, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0020, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0019, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0021, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0020, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0021, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0024, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0022, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0018, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0018, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0021, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0017, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0021, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0018, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0021, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0021, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0020, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0018, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0019, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0018, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0019, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0020, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0020, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0019, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0021, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0018, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0019, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0020, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0017, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_18.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0020, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0021, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0021, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0018, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0018, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0016, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0018, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0020, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0019, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0018, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0018, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0016, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0019, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0017, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0017, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0015, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0016, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0017, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0017, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0017, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0016, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0016, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0016, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0016, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0016, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0016, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0016, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0016, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0012, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0015, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0016, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0017, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0014, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0013, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0015, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0015, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0015, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0014, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0013, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0014, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0013, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0014, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0014, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0014, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0014, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0013, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0013, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0013, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0012, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0014, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0014, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0014, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0012, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0013, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0012, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0011, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0013, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0013, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0013, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0013, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models/model_19.pth.tar\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 6.6108, Train acc: 0.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 6.5206, Train acc: 0.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 6.6739, Train acc: 0.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 6.5601, Train acc: 0.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 6.5561, Train acc: 0.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 6.5371, Train acc: 0.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 6.3773, Train acc: 0.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 6.5447, Train acc: 0.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 6.5883, Train acc: 0.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 6.4214, Train acc: 0.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 6.3830, Train acc: 0.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 6.5310, Train acc: 0.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 6.3437, Train acc: 0.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 6.4631, Train acc: 0.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 6.5996, Train acc: 0.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 6.3984, Train acc: 0.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 6.2038, Train acc: 0.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 6.2854, Train acc: 0.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 6.2949, Train acc: 0.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 6.1963, Train acc: 0.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 6.2221, Train acc: 0.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 6.2337, Train acc: 0.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 6.2703, Train acc: 0.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 6.1489, Train acc: 0.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 6.2396, Train acc: 0.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 6.2112, Train acc: 0.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 5.9022, Train acc: 0.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 6.1780, Train acc: 0.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 6.0945, Train acc: 0.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 6.2038, Train acc: 0.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 6.0771, Train acc: 0.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 6.0980, Train acc: 0.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 6.1422, Train acc: 0.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 6.0941, Train acc: 0.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 6.2350, Train acc: 0.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 5.9123, Train acc: 0.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 6.0405, Train acc: 0.000\n",
            "epoch: 37\n",
            "Batch 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Z-kxyFw5Hdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j9ZHjcgj5Hg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VutLd_xW5HkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hbpgmd-a5Hnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "\n",
        "def run(model,train_set):\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(train_set,\n",
        "                                                                      num_replicas = xm.xrt_world_size(),\n",
        "                                                                      rank         = xm.get_ordinal(),\n",
        "                                                                      shuffle      = True)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_set,\n",
        "                                                batch_size  = 64,\n",
        "                                                sampler     = train_sampler,\n",
        "                                                num_workers = 4,\n",
        "                                                pin_memory  = True)\n",
        "\n",
        "  mx    = xmp.MpModelWrapper(model)\n",
        "  device = xm.xla_device()\n",
        "  model  = mx.to(device)\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  #criterion = torch.nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=(0.00001*xm.xrt_world_size()))\n",
        "  para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "\n",
        "  for epoch in range(0, 120):\n",
        "          train_loss = AverageMeter()\n",
        "          train_acc = AverageMeter()\n",
        "          model.train()\n",
        "          for batch_idx, batch in enumerate(para_loader):\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              data, target = batch\n",
        "              batch_size = target.size(0)\n",
        "              data = data.to(device)\n",
        "              target = target.to(device)\n",
        "\n",
        "              output = model(data)\n",
        "              loss = criterion(output, target)\n",
        "              loss.backward()\n",
        "              xm.optimizer_step(optimizer, barrier = True)\n",
        "\n",
        "              train_loss.update(loss.item(), batch_size)\n",
        "              _output = torch.nn.Softmax(dim=1)(output)\n",
        "              #criterion = torch.nn.BCEWithLogitsLoss()\n",
        "              _, predicted = torch.max(_output.data, 1)\n",
        "              acc = (predicted == target).sum().item() / batch_size\n",
        "              train_acc.update(acc, batch_size)\n",
        "\n",
        "          if (epoch + 1) %  epoch == 120 - 1:  # eval\n",
        "              xm.master_print(\"Epoch {}: Train loss: {train_loss.avg:.4f} Train acc: {train_acc.avg:.3f} \".format(\n",
        "                  epoch, train_loss=train_loss, train_acc=train_acc))\n",
        "\n",
        "          if (epoch + 1) % 10 == 0 or epoch == 1999 - 1:  # save\n",
        "              name = \"model_\" + str(epoch)\n",
        "              model_file = os.path.join('/content/drive/MyDrive/ColabNotebooks/videos_imra/models', name + \".pth.tar\")\n",
        "              state = {'epoch': epoch + 1,\n",
        "                      'arch': 1,\n",
        "                      'state_dict': model.state_dict(),\n",
        "                      }\n",
        "              torch.save(state, model_file)\n",
        "              xm.master_print(\"Saved model to \" + model_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Hs_uMReaZOXd",
        "outputId": "719e79b6-3212-423d-86e8-0b1928b65640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c7d5f4014d7b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_multiprocessing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_loader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_xla'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "def _mp_fn(rank):\n",
        "    device = xm.xla_device()\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    run(model, train_set)\n",
        "\n",
        "# Spawn 50 processes to utilize 50 TPU workers\n",
        "xmp.spawn(_mp_fn, nprocs=1, start_method='fork')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MKoKczktZNFh",
        "outputId": "0c4b1bb9-9628-4a40-ff04-3189e76eb952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-cb36446254d6>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Spawn 50 processes to utilize 50 TPU workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mp_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    393\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpf_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_devices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m     \u001b[0m_start_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpf_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     result = torch.multiprocessing.start_processes(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36m_start_fn\u001b[0;34m(index, pf_cfg, fn, args)\u001b[0m\n\u001b[1;32m    326\u001b[0m   \u001b[0;31m# environment must be fully setup before doing so.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m   \u001b[0m_setup_replication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m   \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-cb36446254d6>\u001b[0m in \u001b[0;36m_mp_fn\u001b[0;34m(rank)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_tensor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'torch.FloatTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Spawn 50 processes to utilize 50 TPU workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-74a10f006f4e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, train_set)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mmx\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMpModelWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mmodel\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m#criterion = torch.nn.BCEWithLogitsLoss()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \"\"\"\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m                 \u001b[0mout_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: torch_xla/csrc/tensor.cpp:351 : Check failed: tensor_data \n*** Begin stack trace ***\n\ttsl::CurrentStackTrace()\n\ttorch_xla::XLATensor::GetIrValue() const\n\ttorch_xla::XLATensor::ShallowCopyTo(c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> >) const\n\ttorch_xla::XLATensorImpl::shallow_copy_from(c10::intrusive_ptr<c10::TensorImpl, c10::detail::intrusive_target_default_null_type<c10::TensorImpl> > const&)\n\t\n\tTHPVariable_set_data(THPVariable*, _object*, void*)\n\t_PyObject_GenericSetAttrWithDict\n\tPyObject_SetAttr\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\tPyObject_Call\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t\n\t_PyEval_EvalFrameDefault\n\t\n\tPyEval_EvalCode\n\t\n\t\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t_PyEval_EvalFrameDefault\n\t_PyFunction_Vectorcall\n\t\n\tPy_RunMain\n\tPy_BytesMain\n\t\n\t__libc_start_main\n\t_start\n*** End stack trace ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "MptIxFScZF2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z5mG9cptcXBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}