{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srujan-b/Assignment-2-Choose-Your-Own-Analysis/blob/main/merged.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51kyjylRd2GA",
        "outputId": "46fee6af-4750-4cdb-9aae-42e6f34dfdc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBUIgy8np_fu",
        "outputId": "79a2b392-5c33-4d32-f23c-85c6fad9b829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ColabNotebooks/videos_imra\n"
          ]
        }
      ],
      "source": [
        "cd '/content/drive/MyDrive/ColabNotebooks/videos_imra/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOXVEN8ld3V5"
      },
      "source": [
        "## ConsensusModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "4OOXlrfMdkjl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class SegmentConsensus(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input_tensor, consensus_type, dim=1):\n",
        "        if consensus_type == 'avg':\n",
        "            output = input_tensor.mean(dim=dim, keepdim=True)\n",
        "        elif consensus_type == 'identity':\n",
        "            output = input_tensor\n",
        "        else:\n",
        "            output = None\n",
        "        ctx.save_for_backward(input_tensor)\n",
        "        ctx.consensus_type = consensus_type\n",
        "        ctx.dim = dim\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input_tensor, = ctx.saved_tensors\n",
        "        consensus_type = ctx.consensus_type\n",
        "        dim = ctx.dim\n",
        "        if consensus_type == 'avg':\n",
        "            grad_in = grad_output.expand(input_tensor.size()) / float(input_tensor.size(dim))\n",
        "        elif consensus_type == 'identity':\n",
        "            grad_in = grad_output\n",
        "        else:\n",
        "            grad_in = None\n",
        "        return grad_in, None, None\n",
        "\n",
        "class ConsensusModule(torch.nn.Module):\n",
        "    def __init__(self, consensus_type, dim=1):\n",
        "        super(ConsensusModule, self).__init__()\n",
        "        self.consensus_type = consensus_type if consensus_type != 'rnn' else 'identity'\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, input):\n",
        "        return SegmentConsensus.apply(input, self.consensus_type, self.dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eDLqwsN0XS6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "yEpftr14XTSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from sys import stderr\n",
        "\n",
        "\n",
        "def log(file, msg):\n",
        "    \"\"\"Log a message.\n",
        "\n",
        "    :param file: File object to which the message will be written.\n",
        "    :param msg:  Message to log (str).\n",
        "    \"\"\"\n",
        "    print(time.strftime(\"[%d.%m.%Y %H:%M:%S]: \"), msg, file=stderr)\n",
        "    file.write(time.strftime(\"[%d.%m.%Y %H:%M:%S]: \") + msg + os.linesep)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n"
      ],
      "metadata": {
        "id": "Zc6vVZRmXVFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_orkXCGd7fS"
      },
      "source": [
        "## Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZiGIh8weCyU"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import random\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import numbers\n",
        "import math\n",
        "import torch\n",
        "\n",
        "\n",
        "class GroupRandomCrop(object):\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        w, h = img_group[0].size\n",
        "        th, tw = self.size\n",
        "\n",
        "        out_images = list()\n",
        "\n",
        "        x1 = random.randint(0, w - tw)\n",
        "        y1 = random.randint(0, h - th)\n",
        "\n",
        "        for img in img_group:\n",
        "            assert(img.size[0] == w and img.size[1] == h)\n",
        "            if w == tw and h == th:\n",
        "                out_images.append(img)\n",
        "            else:\n",
        "                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n",
        "\n",
        "        return out_images\n",
        "\n",
        "\n",
        "class GroupCenterCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.worker = torchvision.transforms.CenterCrop(size)\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        return [self.worker(img) for img in img_group]\n",
        "\n",
        "\n",
        "class GroupRandomHorizontalFlip(object):\n",
        "    \"\"\"Randomly horizontally flips the given PIL.Image with a probability of 0.5\"\"\"\n",
        "    def __init__(self, is_flow=False):\n",
        "        self.is_flow = is_flow\n",
        "\n",
        "    def __call__(self, img_group, is_flow=False):\n",
        "        v = random.random()\n",
        "        if v < 0.5:\n",
        "            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n",
        "            if self.is_flow:\n",
        "                for i in range(0, len(ret), 2):\n",
        "                    ret[i] = ImageOps.invert(ret[i])  # invert flow pixel values when flipping\n",
        "            return ret\n",
        "        else:\n",
        "            return img_group\n",
        "\n",
        "\n",
        "class GroupNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        rep_mean = self.mean * (tensor.size()[0]//len(self.mean))\n",
        "        rep_std = self.std * (tensor.size()[0]//len(self.std))\n",
        "\n",
        "        # TODO: make efficient\n",
        "        for t, m, s in zip(tensor, rep_mean, rep_std):\n",
        "            t.sub_(m).div_(s)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class GroupScale(object):\n",
        "    \"\"\" Rescales the input PIL.Image to the given 'size'.\n",
        "    'size' will be the size of the smaller edge.\n",
        "    For example, if height > width, then image will be\n",
        "    rescaled to (size * height / width, size)\n",
        "    size: size of the smaller edge\n",
        "    interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
        "        self.worker = torchvision.transforms.Resize(size, interpolation)\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        return [self.worker(img) for img in img_group]\n",
        "\n",
        "\n",
        "class GroupOverSample(object):\n",
        "    \"\"\"Optionally scale, then for each of five crop positions (fixed offsets): crop all images and append them to\n",
        "    the resulting list, also append their flipped versions\"\"\"\n",
        "    def __init__(self, crop_size, scale_size=None):\n",
        "        self.crop_size = crop_size if not isinstance(crop_size, int) else (crop_size, crop_size)\n",
        "\n",
        "        if scale_size is not None:\n",
        "            self.scale_worker = GroupScale(scale_size)\n",
        "        else:\n",
        "            self.scale_worker = None\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        if self.scale_worker is not None:\n",
        "            img_group = self.scale_worker(img_group)\n",
        "\n",
        "        image_w, image_h = img_group[0].size\n",
        "        crop_w, crop_h = self.crop_size\n",
        "\n",
        "        offsets = GroupMultiScaleCrop.fill_fix_offset(False, image_w, image_h, crop_w, crop_h)\n",
        "        oversample_group = list()\n",
        "        for o_w, o_h in offsets:\n",
        "            normal_group = list()\n",
        "            flip_group = list()\n",
        "            for i, img in enumerate(img_group):\n",
        "                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n",
        "                normal_group.append(crop)\n",
        "                flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "                if img.mode == 'L' and i % 2 == 0:\n",
        "                    flip_group.append(ImageOps.invert(flip_crop))\n",
        "                else:\n",
        "                    flip_group.append(flip_crop)\n",
        "\n",
        "            oversample_group.extend(normal_group)\n",
        "            oversample_group.extend(flip_group)\n",
        "        return oversample_group\n",
        "\n",
        "\n",
        "class GroupMultiScaleCrop(object):\n",
        "    \"\"\"Crop then resize. Crop size is determined randomly based on scales & max_distort. Crop position is determined\n",
        "    randomly or may be a random one of several fixed choices\"\"\"\n",
        "    def __init__(self, input_size, scales=None, max_distort=1, fix_crop=True, more_fix_crop=True):\n",
        "        self.scales = scales if scales is not None else [1, .875, .75, .66]\n",
        "        self.max_distort = max_distort\n",
        "        self.fix_crop = fix_crop\n",
        "        self.more_fix_crop = more_fix_crop\n",
        "        self.input_size = input_size if not isinstance(input_size, int) else [input_size, input_size]\n",
        "        self.interpolation = Image.BILINEAR\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        im_size = img_group[0].size\n",
        "\n",
        "        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size(im_size)\n",
        "        crop_img_group = [img.crop((offset_w, offset_h, offset_w + crop_w, offset_h + crop_h)) for img in img_group]\n",
        "        ret_img_group = [img.resize((self.input_size[0], self.input_size[1]), self.interpolation)\n",
        "                         for img in crop_img_group]\n",
        "        return ret_img_group\n",
        "\n",
        "    def _sample_crop_size(self, im_size):\n",
        "        image_w, image_h = im_size[0], im_size[1]\n",
        "\n",
        "        # find a crop size\n",
        "        base_size = min(image_w, image_h)\n",
        "        crop_sizes = [int(base_size * x) for x in self.scales]\n",
        "        crop_h = [self.input_size[1] if abs(x - self.input_size[1]) < 3 else x for x in crop_sizes]\n",
        "        crop_w = [self.input_size[0] if abs(x - self.input_size[0]) < 3 else x for x in crop_sizes]\n",
        "\n",
        "        pairs = []\n",
        "        for i, h in enumerate(crop_h):\n",
        "            for j, w in enumerate(crop_w):\n",
        "                if abs(i - j) <= self.max_distort:\n",
        "                    pairs.append((w, h))\n",
        "\n",
        "        crop_pair = random.choice(pairs)\n",
        "        if not self.fix_crop:\n",
        "            w_offset = random.randint(0, image_w - crop_pair[0])\n",
        "            h_offset = random.randint(0, image_h - crop_pair[1])\n",
        "        else:\n",
        "            w_offset, h_offset = self._sample_fix_offset(image_w, image_h, crop_pair[0], crop_pair[1])\n",
        "\n",
        "        return crop_pair[0], crop_pair[1], w_offset, h_offset\n",
        "\n",
        "    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):\n",
        "        offsets = self.fill_fix_offset(self.more_fix_crop, image_w, image_h, crop_w, crop_h)\n",
        "        return random.choice(offsets)\n",
        "\n",
        "    @staticmethod\n",
        "    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):\n",
        "        \"\"\"Choices of cropping an image of the given size (crop_w, crop_h) from the original image\"\"\"\n",
        "        w_step = (image_w - crop_w) // 4\n",
        "        h_step = (image_h - crop_h) // 4\n",
        "\n",
        "        ret = list()\n",
        "        ret.append((0, 0))  # upper left\n",
        "        ret.append((4 * w_step, 0))  # upper right\n",
        "        ret.append((0, 4 * h_step))  # lower left\n",
        "        ret.append((4 * w_step, 4 * h_step))  # lower right\n",
        "        ret.append((2 * w_step, 2 * h_step))  # center\n",
        "\n",
        "        if more_fix_crop:\n",
        "            ret.append((0, 2 * h_step))  # center left\n",
        "            ret.append((4 * w_step, 2 * h_step))  # center right\n",
        "            ret.append((2 * w_step, 4 * h_step))  # lower center\n",
        "            ret.append((2 * w_step, 0 * h_step))  # upper center\n",
        "\n",
        "            ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n",
        "            ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n",
        "            ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n",
        "            ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "class GroupRandomSizedCrop(object):\n",
        "    \"\"\"Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the original size\n",
        "    and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio (then resize)\n",
        "    This is popularly used to train the Inception networks\n",
        "    size: size of the smaller edge\n",
        "    interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        for attempt in range(10):\n",
        "            area = img_group[0].size[0] * img_group[0].size[1]\n",
        "            target_area = random.uniform(0.08, 1.0) * area\n",
        "            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n",
        "\n",
        "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
        "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
        "\n",
        "            if random.random() < 0.5:\n",
        "                w, h = h, w\n",
        "\n",
        "            if w <= img_group[0].size[0] and h <= img_group[0].size[1]:\n",
        "                x1 = random.randint(0, img_group[0].size[0] - w)\n",
        "                y1 = random.randint(0, img_group[0].size[1] - h)\n",
        "                found = True\n",
        "                break\n",
        "        else:\n",
        "            found = False\n",
        "            x1 = 0\n",
        "            y1 = 0\n",
        "\n",
        "        if found:\n",
        "            out_group = list()\n",
        "            for img in img_group:\n",
        "                img = img.crop((x1, y1, x1 + w, y1 + h))\n",
        "                assert(img.size == (w, h))\n",
        "                out_group.append(img.resize((self.size, self.size), self.interpolation))\n",
        "            return out_group\n",
        "        else:\n",
        "            # Fallback\n",
        "            scale = GroupScale(self.size, interpolation=self.interpolation)\n",
        "            crop = GroupRandomCrop(self.size)\n",
        "            return crop(scale(img_group))\n",
        "\n",
        "\n",
        "class Stack(object):\n",
        "\n",
        "    def __init__(self, roll=False):\n",
        "        self.roll = roll\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        if img_group[0].mode == 'L':\n",
        "            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n",
        "        elif img_group[0].mode == 'RGB':\n",
        "            if self.roll:\n",
        "                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n",
        "            else:\n",
        "                return np.concatenate(img_group, axis=2)\n",
        "\n",
        "\n",
        "class ToTorchFormatTensor(object):\n",
        "    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n",
        "    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \"\"\"\n",
        "    def __init__(self, div=True):\n",
        "        self.div = div\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        if isinstance(pic, np.ndarray):\n",
        "            # handle numpy array\n",
        "            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n",
        "        else:\n",
        "            # handle PIL Image\n",
        "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
        "            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n",
        "            # put it from HWC to CHW format\n",
        "            # yikes, this transpose takes 80% of the loading time/CPU\n",
        "            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
        "        return img.float().div(255) if self.div else img.float()\n",
        "\n",
        "\n",
        "class IdentityTransform(object):\n",
        "\n",
        "    def __call__(self, data):\n",
        "        return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-MC5P3_IubC"
      },
      "source": [
        "## Bin Inception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NCErS36JSOX"
      },
      "source": [
        "### layer_factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPLjiPueJVN4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "LAYER_BUILDER_DICT=dict()\n",
        "\n",
        "\n",
        "def parse_expr(expr):\n",
        "    parts = expr.split('<=')\n",
        "    return parts[0].split(','), parts[1], parts[2].split(',')\n",
        "\n",
        "\n",
        "def get_basic_layer(info, channels=None, conv_bias=False):\n",
        "    id = info['id']\n",
        "    attr = info['attrs'] if 'attrs' in info else list()\n",
        "\n",
        "    out, op, in_vars = parse_expr(info['expr'])\n",
        "    assert(len(out) == 1)\n",
        "    assert(len(in_vars) == 1)\n",
        "    mod, out_channel, = LAYER_BUILDER_DICT[op](attr, channels, conv_bias)\n",
        "\n",
        "    return id, out[0], mod, out_channel, in_vars[0]\n",
        "\n",
        "\n",
        "def build_conv(attr, channels=None, conv_bias=False):\n",
        "    out_channels = attr['num_output']\n",
        "    ks = attr['kernel_size'] if 'kernel_size' in attr else (attr['kernel_h'], attr['kernel_w'])\n",
        "    if 'pad' in attr or 'pad_w' in attr and 'pad_h' in attr:\n",
        "        padding = attr['pad'] if 'pad' in attr else (attr['pad_h'], attr['pad_w'])\n",
        "    else:\n",
        "        padding = 0\n",
        "    if 'stride' in attr or 'stride_w' in attr and 'stride_h' in attr:\n",
        "        stride = attr['stride'] if 'stride' in attr else (attr['stride_h'], attr['stride_w'])\n",
        "    else:\n",
        "        stride = 1\n",
        "\n",
        "    conv = nn.Conv2d(channels, out_channels, ks, stride, padding, bias=conv_bias)\n",
        "\n",
        "    return conv, out_channels\n",
        "\n",
        "\n",
        "def build_pooling(attr, channels=None, conv_bias=False):\n",
        "    method = attr['mode']\n",
        "    pad = attr['pad'] if 'pad' in attr else 0\n",
        "    if method == 'max':\n",
        "        pool = nn.MaxPool2d(attr['kernel_size'], attr['stride'], pad,\n",
        "                            ceil_mode=True) # all Caffe pooling use ceil model\n",
        "    elif method == 'ave':\n",
        "        pool = nn.AvgPool2d(attr['kernel_size'], attr['stride'], pad,\n",
        "                            ceil_mode=True)  # all Caffe pooling use ceil model\n",
        "    else:\n",
        "        raise ValueError(\"Unknown pooling method: {}\".format(method))\n",
        "\n",
        "    return pool, channels\n",
        "\n",
        "\n",
        "def build_relu(attr, channels=None, conv_bias=False):\n",
        "    return nn.ReLU(inplace=True), channels\n",
        "\n",
        "\n",
        "def build_bn(attr, channels=None, conv_bias=False):\n",
        "    return nn.BatchNorm2d(channels, momentum=0.1), channels\n",
        "\n",
        "\n",
        "def build_linear(attr, channels=None, conv_bias=False):\n",
        "    return nn.Linear(channels, attr['num_output']), channels\n",
        "\n",
        "\n",
        "def build_dropout(attr, channels=None, conv_bias=False):\n",
        "    return nn.Dropout(p=attr['dropout_ratio']), channels\n",
        "\n",
        "\n",
        "LAYER_BUILDER_DICT['Convolution'] = build_conv\n",
        "\n",
        "LAYER_BUILDER_DICT['Pooling'] = build_pooling\n",
        "\n",
        "LAYER_BUILDER_DICT['ReLU'] = build_relu\n",
        "\n",
        "LAYER_BUILDER_DICT['Dropout'] = build_dropout\n",
        "\n",
        "LAYER_BUILDER_DICT['BN'] = build_bn\n",
        "\n",
        "LAYER_BUILDER_DICT['InnerProduct'] = build_linear\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ8Zn2OMJDE2"
      },
      "source": [
        "### pytorch_load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyjo7NYJja8T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "\n",
        "class BNInception(nn.Module):\n",
        "    def __init__(self, model_path='model_zoo/bninception/bn_inception.yaml', num_classes=101,\n",
        "                       weight_url='https://yjxiong.blob.core.windows.net/models/bn_inception-9f5701afb96c8044.pth'):\n",
        "        super(BNInception, self).__init__()\n",
        "\n",
        "        with open('/content/drive/MyDrive/ColabNotebooks/videos_imra/bninception/inceptionv3.yaml', 'r') as file:\n",
        "          manifest = yaml.full_load(file)\n",
        "\n",
        "        layers = manifest['layers']\n",
        "\n",
        "        self._channel_dict = dict()\n",
        "\n",
        "        self._op_list = list()\n",
        "        for l in layers:\n",
        "            out_var, op, in_var = parse_expr(l['expr'])\n",
        "            if op != 'Concat':\n",
        "                id, out_name, module, out_channel, in_name = get_basic_layer(l,\n",
        "                                                                3 if len(self._channel_dict) == 0 else self._channel_dict[in_var[0]],\n",
        "                                                                             conv_bias=True)\n",
        "\n",
        "                self._channel_dict[out_name] = out_channel\n",
        "                setattr(self, id, module)\n",
        "                self._op_list.append((id, op, out_name, in_name))\n",
        "            else:\n",
        "                self._op_list.append((id, op, out_var[0], in_var))\n",
        "                channel = sum([self._channel_dict[x] for x in in_var])\n",
        "                self._channel_dict[out_var[0]] = channel\n",
        "\n",
        "        if weight_url is not None:\n",
        "            self.load_state_dict(torch.utils.model_zoo.load_url(weight_url))\n",
        "\n",
        "    def forward(self, input):\n",
        "        data_dict = dict()\n",
        "        data_dict[self._op_list[0][-1]] = input\n",
        "\n",
        "        def get_hook(name):\n",
        "\n",
        "            def hook(m, grad_in, grad_out):\n",
        "                print(name, grad_out[0].data.abs().mean())\n",
        "\n",
        "            return hook\n",
        "        for op in self._op_list:\n",
        "            if op[1] != 'Concat' and op[1] != 'InnerProduct':\n",
        "                data_dict[op[2]] = getattr(self, op[0])(data_dict[op[-1]])\n",
        "                # getattr(self, op[0]).register_backward_hook(get_hook(op[0]))\n",
        "            elif op[1] == 'InnerProduct':\n",
        "                x = data_dict[op[-1]]\n",
        "                data_dict[op[2]] = getattr(self, op[0])(x.view(x.size(0), -1))\n",
        "            else:\n",
        "                try:\n",
        "                    data_dict[op[2]] = torch.cat(tuple(data_dict[x] for x in op[-1]), 1)\n",
        "                except:\n",
        "                    for x in op[-1]:\n",
        "                        print(x,data_dict[x].size())\n",
        "                    raise\n",
        "        return data_dict[self._op_list[-1][2]]\n",
        "\n",
        "\n",
        "class InceptionV3(BNInception):\n",
        "    def __init__(self, model_path=None, num_classes=101,\n",
        "                 weight_url='https://yjxiong.blob.core.windows.net/models/inceptionv3-cuhk-0e09b300b493bc74c.pth'):\n",
        "        super(InceptionV3, self).__init__(model_path=model_path, weight_url=weight_url, num_classes=num_classes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4OhDC3RN1OX"
      },
      "source": [
        "### Python I 3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi4bXaxkJj4G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class MaxPool3dSamePadding(nn.MaxPool3d):\n",
        "\n",
        "    def compute_pad(self, dim, s):\n",
        "        if s % self.stride[dim] == 0:\n",
        "            return max(self.kernel_size[dim] - self.stride[dim], 0)\n",
        "        else:\n",
        "            return max(self.kernel_size[dim] - (s % self.stride[dim]), 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute 'same' padding\n",
        "        (batch, channel, t, h, w) = x.size()\n",
        "        #print t,h,w\n",
        "        out_t = np.ceil(float(t) / float(self.stride[0]))\n",
        "        out_h = np.ceil(float(h) / float(self.stride[1]))\n",
        "        out_w = np.ceil(float(w) / float(self.stride[2]))\n",
        "        #print out_t, out_h, out_w\n",
        "        pad_t = self.compute_pad(0, t)\n",
        "        pad_h = self.compute_pad(1, h)\n",
        "        pad_w = self.compute_pad(2, w)\n",
        "        #print pad_t, pad_h, pad_w\n",
        "\n",
        "        pad_t_f = pad_t // 2\n",
        "        pad_t_b = pad_t - pad_t_f\n",
        "        pad_h_f = pad_h // 2\n",
        "        pad_h_b = pad_h - pad_h_f\n",
        "        pad_w_f = pad_w // 2\n",
        "        pad_w_b = pad_w - pad_w_f\n",
        "\n",
        "        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n",
        "        #print x.size()\n",
        "        #print pad\n",
        "        x = F.pad(x, pad)\n",
        "        return super(MaxPool3dSamePadding, self).forward(x)\n",
        "\n",
        "\n",
        "class Unit3D(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels,\n",
        "                 output_channels,\n",
        "                 kernel_shape=(1, 1, 1),\n",
        "                 stride=(1, 1, 1),\n",
        "                 padding=0,\n",
        "                 activation_fn=F.relu,\n",
        "                 use_batch_norm=True,\n",
        "                 use_bias=False,\n",
        "                 name='unit_3d'):\n",
        "\n",
        "        \"\"\"Initializes Unit3D module.\"\"\"\n",
        "        super(Unit3D, self).__init__()\n",
        "\n",
        "        self._output_channels = output_channels\n",
        "        self._kernel_shape = kernel_shape\n",
        "        self._stride = stride\n",
        "        self._use_batch_norm = use_batch_norm\n",
        "        self._activation_fn = activation_fn\n",
        "        self._use_bias = use_bias\n",
        "        self.name = name\n",
        "        self.padding = padding\n",
        "\n",
        "        self.conv3d = nn.Conv3d(in_channels=in_channels,\n",
        "                                out_channels=self._output_channels,\n",
        "                                kernel_size=self._kernel_shape,\n",
        "                                stride=self._stride,\n",
        "                                padding=0, # we always want padding to be 0 here. We will dynamically pad based on input size in forward function\n",
        "                                bias=self._use_bias)\n",
        "\n",
        "        if self._use_batch_norm:\n",
        "            self.bn = nn.BatchNorm3d(self._output_channels, eps=0.001, momentum=0.01)\n",
        "\n",
        "    def compute_pad(self, dim, s):\n",
        "        if s % self._stride[dim] == 0:\n",
        "            return max(self._kernel_shape[dim] - self._stride[dim], 0)\n",
        "        else:\n",
        "            return max(self._kernel_shape[dim] - (s % self._stride[dim]), 0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute 'same' padding\n",
        "        (batch, channel, t, h, w) = x.size()\n",
        "        #print t,h,w\n",
        "        out_t = np.ceil(float(t) / float(self._stride[0]))\n",
        "        out_h = np.ceil(float(h) / float(self._stride[1]))\n",
        "        out_w = np.ceil(float(w) / float(self._stride[2]))\n",
        "        #print out_t, out_h, out_w\n",
        "        pad_t = self.compute_pad(0, t)\n",
        "        pad_h = self.compute_pad(1, h)\n",
        "        pad_w = self.compute_pad(2, w)\n",
        "        #print pad_t, pad_h, pad_w\n",
        "\n",
        "        pad_t_f = pad_t // 2\n",
        "        pad_t_b = pad_t - pad_t_f\n",
        "        pad_h_f = pad_h // 2\n",
        "        pad_h_b = pad_h - pad_h_f\n",
        "        pad_w_f = pad_w // 2\n",
        "        pad_w_b = pad_w - pad_w_f\n",
        "\n",
        "        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n",
        "        #print x.size()\n",
        "        #print pad\n",
        "        x = F.pad(x, pad)\n",
        "        #print x.size()\n",
        "\n",
        "        x = self.conv3d(x)\n",
        "        if self._use_batch_norm:\n",
        "            x = self.bn(x)\n",
        "        if self._activation_fn is not None:\n",
        "            x = self._activation_fn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, name):\n",
        "        super(InceptionModule, self).__init__()\n",
        "\n",
        "        self.b0 = Unit3D(in_channels=in_channels, output_channels=out_channels[0], kernel_shape=[1, 1, 1], padding=0,\n",
        "                         name=name+'/Branch_0/Conv3d_0a_1x1')\n",
        "        self.b1a = Unit3D(in_channels=in_channels, output_channels=out_channels[1], kernel_shape=[1, 1, 1], padding=0,\n",
        "                          name=name+'/Branch_1/Conv3d_0a_1x1')\n",
        "        self.b1b = Unit3D(in_channels=out_channels[1], output_channels=out_channels[2], kernel_shape=[3, 3, 3],\n",
        "                          name=name+'/Branch_1/Conv3d_0b_3x3')\n",
        "        self.b2a = Unit3D(in_channels=in_channels, output_channels=out_channels[3], kernel_shape=[1, 1, 1], padding=0,\n",
        "                          name=name+'/Branch_2/Conv3d_0a_1x1')\n",
        "        self.b2b = Unit3D(in_channels=out_channels[3], output_channels=out_channels[4], kernel_shape=[3, 3, 3],\n",
        "                          name=name+'/Branch_2/Conv3d_0b_3x3')\n",
        "        self.b3a = MaxPool3dSamePadding(kernel_size=[3, 3, 3],\n",
        "                                stride=(1, 1, 1), padding=0)\n",
        "        self.b3b = Unit3D(in_channels=in_channels, output_channels=out_channels[5], kernel_shape=[1, 1, 1], padding=0,\n",
        "                          name=name+'/Branch_3/Conv3d_0b_1x1')\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, x):\n",
        "        b0 = self.b0(x)\n",
        "        b1 = self.b1b(self.b1a(x))\n",
        "        b2 = self.b2b(self.b2a(x))\n",
        "        b3 = self.b3b(self.b3a(x))\n",
        "        return torch.cat([b0,b1,b2,b3], dim=1)\n",
        "\n",
        "\n",
        "class InceptionI3d(nn.Module):\n",
        "    \"\"\"Inception-v1 I3D architecture.\n",
        "    The model is introduced in:\n",
        "        Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\n",
        "        Joao Carreira, Andrew Zisserman\n",
        "        https://arxiv.org/pdf/1705.07750v1.pdf.\n",
        "    See also the Inception architecture, introduced in:\n",
        "        Going deeper with convolutions\n",
        "        Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n",
        "        Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n",
        "        http://arxiv.org/pdf/1409.4842v1.pdf.\n",
        "    \"\"\"\n",
        "\n",
        "    # Endpoints of the model in order. During construction, all the endpoints up\n",
        "    # to a designated `final_endpoint` are returned in a dictionary as the\n",
        "    # second return value.\n",
        "    VALID_ENDPOINTS = (\n",
        "        'Conv3d_1a_7x7',\n",
        "        'MaxPool3d_2a_3x3',\n",
        "        'Conv3d_2b_1x1',\n",
        "        'Conv3d_2c_3x3',\n",
        "        'MaxPool3d_3a_3x3',\n",
        "        'Mixed_3b',\n",
        "        'Mixed_3c',\n",
        "        'MaxPool3d_4a_3x3',\n",
        "        'Mixed_4b',\n",
        "        'Mixed_4c',\n",
        "        'Mixed_4d',\n",
        "        'Mixed_4e',\n",
        "        'Mixed_4f',\n",
        "        'MaxPool3d_5a_2x2',\n",
        "        'Mixed_5b',\n",
        "        'Mixed_5c',\n",
        "        'Logits',\n",
        "        'Predictions',\n",
        "    )\n",
        "\n",
        "    def __init__(self, num_classes=400, spatial_squeeze=True,\n",
        "                 final_endpoint='Logits', name='inception_i3d', in_channels=3, dropout_keep_prob=0.5):\n",
        "        \"\"\"Initializes I3D model instance.\n",
        "        Args:\n",
        "          num_classes: The number of outputs in the logit layer (default 400, which\n",
        "              matches the Kinetics dataset).\n",
        "          spatial_squeeze: Whether to squeeze the spatial dimensions for the logits\n",
        "              before returning (default True).\n",
        "          final_endpoint: The model contains many possible endpoints.\n",
        "              `final_endpoint` specifies the last endpoint for the model to be built\n",
        "              up to. In addition to the output at `final_endpoint`, all the outputs\n",
        "              at endpoints up to `final_endpoint` will also be returned, in a\n",
        "              dictionary. `final_endpoint` must be one of\n",
        "              InceptionI3d.VALID_ENDPOINTS (default 'Logits').\n",
        "          name: A string (optional). The name of this module.\n",
        "        Raises:\n",
        "          ValueError: if `final_endpoint` is not recognized.\n",
        "        \"\"\"\n",
        "\n",
        "        if final_endpoint not in self.VALID_ENDPOINTS:\n",
        "            raise ValueError('Unknown final endpoint %s' % final_endpoint)\n",
        "\n",
        "        super(InceptionI3d, self).__init__()\n",
        "        self._num_classes = num_classes\n",
        "        self._spatial_squeeze = spatial_squeeze\n",
        "        self._final_endpoint = final_endpoint\n",
        "        self.logits = None\n",
        "\n",
        "        if self._final_endpoint not in self.VALID_ENDPOINTS:\n",
        "            raise ValueError('Unknown final endpoint %s' % self._final_endpoint)\n",
        "\n",
        "        self.end_points = {}\n",
        "        end_point = 'Conv3d_1a_7x7'\n",
        "        self.end_points[end_point] = Unit3D(in_channels=in_channels, output_channels=64, kernel_shape=[7, 7, 7],\n",
        "                                            stride=(2, 2, 2), padding=(3,3,3),  name=name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_2a_3x3'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Conv3d_2b_1x1'\n",
        "        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=64, kernel_shape=[1, 1, 1], padding=0,\n",
        "                                       name=name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Conv3d_2c_3x3'\n",
        "        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=192, kernel_shape=[3, 3, 3], padding=1,\n",
        "                                       name=name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_3a_3x3'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_3b'\n",
        "        self.end_points[end_point] = InceptionModule(192, [64,96,128,16,32,32], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_3c'\n",
        "        self.end_points[end_point] = InceptionModule(256, [128,128,192,32,96,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_4a_3x3'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(2, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4b'\n",
        "        self.end_points[end_point] = InceptionModule(128+192+96+64, [192,96,208,16,48,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4c'\n",
        "        self.end_points[end_point] = InceptionModule(192+208+48+64, [160,112,224,24,64,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4d'\n",
        "        self.end_points[end_point] = InceptionModule(160+224+64+64, [128,128,256,24,64,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4e'\n",
        "        self.end_points[end_point] = InceptionModule(128+256+64+64, [112,144,288,32,64,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4f'\n",
        "        self.end_points[end_point] = InceptionModule(112+288+64+64, [256,160,320,32,128,128], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_5a_2x2'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(2, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_5b'\n",
        "        self.end_points[end_point] = InceptionModule(256+320+128+128, [256,160,320,32,128,128], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_5c'\n",
        "        self.end_points[end_point] = InceptionModule(256+320+128+128, [384,192,384,48,128,128], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Logits'\n",
        "        self.avg_pool = nn.AvgPool3d(kernel_size=[2, 7, 7],\n",
        "                                     stride=(1, 1, 1))\n",
        "        self.dropout = nn.Dropout(dropout_keep_prob)\n",
        "        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n",
        "                             kernel_shape=[1, 1, 1],\n",
        "                             padding=0,\n",
        "                             activation_fn=None,\n",
        "                             use_batch_norm=False,\n",
        "                             use_bias=True,\n",
        "                             name='logits')\n",
        "\n",
        "        self.build()\n",
        "\n",
        "\n",
        "    def replace_logits(self, num_classes):\n",
        "        self._num_classes = num_classes\n",
        "        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n",
        "                             kernel_shape=[1, 1, 1],\n",
        "                             padding=0,\n",
        "                             activation_fn=None,\n",
        "                             use_batch_norm=False,\n",
        "                             use_bias=True,\n",
        "                             name='logits')\n",
        "\n",
        "    def set_dropout(self, dropout):\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def build(self):\n",
        "        for k in self.end_points.keys():\n",
        "            self.add_module(k, self.end_points[k])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for end_point in self.VALID_ENDPOINTS:\n",
        "            if end_point in self.end_points:\n",
        "                x = self._modules[end_point](x) # use _modules to work with dataparallel\n",
        "\n",
        "        x = self.logits(self.dropout(self.avg_pool(x)))\n",
        "        if self._spatial_squeeze:\n",
        "            logits = x.squeeze(3).squeeze(3)  # tensor B x C x T\n",
        "\n",
        "        # avgpooling along temporal dimension\n",
        "        logits = torch.mean(logits, 2)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        for end_point in self.VALID_ENDPOINTS:\n",
        "            if end_point in self.end_points:\n",
        "                x = self._modules[end_point](x)\n",
        "        return self.avg_pool(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPaF2gv8JHnb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5XnBJLNfq_c"
      },
      "source": [
        "## Model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGM0UMdFftJq"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn.init import normal, constant\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "class TSN(nn.Module):\n",
        "    def __init__(self, num_class, num_segments, modality,\n",
        "                 base_model='resnet101', new_length=None,\n",
        "                 consensus_type='avg', before_softmax=True,\n",
        "                 dropout=0.8, partial_bn=True, use_three_input_channels=False, pretrained_model=None):\n",
        "        super(TSN, self).__init__()\n",
        "        self.arch = base_model\n",
        "        self.modality = modality\n",
        "        self.num_segments = num_segments\n",
        "        self.reshape = True\n",
        "        self.before_softmax = before_softmax\n",
        "        self.dropout = dropout\n",
        "        self.consensus_type = consensus_type\n",
        "        if not before_softmax and consensus_type != 'avg':\n",
        "            raise ValueError(\"Only avg consensus can be used after Softmax\")\n",
        "\n",
        "        if new_length is None:\n",
        "            self.new_length = 1 if modality == \"RGB\" else 5\n",
        "        else:\n",
        "            self.new_length = new_length  # number of consecutive frames contained in a snippet\n",
        "\n",
        "        self.use_three_input_channels = use_three_input_channels\n",
        "\n",
        "        print((\"\"\"Initializing TSN with base model: {}.\n",
        "                TSN Configurations:\n",
        "                input_modality:     {}\n",
        "                num_segments:       {}\n",
        "                new_length:         {}\n",
        "                consensus_module:   {}\n",
        "                dropout_ratio:      {}\n",
        "        \"\"\".format(base_model, self.modality, self.num_segments, self.new_length, consensus_type, self.dropout)))\n",
        "\n",
        "        self._prepare_base_model(base_model, pretrained_model)\n",
        "\n",
        "        if not self.is_3D_architecture:\n",
        "            if base_model != 'Pretrained-Inception-v3':\n",
        "                self._prepare_tsn(num_class)\n",
        "\n",
        "                if self.modality == 'Flow':\n",
        "                    print(\"Converting the ImageNet model to a flow init model\")\n",
        "                    self.base_model = self._construct_flow_model(self.base_model)\n",
        "                    print(\"Done. Flow model ready...\")\n",
        "                elif self.modality == 'RGBDiff':\n",
        "                    print(\"Converting the ImageNet model to RGB+Diff init model\")\n",
        "                    self.base_model = self._construct_diff_model(self.base_model)\n",
        "                    print(\"Done. RGBDiff model ready.\")\n",
        "            else:\n",
        "                if self.modality == 'Flow':\n",
        "                    print(\"Converting the ImageNet model to a flow init model\")\n",
        "                    self.base_model = self._construct_flow_model(self.base_model)\n",
        "                    print(\"Done. Flow model ready...\")\n",
        "                elif self.modality == 'RGBDiff':\n",
        "                    print(\"Converting the ImageNet model to RGB+Diff init model\")\n",
        "                    self.base_model = self._construct_diff_model(self.base_model)\n",
        "                    print(\"Done. RGBDiff model ready.\")\n",
        "\n",
        "                if pretrained_model is not None:\n",
        "                    print('loading pretrained model weights from {}'.format(pretrained_model))\n",
        "                    state_dict = torch.load(pretrained_model)\n",
        "                    for k, v in state_dict.items():\n",
        "                        state_dict[k] = torch.squeeze(v, dim=0)\n",
        "                    self.base_model.load_state_dict(state_dict)\n",
        "                self._prepare_tsn(num_class)\n",
        "        else:\n",
        "            self._prepare_tsn(num_class)\n",
        "\n",
        "        self.consensus = ConsensusModule(consensus_type)\n",
        "\n",
        "        if not self.before_softmax:\n",
        "            self.softmax = nn.Softmax()\n",
        "\n",
        "        self._enable_pbn = partial_bn\n",
        "        if partial_bn:\n",
        "            self.partialBN(True)\n",
        "\n",
        "    def _prepare_tsn(self, num_class):\n",
        "        if self.arch == 'Inception3D':\n",
        "            self.base_model.set_dropout(self.dropout)\n",
        "            self.base_model.replace_logits(num_class)\n",
        "            self.new_fc = None\n",
        "        else:\n",
        "            if self.arch == 'Pretrained-Inception-v3':\n",
        "\n",
        "                setattr(self.base_model, 'top_cls_drop', nn.Dropout(p=self.dropout))\n",
        "                feature_dim = getattr(self.base_model, self.base_model.last_layer_name).in_features\n",
        "                setattr(self.base_model, self.base_model.last_layer_name, nn.Linear(feature_dim, num_class))\n",
        "                self.new_fc = None\n",
        "            elif self.arch == 'alexnet':\n",
        "                feature_dim = self.base_model.classifier_layers[self.base_model.last_fc_key].in_features\n",
        "                self.base_model.classifier_layers[self.base_model.last_fc_key] = nn.Dropout(p=self.dropout)\n",
        "                self.new_fc = nn.Linear(feature_dim, num_class)\n",
        "            else:\n",
        "                feature_dim = getattr(self.base_model, self.base_model.last_layer_name).in_features\n",
        "                if self.dropout == 0:\n",
        "                    setattr(self.base_model, self.base_model.last_layer_name, nn.Linear(feature_dim, num_class))\n",
        "                    self.new_fc = None\n",
        "                else:\n",
        "                    setattr(self.base_model, self.base_model.last_layer_name, nn.Dropout(p=self.dropout))\n",
        "                    self.new_fc = nn.Linear(feature_dim, num_class)\n",
        "\n",
        "            std = 0.001\n",
        "            if self.new_fc is None:\n",
        "                normal(getattr(self.base_model, self.base_model.last_layer_name).weight, 0, std)\n",
        "                constant(getattr(self.base_model, self.base_model.last_layer_name).bias, 0)\n",
        "            else:\n",
        "                normal(self.new_fc.weight, 0, std)\n",
        "                constant(self.new_fc.bias, 0)\n",
        "\n",
        "    def _prepare_base_model(self, base_model, pretrained_model=None):\n",
        "        if base_model == 'Inception3D':\n",
        "            if self.modality == 'RGB' or self.use_three_input_channels:\n",
        "                self.base_model = InceptionI3d(num_classes=400, in_channels=3,\n",
        "                                               dropout_keep_prob=self.dropout)\n",
        "            else:\n",
        "                assert (self.modality == 'Flow')\n",
        "                self.base_model = InceptionI3d(num_classes=400, in_channels=2,\n",
        "                                               dropout_keep_prob=self.dropout)\n",
        "\n",
        "            if pretrained_model is not None:\n",
        "                print('loading pretrained model weights from {}'.format(pretrained_model))\n",
        "                state_dict = torch.load(pretrained_model)\n",
        "                self.base_model.load_state_dict(state_dict)\n",
        "\n",
        "            self.input_size = 224\n",
        "            self.input_mean = [0.485, 0.456, 0.406]\n",
        "            self.input_std = [0.229, 0.224, 0.225]\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [0.5]\n",
        "                self.input_std = [np.mean(self.input_std)]\n",
        "        elif base_model == 'Pretrained-Inception-v3':\n",
        "          model_path = '/content/drive/MyDrive/ColabNotebooks/videos_imra/bninception/inceptionv3.yaml'\n",
        "          if os.path.isfile(model_path):\n",
        "            print('Model Path:', model_path)\n",
        "            self.base_model = InceptionV3(model_path=model_path, weight_url=None)\n",
        "          else:\n",
        "            print('Model file does not exist:', model_path)\n",
        "\n",
        "            self.base_model.last_layer_name = 'fc_action'\n",
        "            self.input_size = 299\n",
        "            self.input_mean = [0.5]\n",
        "            self.input_std = [0.5]\n",
        "        elif base_model == '3D-Resnet-34':\n",
        "            import resnet\n",
        "            shortcut_type = 'A'\n",
        "            sample_size = 112\n",
        "            sample_duration = 16\n",
        "\n",
        "            self.base_model = resnet.resnet34(\n",
        "                num_classes=400,\n",
        "                shortcut_type=shortcut_type,\n",
        "                sample_size=sample_size,\n",
        "                sample_duration=sample_duration)\n",
        "            self.base_model.last_layer_name = 'fc'\n",
        "            self.input_size = train_opts.sample_size\n",
        "            self.input_mean = [0.485, 0.456, 0.406]\n",
        "            self.input_std = [0.229, 0.224, 0.225]\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [0.5]\n",
        "                self.input_std = [np.mean(self.input_std)]\n",
        "\n",
        "            if pretrained_model is not None:\n",
        "                print('loading pretrained model weights from {}'.format(pretrained_model))\n",
        "                pretrain = torch.load(pretrained_model)\n",
        "                assert pretrain['arch'] == \"resnet-34\"\n",
        "                base_dict = {'.'.join(k.split('.')[1:]): v for k, v in list(pretrain['state_dict'].items())}\n",
        "                self.base_model.load_state_dict(base_dict)\n",
        "        elif base_model == \"alexnet\":\n",
        "            self.base_model = getattr(torchvision.models, base_model)(True)\n",
        "            self.base_model.last_layer_name = None\n",
        "            self.base_model.classifier_layers = getattr(getattr(self.base_model, '_modules')['classifier'], '_modules')\n",
        "            self.base_model.last_fc_key = '6'\n",
        "\n",
        "            self.input_size = 224\n",
        "            self.input_mean = [0.485, 0.456, 0.406]\n",
        "            self.input_std = [0.229, 0.224, 0.225]\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [0.5]\n",
        "                self.input_std = [np.mean(self.input_std)]\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                self.input_mean = [0.485, 0.456, 0.406] + [0] * 3 * self.new_length\n",
        "                self.input_std = self.input_std + [np.mean(self.input_std) * 2] * 3 * self.new_length\n",
        "        elif 'resnet' in base_model or 'vgg' in base_model:\n",
        "            self.base_model = getattr(torchvision.models, base_model)(True)\n",
        "            self.base_model.last_layer_name = 'fc'\n",
        "            self.input_size = 224\n",
        "            self.input_mean = [0.485, 0.456, 0.406]\n",
        "            self.input_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [0.5]\n",
        "                self.input_std = [np.mean(self.input_std)]\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                self.input_mean = [0.485, 0.456, 0.406] + [0] * 3 * self.new_length\n",
        "                self.input_std = self.input_std + [np.mean(self.input_std) * 2] * 3 * self.new_length\n",
        "        elif base_model == 'BNInception':\n",
        "            import tf_model_zoo  # clone tf_model_zoo repository for this to work!\n",
        "                                 #  (see original repository at https://github.com/yjxiong/tsn-pytorch)\n",
        "            self.base_model = getattr(tf_model_zoo, base_model)()\n",
        "            self.base_model.last_layer_name = 'fc'\n",
        "            self.input_size = 224\n",
        "            self.input_mean = [104, 117, 128]\n",
        "            self.input_std = [1]\n",
        "\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [128]\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                self.input_mean = self.input_mean * (1 + self.new_length)\n",
        "\n",
        "        elif 'inception' in base_model:\n",
        "            print(base_model)\n",
        "            import tf_model_zoo\n",
        "            self.base_model = getattr(tf_model_zoo, base_model)()\n",
        "            self.base_model.last_layer_name = 'classif'\n",
        "            self.input_size = 299\n",
        "            self.input_mean = [0.5]\n",
        "            self.input_std = [0.5]\n",
        "        else:\n",
        "            raise ValueError('Unknown base model: {}'.format(base_model))\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        \"\"\"\n",
        "        Override the default train() to freeze the BN parameters\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        super(TSN, self).train(mode)\n",
        "        count = 0\n",
        "        if self._enable_pbn:\n",
        "            # print(\"Freezing BatchNorm2D except the first one.\")\n",
        "            for m in self.base_model.modules():\n",
        "                if isinstance(m, nn.BatchNorm2d):\n",
        "                    count += 1\n",
        "                    if count >= (2 if self._enable_pbn else 1):\n",
        "                        m.eval()\n",
        "\n",
        "                        # shutdown update in frozen mode\n",
        "                        m.weight.requires_grad = False\n",
        "                        m.bias.requires_grad = False\n",
        "\n",
        "    def partialBN(self, enable):\n",
        "        self._enable_pbn = enable\n",
        "\n",
        "    def get_optim_policies(self):\n",
        "        first_conv_weight = []\n",
        "        first_conv_bias = []\n",
        "        normal_weight = []\n",
        "        normal_bias = []\n",
        "        bn = []\n",
        "\n",
        "        conv_cnt = 0\n",
        "        bn_cnt = 0\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Conv1d):\n",
        "                ps = list(m.parameters())\n",
        "                conv_cnt += 1\n",
        "                if conv_cnt == 1:\n",
        "                    first_conv_weight.append(ps[0])\n",
        "                    if len(ps) == 2:\n",
        "                        first_conv_bias.append(ps[1])\n",
        "                else:\n",
        "                    normal_weight.append(ps[0])\n",
        "                    if len(ps) == 2:\n",
        "                        normal_bias.append(ps[1])\n",
        "            elif isinstance(m, torch.nn.Linear):\n",
        "                ps = list(m.parameters())\n",
        "                normal_weight.append(ps[0])\n",
        "                if len(ps) == 2:\n",
        "                    normal_bias.append(ps[1])\n",
        "\n",
        "            elif isinstance(m, torch.nn.BatchNorm1d):\n",
        "                bn.extend(list(m.parameters()))\n",
        "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
        "                bn_cnt += 1\n",
        "                # later BN's are frozen\n",
        "                if not self._enable_pbn or bn_cnt == 1:\n",
        "                    bn.extend(list(m.parameters()))\n",
        "            elif len(m._modules) == 0:\n",
        "                if len(list(m.parameters())) > 0:\n",
        "                    raise ValueError(\"New atomic module type: {}. Need to give it a learning policy\".format(type(m)))\n",
        "\n",
        "        return [\n",
        "            {'params': first_conv_weight, 'lr_mult': 5 if self.modality == 'Flow' else 1, 'decay_mult': 1,\n",
        "             'name': \"first_conv_weight\"},\n",
        "            {'params': first_conv_bias, 'lr_mult': 10 if self.modality == 'Flow' else 2, 'decay_mult': 0,\n",
        "             'name': \"first_conv_bias\"},\n",
        "            {'params': normal_weight, 'lr_mult': 1, 'decay_mult': 1,\n",
        "             'name': \"normal_weight\"},\n",
        "            {'params': normal_bias, 'lr_mult': 2, 'decay_mult': 0,\n",
        "             'name': \"normal_bias\"},\n",
        "            {'params': bn, 'lr_mult': 1, 'decay_mult': 0,\n",
        "             'name': \"BN scale/shift\"},\n",
        "        ]\n",
        "\n",
        "    def forward(self, input):\n",
        "        sample_len = (3 if self.modality == \"RGB\" else 2) * self.new_length\n",
        "\n",
        "        if self.modality == 'RGBDiff':\n",
        "            sample_len = 3 * self.new_length\n",
        "            input = self._get_diff(input)\n",
        "\n",
        "        if self.is_3D_architecture:\n",
        "            input = input.view((-1,) + input.size()[-4:])\n",
        "        else:\n",
        "            input = input.view((-1, sample_len) + input.size()[-2:])\n",
        "\n",
        "        base_out = self.base_model(input)\n",
        "\n",
        "        if self.new_fc is not None:\n",
        "            base_out = self.new_fc(base_out)\n",
        "\n",
        "        if not self.before_softmax:\n",
        "            base_out = self.softmax(base_out)\n",
        "        if self.reshape:\n",
        "            base_out = base_out.view((-1, self.num_segments) + base_out.size()[1:])\n",
        "\n",
        "        output = self.consensus(base_out)\n",
        "        return output.squeeze(1)\n",
        "\n",
        "    def _get_diff(self, input, keep_rgb=False):\n",
        "        input_c = 3 if self.modality in [\"RGB\", \"RGBDiff\"] else 2\n",
        "        input_view = input.view((-1, self.num_segments, self.new_length + 1, input_c,) + input.size()[2:])\n",
        "        if keep_rgb:\n",
        "            new_data = input_view.clone()\n",
        "        else:\n",
        "            new_data = input_view[:, :, 1:, :, :, :].clone()\n",
        "\n",
        "        for x in reversed(list(range(1, self.new_length + 1))):\n",
        "            if keep_rgb:\n",
        "                new_data[:, :, x, :, :, :] = input_view[:, :, x, :, :, :] - input_view[:, :, x - 1, :, :, :]\n",
        "            else:\n",
        "                new_data[:, :, x - 1, :, :, :] = input_view[:, :, x, :, :, :] - input_view[:, :, x - 1, :, :, :]\n",
        "\n",
        "        return new_data\n",
        "\n",
        "    def _construct_flow_model(self, base_model):\n",
        "        # modify the convolution layers\n",
        "        # Torch models are usually defined in a hierarchical way.\n",
        "        # nn.modules.children() return all sub modules in a DFS manner\n",
        "        modules = list(self.base_model.modules())\n",
        "        first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n",
        "        conv_layer = modules[first_conv_idx]\n",
        "        container = modules[first_conv_idx - 1]\n",
        "\n",
        "        # modify parameters, assume the first blob contains the convolution kernels\n",
        "        params = [x.clone() for x in conv_layer.parameters()]\n",
        "        kernel_size = params[0].size()\n",
        "        new_kernel_size = kernel_size[:1] + (2 * self.new_length, ) + kernel_size[2:]\n",
        "        new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n",
        "\n",
        "        new_conv = nn.Conv2d(2 * self.new_length, conv_layer.out_channels,\n",
        "                             conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n",
        "                             bias=True if len(params) == 2 else False)\n",
        "        new_conv.weight.data = new_kernels\n",
        "        if len(params) == 2:\n",
        "            new_conv.bias.data = params[1].data # add bias if neccessary\n",
        "        layer_name = list(container.state_dict().keys())[0][:-7] # remove .weight suffix to get the layer name\n",
        "\n",
        "        # replace the first convlution layer\n",
        "        setattr(container, layer_name, new_conv)\n",
        "        return base_model\n",
        "\n",
        "    def _construct_diff_model(self, base_model, keep_rgb=False):\n",
        "        # modify the convolution layers\n",
        "        # Torch models are usually defined in a hierarchical way.\n",
        "        # nn.modules.children() return all sub modules in a DFS manner\n",
        "        modules = list(self.base_model.modules())\n",
        "        first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n",
        "        conv_layer = modules[first_conv_idx]\n",
        "        container = modules[first_conv_idx - 1]\n",
        "\n",
        "        # modify parameters, assume the first blob contains the convolution kernels\n",
        "        params = [x.clone() for x in conv_layer.parameters()]\n",
        "        kernel_size = params[0].size()\n",
        "        if not keep_rgb:\n",
        "            new_kernel_size = kernel_size[:1] + (3 * self.new_length,) + kernel_size[2:]\n",
        "            new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n",
        "        else:\n",
        "            new_kernel_size = kernel_size[:1] + (3 * self.new_length,) + kernel_size[2:]\n",
        "            new_kernels = torch.cat((params[0].data, params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()),\n",
        "                                    1)\n",
        "            new_kernel_size = kernel_size[:1] + (3 + 3 * self.new_length,) + kernel_size[2:]\n",
        "\n",
        "        new_conv = nn.Conv2d(new_kernel_size[1], conv_layer.out_channels,\n",
        "                             conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n",
        "                             bias=True if len(params) == 2 else False)\n",
        "        new_conv.weight.data = new_kernels\n",
        "        if len(params) == 2:\n",
        "            new_conv.bias.data = params[1].data  # add bias if neccessary\n",
        "        layer_name = list(container.state_dict().keys())[0][:-7]  # remove .weight suffix to get the layer name\n",
        "\n",
        "        # replace the first convolution layer\n",
        "        setattr(container, layer_name, new_conv)\n",
        "        return base_model\n",
        "\n",
        "    @property\n",
        "    def crop_size(self):\n",
        "        return self.input_size\n",
        "\n",
        "    @property\n",
        "    def scale_size(self):\n",
        "        return self.input_size * 256 // 224\n",
        "\n",
        "    @property\n",
        "    def is_3D_architecture(self):\n",
        "        return \"3d\" in self.arch or \"3D\" in self.arch\n",
        "\n",
        "    def get_augmentation(self, do_horizontal_flip=True):\n",
        "        if do_horizontal_flip:\n",
        "            if self.modality == 'RGB':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75, .66]),\n",
        "                                                       GroupRandomHorizontalFlip(is_flow=False)])\n",
        "            elif self.modality == 'Flow':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75]),\n",
        "                                                       GroupRandomHorizontalFlip(is_flow=True)])\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75]),\n",
        "                                                       GroupRandomHorizontalFlip(is_flow=False)])\n",
        "        else:\n",
        "            if self.modality == 'RGB':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75, .66])])\n",
        "            elif self.modality == 'Flow':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75])])\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75])])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62jFMZ1HQavl"
      },
      "source": [
        "# Data Loder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EESrAtUICBCr"
      },
      "source": [
        "## Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8zrkeePB8qB"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import random\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import numbers\n",
        "import math\n",
        "import torch\n",
        "\n",
        "\n",
        "class GroupRandomCrop(object):\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        w, h = img_group[0].size\n",
        "        th, tw = self.size\n",
        "\n",
        "        out_images = list()\n",
        "\n",
        "        x1 = random.randint(0, w - tw)\n",
        "        y1 = random.randint(0, h - th)\n",
        "\n",
        "        for img in img_group:\n",
        "            assert(img.size[0] == w and img.size[1] == h)\n",
        "            if w == tw and h == th:\n",
        "                out_images.append(img)\n",
        "            else:\n",
        "                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n",
        "\n",
        "        return out_images\n",
        "\n",
        "\n",
        "class GroupCenterCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.worker = torchvision.transforms.CenterCrop(size)\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        return [self.worker(img) for img in img_group]\n",
        "\n",
        "\n",
        "class GroupRandomHorizontalFlip(object):\n",
        "    \"\"\"Randomly horizontally flips the given PIL.Image with a probability of 0.5\"\"\"\n",
        "    def __init__(self, is_flow=False):\n",
        "        self.is_flow = is_flow\n",
        "\n",
        "    def __call__(self, img_group, is_flow=False):\n",
        "        v = random.random()\n",
        "        if v < 0.5:\n",
        "            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n",
        "            if self.is_flow:\n",
        "                for i in range(0, len(ret), 2):\n",
        "                    ret[i] = ImageOps.invert(ret[i])  # invert flow pixel values when flipping\n",
        "            return ret\n",
        "        else:\n",
        "            return img_group\n",
        "\n",
        "\n",
        "class GroupNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        rep_mean = self.mean * (tensor.size()[0]//len(self.mean))\n",
        "        rep_std = self.std * (tensor.size()[0]//len(self.std))\n",
        "\n",
        "        # TODO: make efficient\n",
        "        for t, m, s in zip(tensor, rep_mean, rep_std):\n",
        "            t.sub_(m).div_(s)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class GroupScale(object):\n",
        "    \"\"\" Rescales the input PIL.Image to the given 'size'.\n",
        "    'size' will be the size of the smaller edge.\n",
        "    For example, if height > width, then image will be\n",
        "    rescaled to (size * height / width, size)\n",
        "    size: size of the smaller edge\n",
        "    interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
        "        self.worker = torchvision.transforms.Resize(size, interpolation)\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        return [self.worker(img) for img in img_group]\n",
        "\n",
        "\n",
        "class GroupOverSample(object):\n",
        "    \"\"\"Optionally scale, then for each of five crop positions (fixed offsets): crop all images and append them to\n",
        "    the resulting list, also append their flipped versions\"\"\"\n",
        "    def __init__(self, crop_size, scale_size=None):\n",
        "        self.crop_size = crop_size if not isinstance(crop_size, int) else (crop_size, crop_size)\n",
        "\n",
        "        if scale_size is not None:\n",
        "            self.scale_worker = GroupScale(scale_size)\n",
        "        else:\n",
        "            self.scale_worker = None\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        if self.scale_worker is not None:\n",
        "            img_group = self.scale_worker(img_group)\n",
        "\n",
        "        image_w, image_h = img_group[0].size\n",
        "        crop_w, crop_h = self.crop_size\n",
        "\n",
        "        offsets = GroupMultiScaleCrop.fill_fix_offset(False, image_w, image_h, crop_w, crop_h)\n",
        "        oversample_group = list()\n",
        "        for o_w, o_h in offsets:\n",
        "            normal_group = list()\n",
        "            flip_group = list()\n",
        "            for i, img in enumerate(img_group):\n",
        "                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n",
        "                normal_group.append(crop)\n",
        "                flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "                if img.mode == 'L' and i % 2 == 0:\n",
        "                    flip_group.append(ImageOps.invert(flip_crop))\n",
        "                else:\n",
        "                    flip_group.append(flip_crop)\n",
        "\n",
        "            oversample_group.extend(normal_group)\n",
        "            oversample_group.extend(flip_group)\n",
        "        return oversample_group\n",
        "\n",
        "\n",
        "class GroupMultiScaleCrop(object):\n",
        "    \"\"\"Crop then resize. Crop size is determined randomly based on scales & max_distort. Crop position is determined\n",
        "    randomly or may be a random one of several fixed choices\"\"\"\n",
        "    def __init__(self, input_size, scales=None, max_distort=1, fix_crop=True, more_fix_crop=True):\n",
        "        self.scales = scales if scales is not None else [1, .875, .75, .66]\n",
        "        self.max_distort = max_distort\n",
        "        self.fix_crop = fix_crop\n",
        "        self.more_fix_crop = more_fix_crop\n",
        "        self.input_size = input_size if not isinstance(input_size, int) else [input_size, input_size]\n",
        "        self.interpolation = Image.BILINEAR\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        im_size = img_group[0].size\n",
        "\n",
        "        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size(im_size)\n",
        "        crop_img_group = [img.crop((offset_w, offset_h, offset_w + crop_w, offset_h + crop_h)) for img in img_group]\n",
        "        ret_img_group = [img.resize((self.input_size[0], self.input_size[1]), self.interpolation)\n",
        "                         for img in crop_img_group]\n",
        "        return ret_img_group\n",
        "\n",
        "    def _sample_crop_size(self, im_size):\n",
        "        image_w, image_h = im_size[0], im_size[1]\n",
        "\n",
        "        # find a crop size\n",
        "        base_size = min(image_w, image_h)\n",
        "        crop_sizes = [int(base_size * x) for x in self.scales]\n",
        "        crop_h = [self.input_size[1] if abs(x - self.input_size[1]) < 3 else x for x in crop_sizes]\n",
        "        crop_w = [self.input_size[0] if abs(x - self.input_size[0]) < 3 else x for x in crop_sizes]\n",
        "\n",
        "        pairs = []\n",
        "        for i, h in enumerate(crop_h):\n",
        "            for j, w in enumerate(crop_w):\n",
        "                if abs(i - j) <= self.max_distort:\n",
        "                    pairs.append((w, h))\n",
        "\n",
        "        crop_pair = random.choice(pairs)\n",
        "        if not self.fix_crop:\n",
        "            w_offset = random.randint(0, image_w - crop_pair[0])\n",
        "            h_offset = random.randint(0, image_h - crop_pair[1])\n",
        "        else:\n",
        "            w_offset, h_offset = self._sample_fix_offset(image_w, image_h, crop_pair[0], crop_pair[1])\n",
        "\n",
        "        return crop_pair[0], crop_pair[1], w_offset, h_offset\n",
        "\n",
        "    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):\n",
        "        offsets = self.fill_fix_offset(self.more_fix_crop, image_w, image_h, crop_w, crop_h)\n",
        "        return random.choice(offsets)\n",
        "\n",
        "    @staticmethod\n",
        "    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):\n",
        "        \"\"\"Choices of cropping an image of the given size (crop_w, crop_h) from the original image\"\"\"\n",
        "        w_step = (image_w - crop_w) // 4\n",
        "        h_step = (image_h - crop_h) // 4\n",
        "\n",
        "        ret = list()\n",
        "        ret.append((0, 0))  # upper left\n",
        "        ret.append((4 * w_step, 0))  # upper right\n",
        "        ret.append((0, 4 * h_step))  # lower left\n",
        "        ret.append((4 * w_step, 4 * h_step))  # lower right\n",
        "        ret.append((2 * w_step, 2 * h_step))  # center\n",
        "\n",
        "        if more_fix_crop:\n",
        "            ret.append((0, 2 * h_step))  # center left\n",
        "            ret.append((4 * w_step, 2 * h_step))  # center right\n",
        "            ret.append((2 * w_step, 4 * h_step))  # lower center\n",
        "            ret.append((2 * w_step, 0 * h_step))  # upper center\n",
        "\n",
        "            ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n",
        "            ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n",
        "            ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n",
        "            ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "class GroupRandomSizedCrop(object):\n",
        "    \"\"\"Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the original size\n",
        "    and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio (then resize)\n",
        "    This is popularly used to train the Inception networks\n",
        "    size: size of the smaller edge\n",
        "    interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        for attempt in range(10):\n",
        "            area = img_group[0].size[0] * img_group[0].size[1]\n",
        "            target_area = random.uniform(0.08, 1.0) * area\n",
        "            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n",
        "\n",
        "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
        "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
        "\n",
        "            if random.random() < 0.5:\n",
        "                w, h = h, w\n",
        "\n",
        "            if w <= img_group[0].size[0] and h <= img_group[0].size[1]:\n",
        "                x1 = random.randint(0, img_group[0].size[0] - w)\n",
        "                y1 = random.randint(0, img_group[0].size[1] - h)\n",
        "                found = True\n",
        "                break\n",
        "        else:\n",
        "            found = False\n",
        "            x1 = 0\n",
        "            y1 = 0\n",
        "\n",
        "        if found:\n",
        "            out_group = list()\n",
        "            for img in img_group:\n",
        "                img = img.crop((x1, y1, x1 + w, y1 + h))\n",
        "                assert(img.size == (w, h))\n",
        "                out_group.append(img.resize((self.size, self.size), self.interpolation))\n",
        "            return out_group\n",
        "        else:\n",
        "            # Fallback\n",
        "            scale = GroupScale(self.size, interpolation=self.interpolation)\n",
        "            crop = GroupRandomCrop(self.size)\n",
        "            return crop(scale(img_group))\n",
        "\n",
        "\n",
        "class Stack(object):\n",
        "\n",
        "    def __init__(self, roll=False):\n",
        "        self.roll = roll\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        if img_group[0].mode == 'L':\n",
        "            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n",
        "        elif img_group[0].mode == 'RGB':\n",
        "            if self.roll:\n",
        "                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n",
        "            else:\n",
        "                return np.concatenate(img_group, axis=2)\n",
        "\n",
        "\n",
        "class ToTorchFormatTensor(object):\n",
        "    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n",
        "    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \"\"\"\n",
        "    def __init__(self, div=True):\n",
        "        self.div = div\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        if isinstance(pic, np.ndarray):\n",
        "            # handle numpy array\n",
        "            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n",
        "        else:\n",
        "            # handle PIL Image\n",
        "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
        "            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n",
        "            # put it from HWC to CHW format\n",
        "            # yikes, this transpose takes 80% of the loading time/CPU\n",
        "            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
        "        return img.float().div(255) if self.div else img.float()\n",
        "\n",
        "\n",
        "class IdentityTransform(object):\n",
        "\n",
        "    def __call__(self, data):\n",
        "        return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqZLND_2CGSE"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp3yGpi1-xny"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "import torch\n",
        "from torchvision.transforms import ToTensor, Compose\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "\n",
        "\n",
        "class VideoRecord(object):\n",
        "    def __init__(self, row):\n",
        "        self._data = row\n",
        "        self.frame_count = 0\n",
        "\n",
        "    @property\n",
        "    def trial(self):\n",
        "        return self._data[0]\n",
        "\n",
        "    @property\n",
        "    def num_frames(self):  # number of frames if sampled at full temporal resolution (30 fps)\n",
        "        return int(self._data[1])\n",
        "\n",
        "    @property\n",
        "    def root_path(self):\n",
        "      return self._data[2]\n",
        "\n",
        "    @property\n",
        "    def label(self):\n",
        "        return int(self._data[3])\n",
        "\n",
        "class TSNDataSet(data.Dataset):\n",
        "    def __init__(self, list_of_list_files,\n",
        "                 num_segments=3, new_length=1, modality='RGB',\n",
        "                 image_tmpl='img_{:05d}.jpg', transform=None, normalize=None,\n",
        "                 random_shift=True, test_mode=False,\n",
        "                 video_sampling_step=3,\n",
        "                 return_3D_tensor=False, return_three_channels=False,\n",
        "                 preload_to_RAM=False, return_trial_id=False):\n",
        "\n",
        "\n",
        "        self.list_of_list_files = list_of_list_files\n",
        "        self.num_segments = num_segments\n",
        "        self.new_length = new_length  # number of consecutive frames contained in a snippet\n",
        "        self.modality = modality\n",
        "        self.image_tmpl = image_tmpl\n",
        "        self.transform = transform\n",
        "        self.normalize = normalize\n",
        "        self.random_shift = random_shift\n",
        "        self.test_mode = test_mode\n",
        "\n",
        "        self.video_sampling_step = video_sampling_step\n",
        "        self.return_3D_tensor = return_3D_tensor\n",
        "        self.return_three_channels = return_three_channels\n",
        "        self.preload_to_RAM = preload_to_RAM\n",
        "        self.return_trial_id = return_trial_id\n",
        "\n",
        "        if self.modality == 'RGBDiff':\n",
        "            self.new_length += 1# Diff needs one more image to calculate diff\n",
        "\n",
        "        self._parse_list_files()\n",
        "\n",
        "        if self.preload_to_RAM:\n",
        "            self._preload_images()\n",
        "\n",
        "    def _load_image(self, directory, idx):\n",
        "        if self.modality == 'RGB' or self.modality == 'RGBDiff':\n",
        "            return [Image.open(os.path.join(directory, self.image_tmpl.format(idx + 1))).convert('RGB')]\n",
        "            # extracted images are numbered from 1 to N (instead of 0 to N-1)\n",
        "        elif self.modality == 'Flow':\n",
        "            x_img = Image.open(os.path.join(directory, self.image_tmpl.format('x', idx + 1))).convert('L')\n",
        "            y_img = Image.open(os.path.join(directory, self.image_tmpl.format('y', idx + 1))).convert('L')\n",
        "\n",
        "            return [x_img, y_img]\n",
        "\n",
        "    def _parse_list_files(self):\n",
        "        self.video_list = []\n",
        "        for list_file in self.list_of_list_files:\n",
        "            video_list = [VideoRecord(x.strip().split(',')) for x in open(list_file)]\n",
        "            self.video_list += video_list\n",
        "        for record in self.video_list:\n",
        "            frame_count = record.num_frames // self.video_sampling_step\n",
        "            try:\n",
        "                # check whether last frame is there (sometimes gets lost during the extraction process)\n",
        "                self._load_image(os.path.join(record.root_path), frame_count - 1)\n",
        "            except FileNotFoundError:\n",
        "                frame_count = frame_count - 1\n",
        "            record.frame_count = frame_count\n",
        "\n",
        "    def _preload_images(self):\n",
        "        self.image_data = {}\n",
        "        for record in self.video_list:\n",
        "            print(\"Loading images for {}...\".format(record.trial))\n",
        "            images = []\n",
        "            img_dir = os.path.join(record.root_path)\n",
        "            for p in range(0, record.frame_count):\n",
        "                images.extend(self._load_image(img_dir, p))\n",
        "            self.image_data[record.trial] = images\n",
        "\n",
        "    def _sample_indices(self, record):\n",
        "        \"\"\"\n",
        "\n",
        "        :param record: VideoRecord\n",
        "        :return: list\n",
        "        \"\"\"\n",
        "        average_duration = (record.frame_count - self.new_length + 1) // self.num_segments\n",
        "        if average_duration > 0:\n",
        "            offsets = np.multiply(list(range(self.num_segments)), average_duration) + randint(average_duration, size=self.num_segments)\n",
        "        elif record.frame_count > self.num_segments:\n",
        "            offsets = np.sort(randint(record.frame_count - self.new_length + 1, size=self.num_segments))\n",
        "        else:\n",
        "            offsets = np.zeros((self.num_segments,))\n",
        "        return offsets\n",
        "\n",
        "    def _get_val_indices(self, record):\n",
        "        if record.frame_count > self.num_segments + self.new_length - 1:\n",
        "            tick = (record.frame_count - self.new_length + 1) / float(self.num_segments)\n",
        "            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n",
        "        else:\n",
        "            offsets = np.zeros((self.num_segments,))\n",
        "        return offsets\n",
        "\n",
        "    def _get_test_indices(self, record):\n",
        "        tick = (record.frame_count - self.new_length + 1) / float(self.num_segments)\n",
        "        offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n",
        "        return offsets\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        record = self.video_list[index]\n",
        "\n",
        "        if not self.test_mode:\n",
        "            segment_indices = self._sample_indices(record) if self.random_shift else self._get_val_indices(record)\n",
        "        else:\n",
        "            segment_indices = self._get_test_indices(record)\n",
        "\n",
        "        return self.get(record, segment_indices)\n",
        "\n",
        "    def _get_snippet(self, record, seg_ind):\n",
        "        snippet = list()\n",
        "        p = int(seg_ind)\n",
        "        for _ in range(self.new_length):\n",
        "            if self.preload_to_RAM:\n",
        "                if self.modality == 'RGB' or self.modality == 'RGBDiff':\n",
        "                    seg_imgs = self.image_data[record.trial][p: p + 1]\n",
        "                elif self.modality == 'Flow':\n",
        "                    idx = p * 2\n",
        "                    seg_imgs = self.image_data[record.trial][idx: idx + 2]\n",
        "            else:\n",
        "                img_dir = os.path.join(record.root_path)\n",
        "                seg_imgs = self._load_image(img_dir, p)\n",
        "            snippet.extend(seg_imgs)\n",
        "            if p < (record.frame_count - 1):\n",
        "                p += 1\n",
        "        return snippet\n",
        "\n",
        "    def get(self, record, indices):\n",
        "\n",
        "        images = list()\n",
        "        for seg_ind in indices:\n",
        "            images.extend(self._get_snippet(record, seg_ind))\n",
        "\n",
        "        if self.return_3D_tensor:\n",
        "            images = self.transform(images)\n",
        "            images = [ToTensor()(img) for img in images]\n",
        "            if self.modality == 'RGB':\n",
        "                images = torch.stack(images, 0)\n",
        "            elif self.modality == 'Flow':\n",
        "                _images = []\n",
        "                if self.return_three_channels:\n",
        "                    for i in range(len(images) // 2):\n",
        "                        image_dummy = (images[i] + images[i + 1]) / 2\n",
        "                        _images.append(torch.cat([images[i], images[i + 1], image_dummy], 0))\n",
        "                else:\n",
        "                    for i in range(len(images) // 2):\n",
        "                        _images.append(torch.cat([images[i], images[i + 1]], 0))\n",
        "                images = torch.stack(_images, 0)\n",
        "            images = self.normalize(images)\n",
        "            images = images.view(((-1, self.new_length) + images.size()[-3:]))\n",
        "            images = images.permute(0, 2, 1, 3, 4)\n",
        "            process_data = images\n",
        "        else:\n",
        "            transform = Compose([\n",
        "                self.transform,\n",
        "                Stack(roll=False),\n",
        "                ToTensor(),\n",
        "                self.normalize,\n",
        "            ])\n",
        "            process_data = transform(images)\n",
        "\n",
        "        target = record.label\n",
        "\n",
        "        if self.return_trial_id:\n",
        "            trial_id = record.trial.split('_')[-2]\n",
        "            print(\"Trial ID:\", trial_id)\n",
        "            return trial_id, process_data, target\n",
        "        else:\n",
        "            return process_data, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "consensus_type = 'avg'\n",
        "model = TSN(2, 10, 'RGB', base_model='Inception3D', new_length=64,\n",
        "            consensus_type='avg', before_softmax=True, dropout=0.7, partial_bn=False,\n",
        "            use_three_input_channels=False, pretrained_model='/content/drive/MyDrive/ColabNotebooks/videos_imra/rgb_imagenet.pt')\n",
        "\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "\n",
        "train_lists = ['/content/drive/MyDrive/ColabNotebooks/videos_imra/data_1.csv']\n",
        "\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "\n",
        "# Define your transformations\n",
        "normalize = GroupNormalize(model.input_mean, model.input_std)\n",
        "train_augmentation = model.get_augmentation(True)\n",
        "\n",
        "train_set = TSNDataSet(\n",
        "    list_of_list_files=train_lists,\n",
        "    num_segments=10,\n",
        "    new_length=64,\n",
        "    modality='RGB',\n",
        "    image_tmpl='frame_{:05d}.jpg',\n",
        "    transform=train_augmentation,\n",
        "    normalize=normalize,\n",
        "    random_shift=True,\n",
        "    test_mode=False,\n",
        "    video_sampling_step=3,\n",
        "    return_3D_tensor=True,\n",
        "    return_three_channels=False,\n",
        "    preload_to_RAM=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnVJyhJbaQHK",
        "outputId": "8ce9aef9-7a44-4aec-b1ce-e86a4cdd13f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing TSN with base model: Inception3D.\n",
            "                TSN Configurations:\n",
            "                input_modality:     RGB\n",
            "                num_segments:       10\n",
            "                new_length:         64\n",
            "                consensus_module:   avg\n",
            "                dropout_ratio:      0.7\n",
            "        \n",
            "loading pretrained model weights from /content/drive/MyDrive/ColabNotebooks/videos_imra/rgb_imagenet.pt\n",
            "Loading images for ﻿Expert_dV_008_interupted...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "Wr01c_DBIZnk",
        "outputId": "159d0745-a683-48cd-a043-32fab8ef0835"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-30ae0b2de9e8>\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-30ae0b2de9e8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Number of TPUs available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mnum_processes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Create data loaders here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch_xla.core.xla_model' has no attribute 'get_global_size'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "\n",
        "def train(rank, device, train_loader):\n",
        "    # Create model and optimizer inside the training function to ensure it runs on the TPU device\n",
        "    model = model().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(0, 1200):\n",
        "        train_loss = AverageMeter()\n",
        "        train_acc = AverageMeter()\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data, target = batch\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            batch_size = target.size(0)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            xm.mark_step()\n",
        "\n",
        "            train_loss.update(loss.item(), batch_size)\n",
        "            _output = torch.nn.Softmax(dim=1)(output)\n",
        "            _, predicted = torch.max(_output.data, 1)\n",
        "            acc = (predicted == target).sum().item() / batch_size\n",
        "            train_acc.update(acc, batch_size)\n",
        "\n",
        "            # Print verbose output every 'print_freq' batches\n",
        "            print_freq = 10\n",
        "            if batch_idx % print_freq == 0:\n",
        "                print(\"Rank {}, Epoch [{}/{}], Batch [{}/{}], Train loss: {:.4f}, Train acc: {:.3f}\".format(\n",
        "                    rank, epoch + 1, 1200, batch_idx + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
        "\n",
        "            if (epoch + 1) % 1200 == 0:\n",
        "                print(\"Rank {}: Epoch {}: Train loss: {:.4f} Train acc: {:.3f}\".format(\n",
        "                    rank, epoch + 1, train_loss.avg, train_acc.avg))\n",
        "\n",
        "            output_folder = '/content/drive/MyDrive/ColabNotebooks/videos_imra'\n",
        "            name = \"model_rank_\" + str(rank) + \"_epoch_\" + str(epoch)\n",
        "            model_file = os.path.join(output_folder, name + \".pth.tar\")\n",
        "            state = {'epoch': epoch + 1, 'arch': 'YourModel', 'state_dict': model.state_dict()}\n",
        "            torch.save(state, model_file)\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Initialize XLA devices\n",
        "    devices = xm.xla_device()\n",
        "\n",
        "    # Number of TPUs available\n",
        "    num_processes = xm.get_global_size()\n",
        "\n",
        "    # Create data loaders here\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size= 2, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Use multi-threading to train on multiple TPUs\n",
        "    xmp.spawn(train, args=(train_loader,), nprocs=num_processes, start_method='spawn')\n",
        "\n",
        "    # Check on how many TPUs the code is working on\n",
        "    print(\"Number of TPUs:\", xm.get_global_size())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "\n",
        "def run(model):\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(train_set,\n",
        "                                                                      num_replicas = xm.xrt_world_size(),\n",
        "                                                                      rank         = xm.get_ordinal(),\n",
        "                                                                      shuffle      = True)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_set,\n",
        "                                                batch_size  = 128,\n",
        "                                                sampler     = train_sampler,\n",
        "                                                num_workers = 4,\n",
        "                                                pin_memory  = True)\n",
        "\n",
        "  mx    = xmp.MpModelWrapper(model)\n",
        "  device = xm.xla_device()\n",
        "  model  = mx.to(device)\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  #criterion = torch.nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=(0.00001*xm.xrt_world_size()))\n",
        "  para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "\n",
        "  for epoch in range(0, 120):\n",
        "          train_loss = AverageMeter()\n",
        "          train_acc = AverageMeter()\n",
        "          model.train()\n",
        "          for batch_idx, batch in enumerate(para_loader):\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              data, target = batch\n",
        "              batch_size = target.size(0)\n",
        "              data = data.to(device)\n",
        "              target = target.to(device)\n",
        "\n",
        "              output = model(data)\n",
        "              loss = criterion(output, target)\n",
        "              loss.backward()\n",
        "              xm.optimizer_step(optimizer, barrier = True)\n",
        "\n",
        "              train_loss.update(loss.item(), batch_size)\n",
        "              _output = torch.nn.Softmax(dim=1)(output)\n",
        "              #criterion = torch.nn.BCEWithLogitsLoss()\n",
        "              _, predicted = torch.max(_output.data, 1)\n",
        "              acc = (predicted == target).sum().item() / batch_size\n",
        "              train_acc.update(acc, batch_size)\n",
        "\n",
        "          if (epoch + 1) %  epoch == 120 - 1:  # eval\n",
        "              xm.master_print(\"Epoch {}: Train loss: {train_loss.avg:.4f} Train acc: {train_acc.avg:.3f} \".format(\n",
        "                  epoch, train_loss=train_loss, train_acc=train_acc))\n",
        "\n",
        "          if (epoch + 1) % 10 == 0 or epoch == 1999 - 1:  # save\n",
        "              name = \"model_\" + str(epoch)\n",
        "              model_file = os.path.join('/content/drive/MyDrive/ColabNotebooks/videos_imra/models', name + \".pth.tar\")\n",
        "              state = {'epoch': epoch + 1,\n",
        "                      'arch': 1,\n",
        "                      'state_dict': model.state_dict(),\n",
        "                      }\n",
        "              torch.save(state, model_file)\n",
        "              xm.master_print(\"Saved model to \" + model_file)"
      ],
      "metadata": {
        "id": "Hs_uMReaZOXd"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "def _mp_fn(rank):\n",
        "    device = xm.xla_device()\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    run(model, model, train_set)\n",
        "\n",
        "# Spawn 50 processes to utilize 50 TPU workers\n",
        "xmp.spawn(_mp_fn, nprocs=50, start_method='fork')"
      ],
      "metadata": {
        "id": "MKoKczktZNFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "MptIxFScZF2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z5mG9cptcXBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}