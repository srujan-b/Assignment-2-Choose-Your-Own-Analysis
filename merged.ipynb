{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srujan-b/Assignment-2-Choose-Your-Own-Analysis/blob/main/merged.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51kyjylRd2GA",
        "outputId": "fb7dd665-a495-405b-e499-e09cc129763e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBUIgy8np_fu",
        "outputId": "6bec5d57-f081-4afc-d31e-071c8c5c6cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ColabNotebooks/videos_imra\n"
          ]
        }
      ],
      "source": [
        "cd '/content/drive/MyDrive/ColabNotebooks/videos_imra/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOXVEN8ld3V5"
      },
      "source": [
        "## ConsensusModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4OOXlrfMdkjl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class SegmentConsensus(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input_tensor, consensus_type, dim=1):\n",
        "        if consensus_type == 'avg':\n",
        "            output = input_tensor.mean(dim=dim, keepdim=True)\n",
        "        elif consensus_type == 'identity':\n",
        "            output = input_tensor\n",
        "        else:\n",
        "            output = None\n",
        "        ctx.save_for_backward(input_tensor)\n",
        "        ctx.consensus_type = consensus_type\n",
        "        ctx.dim = dim\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input_tensor, = ctx.saved_tensors\n",
        "        consensus_type = ctx.consensus_type\n",
        "        dim = ctx.dim\n",
        "        if consensus_type == 'avg':\n",
        "            grad_in = grad_output.expand(input_tensor.size()) / float(input_tensor.size(dim))\n",
        "        elif consensus_type == 'identity':\n",
        "            grad_in = grad_output\n",
        "        else:\n",
        "            grad_in = None\n",
        "        return grad_in, None, None\n",
        "\n",
        "class ConsensusModule(torch.nn.Module):\n",
        "    def __init__(self, consensus_type, dim=1):\n",
        "        super(ConsensusModule, self).__init__()\n",
        "        self.consensus_type = consensus_type if consensus_type != 'rnn' else 'identity'\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, input):\n",
        "        return SegmentConsensus.apply(input, self.consensus_type, self.dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eDLqwsN0XS6U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "yEpftr14XTSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from sys import stderr\n",
        "\n",
        "\n",
        "def log(file, msg):\n",
        "    \"\"\"Log a message.\n",
        "\n",
        "    :param file: File object to which the message will be written.\n",
        "    :param msg:  Message to log (str).\n",
        "    \"\"\"\n",
        "    print(time.strftime(\"[%d.%m.%Y %H:%M:%S]: \"), msg, file=stderr)\n",
        "    file.write(time.strftime(\"[%d.%m.%Y %H:%M:%S]: \") + msg + os.linesep)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n"
      ],
      "metadata": {
        "id": "Zc6vVZRmXVFi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_orkXCGd7fS"
      },
      "source": [
        "## Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QZiGIh8weCyU"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import random\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import numbers\n",
        "import math\n",
        "import torch\n",
        "\n",
        "\n",
        "class GroupRandomCrop(object):\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        w, h = img_group[0].size\n",
        "        th, tw = self.size\n",
        "\n",
        "        out_images = list()\n",
        "\n",
        "        x1 = random.randint(0, w - tw)\n",
        "        y1 = random.randint(0, h - th)\n",
        "\n",
        "        for img in img_group:\n",
        "            assert(img.size[0] == w and img.size[1] == h)\n",
        "            if w == tw and h == th:\n",
        "                out_images.append(img)\n",
        "            else:\n",
        "                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n",
        "\n",
        "        return out_images\n",
        "\n",
        "\n",
        "class GroupCenterCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.worker = torchvision.transforms.CenterCrop(size)\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        return [self.worker(img) for img in img_group]\n",
        "\n",
        "\n",
        "class GroupRandomHorizontalFlip(object):\n",
        "    \"\"\"Randomly horizontally flips the given PIL.Image with a probability of 0.5\"\"\"\n",
        "    def __init__(self, is_flow=False):\n",
        "        self.is_flow = is_flow\n",
        "\n",
        "    def __call__(self, img_group, is_flow=False):\n",
        "        v = random.random()\n",
        "        if v < 0.5:\n",
        "            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n",
        "            if self.is_flow:\n",
        "                for i in range(0, len(ret), 2):\n",
        "                    ret[i] = ImageOps.invert(ret[i])  # invert flow pixel values when flipping\n",
        "            return ret\n",
        "        else:\n",
        "            return img_group\n",
        "\n",
        "\n",
        "class GroupNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        rep_mean = self.mean * (tensor.size()[0]//len(self.mean))\n",
        "        rep_std = self.std * (tensor.size()[0]//len(self.std))\n",
        "\n",
        "        # TODO: make efficient\n",
        "        for t, m, s in zip(tensor, rep_mean, rep_std):\n",
        "            t.sub_(m).div_(s)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class GroupScale(object):\n",
        "    \"\"\" Rescales the input PIL.Image to the given 'size'.\n",
        "    'size' will be the size of the smaller edge.\n",
        "    For example, if height > width, then image will be\n",
        "    rescaled to (size * height / width, size)\n",
        "    size: size of the smaller edge\n",
        "    interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
        "        self.worker = torchvision.transforms.Resize(size, interpolation)\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        return [self.worker(img) for img in img_group]\n",
        "\n",
        "\n",
        "class GroupOverSample(object):\n",
        "    \"\"\"Optionally scale, then for each of five crop positions (fixed offsets): crop all images and append them to\n",
        "    the resulting list, also append their flipped versions\"\"\"\n",
        "    def __init__(self, crop_size, scale_size=None):\n",
        "        self.crop_size = crop_size if not isinstance(crop_size, int) else (crop_size, crop_size)\n",
        "\n",
        "        if scale_size is not None:\n",
        "            self.scale_worker = GroupScale(scale_size)\n",
        "        else:\n",
        "            self.scale_worker = None\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        if self.scale_worker is not None:\n",
        "            img_group = self.scale_worker(img_group)\n",
        "\n",
        "        image_w, image_h = img_group[0].size\n",
        "        crop_w, crop_h = self.crop_size\n",
        "\n",
        "        offsets = GroupMultiScaleCrop.fill_fix_offset(False, image_w, image_h, crop_w, crop_h)\n",
        "        oversample_group = list()\n",
        "        for o_w, o_h in offsets:\n",
        "            normal_group = list()\n",
        "            flip_group = list()\n",
        "            for i, img in enumerate(img_group):\n",
        "                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n",
        "                normal_group.append(crop)\n",
        "                flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "                if img.mode == 'L' and i % 2 == 0:\n",
        "                    flip_group.append(ImageOps.invert(flip_crop))\n",
        "                else:\n",
        "                    flip_group.append(flip_crop)\n",
        "\n",
        "            oversample_group.extend(normal_group)\n",
        "            oversample_group.extend(flip_group)\n",
        "        return oversample_group\n",
        "\n",
        "\n",
        "class GroupMultiScaleCrop(object):\n",
        "    \"\"\"Crop then resize. Crop size is determined randomly based on scales & max_distort. Crop position is determined\n",
        "    randomly or may be a random one of several fixed choices\"\"\"\n",
        "    def __init__(self, input_size, scales=None, max_distort=1, fix_crop=True, more_fix_crop=True):\n",
        "        self.scales = scales if scales is not None else [1, .875, .75, .66]\n",
        "        self.max_distort = max_distort\n",
        "        self.fix_crop = fix_crop\n",
        "        self.more_fix_crop = more_fix_crop\n",
        "        self.input_size = input_size if not isinstance(input_size, int) else [input_size, input_size]\n",
        "        self.interpolation = Image.BILINEAR\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        im_size = img_group[0].size\n",
        "\n",
        "        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size(im_size)\n",
        "        crop_img_group = [img.crop((offset_w, offset_h, offset_w + crop_w, offset_h + crop_h)) for img in img_group]\n",
        "        ret_img_group = [img.resize((self.input_size[0], self.input_size[1]), self.interpolation)\n",
        "                         for img in crop_img_group]\n",
        "        return ret_img_group\n",
        "\n",
        "    def _sample_crop_size(self, im_size):\n",
        "        image_w, image_h = im_size[0], im_size[1]\n",
        "\n",
        "        # find a crop size\n",
        "        base_size = min(image_w, image_h)\n",
        "        crop_sizes = [int(base_size * x) for x in self.scales]\n",
        "        crop_h = [self.input_size[1] if abs(x - self.input_size[1]) < 3 else x for x in crop_sizes]\n",
        "        crop_w = [self.input_size[0] if abs(x - self.input_size[0]) < 3 else x for x in crop_sizes]\n",
        "\n",
        "        pairs = []\n",
        "        for i, h in enumerate(crop_h):\n",
        "            for j, w in enumerate(crop_w):\n",
        "                if abs(i - j) <= self.max_distort:\n",
        "                    pairs.append((w, h))\n",
        "\n",
        "        crop_pair = random.choice(pairs)\n",
        "        if not self.fix_crop:\n",
        "            w_offset = random.randint(0, image_w - crop_pair[0])\n",
        "            h_offset = random.randint(0, image_h - crop_pair[1])\n",
        "        else:\n",
        "            w_offset, h_offset = self._sample_fix_offset(image_w, image_h, crop_pair[0], crop_pair[1])\n",
        "\n",
        "        return crop_pair[0], crop_pair[1], w_offset, h_offset\n",
        "\n",
        "    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):\n",
        "        offsets = self.fill_fix_offset(self.more_fix_crop, image_w, image_h, crop_w, crop_h)\n",
        "        return random.choice(offsets)\n",
        "\n",
        "    @staticmethod\n",
        "    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):\n",
        "        \"\"\"Choices of cropping an image of the given size (crop_w, crop_h) from the original image\"\"\"\n",
        "        w_step = (image_w - crop_w) // 4\n",
        "        h_step = (image_h - crop_h) // 4\n",
        "\n",
        "        ret = list()\n",
        "        ret.append((0, 0))  # upper left\n",
        "        ret.append((4 * w_step, 0))  # upper right\n",
        "        ret.append((0, 4 * h_step))  # lower left\n",
        "        ret.append((4 * w_step, 4 * h_step))  # lower right\n",
        "        ret.append((2 * w_step, 2 * h_step))  # center\n",
        "\n",
        "        if more_fix_crop:\n",
        "            ret.append((0, 2 * h_step))  # center left\n",
        "            ret.append((4 * w_step, 2 * h_step))  # center right\n",
        "            ret.append((2 * w_step, 4 * h_step))  # lower center\n",
        "            ret.append((2 * w_step, 0 * h_step))  # upper center\n",
        "\n",
        "            ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n",
        "            ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n",
        "            ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n",
        "            ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "class GroupRandomSizedCrop(object):\n",
        "    \"\"\"Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the original size\n",
        "    and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio (then resize)\n",
        "    This is popularly used to train the Inception networks\n",
        "    size: size of the smaller edge\n",
        "    interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        for attempt in range(10):\n",
        "            area = img_group[0].size[0] * img_group[0].size[1]\n",
        "            target_area = random.uniform(0.08, 1.0) * area\n",
        "            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n",
        "\n",
        "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
        "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
        "\n",
        "            if random.random() < 0.5:\n",
        "                w, h = h, w\n",
        "\n",
        "            if w <= img_group[0].size[0] and h <= img_group[0].size[1]:\n",
        "                x1 = random.randint(0, img_group[0].size[0] - w)\n",
        "                y1 = random.randint(0, img_group[0].size[1] - h)\n",
        "                found = True\n",
        "                break\n",
        "        else:\n",
        "            found = False\n",
        "            x1 = 0\n",
        "            y1 = 0\n",
        "\n",
        "        if found:\n",
        "            out_group = list()\n",
        "            for img in img_group:\n",
        "                img = img.crop((x1, y1, x1 + w, y1 + h))\n",
        "                assert(img.size == (w, h))\n",
        "                out_group.append(img.resize((self.size, self.size), self.interpolation))\n",
        "            return out_group\n",
        "        else:\n",
        "            # Fallback\n",
        "            scale = GroupScale(self.size, interpolation=self.interpolation)\n",
        "            crop = GroupRandomCrop(self.size)\n",
        "            return crop(scale(img_group))\n",
        "\n",
        "\n",
        "class Stack(object):\n",
        "\n",
        "    def __init__(self, roll=False):\n",
        "        self.roll = roll\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        if img_group[0].mode == 'L':\n",
        "            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n",
        "        elif img_group[0].mode == 'RGB':\n",
        "            if self.roll:\n",
        "                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n",
        "            else:\n",
        "                return np.concatenate(img_group, axis=2)\n",
        "\n",
        "\n",
        "class ToTorchFormatTensor(object):\n",
        "    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n",
        "    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \"\"\"\n",
        "    def __init__(self, div=True):\n",
        "        self.div = div\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        if isinstance(pic, np.ndarray):\n",
        "            # handle numpy array\n",
        "            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n",
        "        else:\n",
        "            # handle PIL Image\n",
        "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
        "            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n",
        "            # put it from HWC to CHW format\n",
        "            # yikes, this transpose takes 80% of the loading time/CPU\n",
        "            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
        "        return img.float().div(255) if self.div else img.float()\n",
        "\n",
        "\n",
        "class IdentityTransform(object):\n",
        "\n",
        "    def __call__(self, data):\n",
        "        return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-MC5P3_IubC"
      },
      "source": [
        "## Bin Inception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NCErS36JSOX"
      },
      "source": [
        "### layer_factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GPLjiPueJVN4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "LAYER_BUILDER_DICT=dict()\n",
        "\n",
        "\n",
        "def parse_expr(expr):\n",
        "    parts = expr.split('<=')\n",
        "    return parts[0].split(','), parts[1], parts[2].split(',')\n",
        "\n",
        "\n",
        "def get_basic_layer(info, channels=None, conv_bias=False):\n",
        "    id = info['id']\n",
        "    attr = info['attrs'] if 'attrs' in info else list()\n",
        "\n",
        "    out, op, in_vars = parse_expr(info['expr'])\n",
        "    assert(len(out) == 1)\n",
        "    assert(len(in_vars) == 1)\n",
        "    mod, out_channel, = LAYER_BUILDER_DICT[op](attr, channels, conv_bias)\n",
        "\n",
        "    return id, out[0], mod, out_channel, in_vars[0]\n",
        "\n",
        "\n",
        "def build_conv(attr, channels=None, conv_bias=False):\n",
        "    out_channels = attr['num_output']\n",
        "    ks = attr['kernel_size'] if 'kernel_size' in attr else (attr['kernel_h'], attr['kernel_w'])\n",
        "    if 'pad' in attr or 'pad_w' in attr and 'pad_h' in attr:\n",
        "        padding = attr['pad'] if 'pad' in attr else (attr['pad_h'], attr['pad_w'])\n",
        "    else:\n",
        "        padding = 0\n",
        "    if 'stride' in attr or 'stride_w' in attr and 'stride_h' in attr:\n",
        "        stride = attr['stride'] if 'stride' in attr else (attr['stride_h'], attr['stride_w'])\n",
        "    else:\n",
        "        stride = 1\n",
        "\n",
        "    conv = nn.Conv2d(channels, out_channels, ks, stride, padding, bias=conv_bias)\n",
        "\n",
        "    return conv, out_channels\n",
        "\n",
        "\n",
        "def build_pooling(attr, channels=None, conv_bias=False):\n",
        "    method = attr['mode']\n",
        "    pad = attr['pad'] if 'pad' in attr else 0\n",
        "    if method == 'max':\n",
        "        pool = nn.MaxPool2d(attr['kernel_size'], attr['stride'], pad,\n",
        "                            ceil_mode=True) # all Caffe pooling use ceil model\n",
        "    elif method == 'ave':\n",
        "        pool = nn.AvgPool2d(attr['kernel_size'], attr['stride'], pad,\n",
        "                            ceil_mode=True)  # all Caffe pooling use ceil model\n",
        "    else:\n",
        "        raise ValueError(\"Unknown pooling method: {}\".format(method))\n",
        "\n",
        "    return pool, channels\n",
        "\n",
        "\n",
        "def build_relu(attr, channels=None, conv_bias=False):\n",
        "    return nn.ReLU(inplace=True), channels\n",
        "\n",
        "\n",
        "def build_bn(attr, channels=None, conv_bias=False):\n",
        "    return nn.BatchNorm2d(channels, momentum=0.1), channels\n",
        "\n",
        "\n",
        "def build_linear(attr, channels=None, conv_bias=False):\n",
        "    return nn.Linear(channels, attr['num_output']), channels\n",
        "\n",
        "\n",
        "def build_dropout(attr, channels=None, conv_bias=False):\n",
        "    return nn.Dropout(p=attr['dropout_ratio']), channels\n",
        "\n",
        "\n",
        "LAYER_BUILDER_DICT['Convolution'] = build_conv\n",
        "\n",
        "LAYER_BUILDER_DICT['Pooling'] = build_pooling\n",
        "\n",
        "LAYER_BUILDER_DICT['ReLU'] = build_relu\n",
        "\n",
        "LAYER_BUILDER_DICT['Dropout'] = build_dropout\n",
        "\n",
        "LAYER_BUILDER_DICT['BN'] = build_bn\n",
        "\n",
        "LAYER_BUILDER_DICT['InnerProduct'] = build_linear\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ8Zn2OMJDE2"
      },
      "source": [
        "### pytorch_load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dyjo7NYJja8T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "\n",
        "class BNInception(nn.Module):\n",
        "    def __init__(self, model_path='model_zoo/bninception/bn_inception.yaml', num_classes=101,\n",
        "                       weight_url='https://yjxiong.blob.core.windows.net/models/bn_inception-9f5701afb96c8044.pth'):\n",
        "        super(BNInception, self).__init__()\n",
        "\n",
        "        with open('/content/drive/MyDrive/ColabNotebooks/videos_imra/bninception/inceptionv3.yaml', 'r') as file:\n",
        "          manifest = yaml.full_load(file)\n",
        "\n",
        "        layers = manifest['layers']\n",
        "\n",
        "        self._channel_dict = dict()\n",
        "\n",
        "        self._op_list = list()\n",
        "        for l in layers:\n",
        "            out_var, op, in_var = parse_expr(l['expr'])\n",
        "            if op != 'Concat':\n",
        "                id, out_name, module, out_channel, in_name = get_basic_layer(l,\n",
        "                                                                3 if len(self._channel_dict) == 0 else self._channel_dict[in_var[0]],\n",
        "                                                                             conv_bias=True)\n",
        "\n",
        "                self._channel_dict[out_name] = out_channel\n",
        "                setattr(self, id, module)\n",
        "                self._op_list.append((id, op, out_name, in_name))\n",
        "            else:\n",
        "                self._op_list.append((id, op, out_var[0], in_var))\n",
        "                channel = sum([self._channel_dict[x] for x in in_var])\n",
        "                self._channel_dict[out_var[0]] = channel\n",
        "\n",
        "        if weight_url is not None:\n",
        "            self.load_state_dict(torch.utils.model_zoo.load_url(weight_url))\n",
        "\n",
        "    def forward(self, input):\n",
        "        data_dict = dict()\n",
        "        data_dict[self._op_list[0][-1]] = input\n",
        "\n",
        "        def get_hook(name):\n",
        "\n",
        "            def hook(m, grad_in, grad_out):\n",
        "                print(name, grad_out[0].data.abs().mean())\n",
        "\n",
        "            return hook\n",
        "        for op in self._op_list:\n",
        "            if op[1] != 'Concat' and op[1] != 'InnerProduct':\n",
        "                data_dict[op[2]] = getattr(self, op[0])(data_dict[op[-1]])\n",
        "                # getattr(self, op[0]).register_backward_hook(get_hook(op[0]))\n",
        "            elif op[1] == 'InnerProduct':\n",
        "                x = data_dict[op[-1]]\n",
        "                data_dict[op[2]] = getattr(self, op[0])(x.view(x.size(0), -1))\n",
        "            else:\n",
        "                try:\n",
        "                    data_dict[op[2]] = torch.cat(tuple(data_dict[x] for x in op[-1]), 1)\n",
        "                except:\n",
        "                    for x in op[-1]:\n",
        "                        print(x,data_dict[x].size())\n",
        "                    raise\n",
        "        return data_dict[self._op_list[-1][2]]\n",
        "\n",
        "\n",
        "class InceptionV3(BNInception):\n",
        "    def __init__(self, model_path=None, num_classes=101,\n",
        "                 weight_url='https://yjxiong.blob.core.windows.net/models/inceptionv3-cuhk-0e09b300b493bc74c.pth'):\n",
        "        super(InceptionV3, self).__init__(model_path=model_path, weight_url=weight_url, num_classes=num_classes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4OhDC3RN1OX"
      },
      "source": [
        "### Python I 3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gi4bXaxkJj4G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class MaxPool3dSamePadding(nn.MaxPool3d):\n",
        "\n",
        "    def compute_pad(self, dim, s):\n",
        "        if s % self.stride[dim] == 0:\n",
        "            return max(self.kernel_size[dim] - self.stride[dim], 0)\n",
        "        else:\n",
        "            return max(self.kernel_size[dim] - (s % self.stride[dim]), 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute 'same' padding\n",
        "        (batch, channel, t, h, w) = x.size()\n",
        "        #print t,h,w\n",
        "        out_t = np.ceil(float(t) / float(self.stride[0]))\n",
        "        out_h = np.ceil(float(h) / float(self.stride[1]))\n",
        "        out_w = np.ceil(float(w) / float(self.stride[2]))\n",
        "        #print out_t, out_h, out_w\n",
        "        pad_t = self.compute_pad(0, t)\n",
        "        pad_h = self.compute_pad(1, h)\n",
        "        pad_w = self.compute_pad(2, w)\n",
        "        #print pad_t, pad_h, pad_w\n",
        "\n",
        "        pad_t_f = pad_t // 2\n",
        "        pad_t_b = pad_t - pad_t_f\n",
        "        pad_h_f = pad_h // 2\n",
        "        pad_h_b = pad_h - pad_h_f\n",
        "        pad_w_f = pad_w // 2\n",
        "        pad_w_b = pad_w - pad_w_f\n",
        "\n",
        "        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n",
        "        #print x.size()\n",
        "        #print pad\n",
        "        x = F.pad(x, pad)\n",
        "        return super(MaxPool3dSamePadding, self).forward(x)\n",
        "\n",
        "\n",
        "class Unit3D(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels,\n",
        "                 output_channels,\n",
        "                 kernel_shape=(1, 1, 1),\n",
        "                 stride=(1, 1, 1),\n",
        "                 padding=0,\n",
        "                 activation_fn=F.relu,\n",
        "                 use_batch_norm=True,\n",
        "                 use_bias=False,\n",
        "                 name='unit_3d'):\n",
        "\n",
        "        \"\"\"Initializes Unit3D module.\"\"\"\n",
        "        super(Unit3D, self).__init__()\n",
        "\n",
        "        self._output_channels = output_channels\n",
        "        self._kernel_shape = kernel_shape\n",
        "        self._stride = stride\n",
        "        self._use_batch_norm = use_batch_norm\n",
        "        self._activation_fn = activation_fn\n",
        "        self._use_bias = use_bias\n",
        "        self.name = name\n",
        "        self.padding = padding\n",
        "\n",
        "        self.conv3d = nn.Conv3d(in_channels=in_channels,\n",
        "                                out_channels=self._output_channels,\n",
        "                                kernel_size=self._kernel_shape,\n",
        "                                stride=self._stride,\n",
        "                                padding=0, # we always want padding to be 0 here. We will dynamically pad based on input size in forward function\n",
        "                                bias=self._use_bias)\n",
        "\n",
        "        if self._use_batch_norm:\n",
        "            self.bn = nn.BatchNorm3d(self._output_channels, eps=0.001, momentum=0.01)\n",
        "\n",
        "    def compute_pad(self, dim, s):\n",
        "        if s % self._stride[dim] == 0:\n",
        "            return max(self._kernel_shape[dim] - self._stride[dim], 0)\n",
        "        else:\n",
        "            return max(self._kernel_shape[dim] - (s % self._stride[dim]), 0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute 'same' padding\n",
        "        (batch, channel, t, h, w) = x.size()\n",
        "        #print t,h,w\n",
        "        out_t = np.ceil(float(t) / float(self._stride[0]))\n",
        "        out_h = np.ceil(float(h) / float(self._stride[1]))\n",
        "        out_w = np.ceil(float(w) / float(self._stride[2]))\n",
        "        #print out_t, out_h, out_w\n",
        "        pad_t = self.compute_pad(0, t)\n",
        "        pad_h = self.compute_pad(1, h)\n",
        "        pad_w = self.compute_pad(2, w)\n",
        "        #print pad_t, pad_h, pad_w\n",
        "\n",
        "        pad_t_f = pad_t // 2\n",
        "        pad_t_b = pad_t - pad_t_f\n",
        "        pad_h_f = pad_h // 2\n",
        "        pad_h_b = pad_h - pad_h_f\n",
        "        pad_w_f = pad_w // 2\n",
        "        pad_w_b = pad_w - pad_w_f\n",
        "\n",
        "        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n",
        "        #print x.size()\n",
        "        #print pad\n",
        "        x = F.pad(x, pad)\n",
        "        #print x.size()\n",
        "\n",
        "        x = self.conv3d(x)\n",
        "        if self._use_batch_norm:\n",
        "            x = self.bn(x)\n",
        "        if self._activation_fn is not None:\n",
        "            x = self._activation_fn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, name):\n",
        "        super(InceptionModule, self).__init__()\n",
        "\n",
        "        self.b0 = Unit3D(in_channels=in_channels, output_channels=out_channels[0], kernel_shape=[1, 1, 1], padding=0,\n",
        "                         name=name+'/Branch_0/Conv3d_0a_1x1')\n",
        "        self.b1a = Unit3D(in_channels=in_channels, output_channels=out_channels[1], kernel_shape=[1, 1, 1], padding=0,\n",
        "                          name=name+'/Branch_1/Conv3d_0a_1x1')\n",
        "        self.b1b = Unit3D(in_channels=out_channels[1], output_channels=out_channels[2], kernel_shape=[3, 3, 3],\n",
        "                          name=name+'/Branch_1/Conv3d_0b_3x3')\n",
        "        self.b2a = Unit3D(in_channels=in_channels, output_channels=out_channels[3], kernel_shape=[1, 1, 1], padding=0,\n",
        "                          name=name+'/Branch_2/Conv3d_0a_1x1')\n",
        "        self.b2b = Unit3D(in_channels=out_channels[3], output_channels=out_channels[4], kernel_shape=[3, 3, 3],\n",
        "                          name=name+'/Branch_2/Conv3d_0b_3x3')\n",
        "        self.b3a = MaxPool3dSamePadding(kernel_size=[3, 3, 3],\n",
        "                                stride=(1, 1, 1), padding=0)\n",
        "        self.b3b = Unit3D(in_channels=in_channels, output_channels=out_channels[5], kernel_shape=[1, 1, 1], padding=0,\n",
        "                          name=name+'/Branch_3/Conv3d_0b_1x1')\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, x):\n",
        "        b0 = self.b0(x)\n",
        "        b1 = self.b1b(self.b1a(x))\n",
        "        b2 = self.b2b(self.b2a(x))\n",
        "        b3 = self.b3b(self.b3a(x))\n",
        "        return torch.cat([b0,b1,b2,b3], dim=1)\n",
        "\n",
        "\n",
        "class InceptionI3d(nn.Module):\n",
        "    \"\"\"Inception-v1 I3D architecture.\n",
        "    The model is introduced in:\n",
        "        Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\n",
        "        Joao Carreira, Andrew Zisserman\n",
        "        https://arxiv.org/pdf/1705.07750v1.pdf.\n",
        "    See also the Inception architecture, introduced in:\n",
        "        Going deeper with convolutions\n",
        "        Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n",
        "        Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n",
        "        http://arxiv.org/pdf/1409.4842v1.pdf.\n",
        "    \"\"\"\n",
        "\n",
        "    # Endpoints of the model in order. During construction, all the endpoints up\n",
        "    # to a designated `final_endpoint` are returned in a dictionary as the\n",
        "    # second return value.\n",
        "    VALID_ENDPOINTS = (\n",
        "        'Conv3d_1a_7x7',\n",
        "        'MaxPool3d_2a_3x3',\n",
        "        'Conv3d_2b_1x1',\n",
        "        'Conv3d_2c_3x3',\n",
        "        'MaxPool3d_3a_3x3',\n",
        "        'Mixed_3b',\n",
        "        'Mixed_3c',\n",
        "        'MaxPool3d_4a_3x3',\n",
        "        'Mixed_4b',\n",
        "        'Mixed_4c',\n",
        "        'Mixed_4d',\n",
        "        'Mixed_4e',\n",
        "        'Mixed_4f',\n",
        "        'MaxPool3d_5a_2x2',\n",
        "        'Mixed_5b',\n",
        "        'Mixed_5c',\n",
        "        'Logits',\n",
        "        'Predictions',\n",
        "    )\n",
        "\n",
        "    def __init__(self, num_classes=400, spatial_squeeze=True,\n",
        "                 final_endpoint='Logits', name='inception_i3d', in_channels=3, dropout_keep_prob=0.5):\n",
        "        \"\"\"Initializes I3D model instance.\n",
        "        Args:\n",
        "          num_classes: The number of outputs in the logit layer (default 400, which\n",
        "              matches the Kinetics dataset).\n",
        "          spatial_squeeze: Whether to squeeze the spatial dimensions for the logits\n",
        "              before returning (default True).\n",
        "          final_endpoint: The model contains many possible endpoints.\n",
        "              `final_endpoint` specifies the last endpoint for the model to be built\n",
        "              up to. In addition to the output at `final_endpoint`, all the outputs\n",
        "              at endpoints up to `final_endpoint` will also be returned, in a\n",
        "              dictionary. `final_endpoint` must be one of\n",
        "              InceptionI3d.VALID_ENDPOINTS (default 'Logits').\n",
        "          name: A string (optional). The name of this module.\n",
        "        Raises:\n",
        "          ValueError: if `final_endpoint` is not recognized.\n",
        "        \"\"\"\n",
        "\n",
        "        if final_endpoint not in self.VALID_ENDPOINTS:\n",
        "            raise ValueError('Unknown final endpoint %s' % final_endpoint)\n",
        "\n",
        "        super(InceptionI3d, self).__init__()\n",
        "        self._num_classes = num_classes\n",
        "        self._spatial_squeeze = spatial_squeeze\n",
        "        self._final_endpoint = final_endpoint\n",
        "        self.logits = None\n",
        "\n",
        "        if self._final_endpoint not in self.VALID_ENDPOINTS:\n",
        "            raise ValueError('Unknown final endpoint %s' % self._final_endpoint)\n",
        "\n",
        "        self.end_points = {}\n",
        "        end_point = 'Conv3d_1a_7x7'\n",
        "        self.end_points[end_point] = Unit3D(in_channels=in_channels, output_channels=64, kernel_shape=[7, 7, 7],\n",
        "                                            stride=(2, 2, 2), padding=(3,3,3),  name=name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_2a_3x3'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Conv3d_2b_1x1'\n",
        "        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=64, kernel_shape=[1, 1, 1], padding=0,\n",
        "                                       name=name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Conv3d_2c_3x3'\n",
        "        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=192, kernel_shape=[3, 3, 3], padding=1,\n",
        "                                       name=name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_3a_3x3'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_3b'\n",
        "        self.end_points[end_point] = InceptionModule(192, [64,96,128,16,32,32], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_3c'\n",
        "        self.end_points[end_point] = InceptionModule(256, [128,128,192,32,96,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_4a_3x3'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(2, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4b'\n",
        "        self.end_points[end_point] = InceptionModule(128+192+96+64, [192,96,208,16,48,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4c'\n",
        "        self.end_points[end_point] = InceptionModule(192+208+48+64, [160,112,224,24,64,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4d'\n",
        "        self.end_points[end_point] = InceptionModule(160+224+64+64, [128,128,256,24,64,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4e'\n",
        "        self.end_points[end_point] = InceptionModule(128+256+64+64, [112,144,288,32,64,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4f'\n",
        "        self.end_points[end_point] = InceptionModule(112+288+64+64, [256,160,320,32,128,128], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_5a_2x2'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(2, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_5b'\n",
        "        self.end_points[end_point] = InceptionModule(256+320+128+128, [256,160,320,32,128,128], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_5c'\n",
        "        self.end_points[end_point] = InceptionModule(256+320+128+128, [384,192,384,48,128,128], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Logits'\n",
        "        self.avg_pool = nn.AvgPool3d(kernel_size=[2, 7, 7],\n",
        "                                     stride=(1, 1, 1))\n",
        "        self.dropout = nn.Dropout(dropout_keep_prob)\n",
        "        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n",
        "                             kernel_shape=[1, 1, 1],\n",
        "                             padding=0,\n",
        "                             activation_fn=None,\n",
        "                             use_batch_norm=False,\n",
        "                             use_bias=True,\n",
        "                             name='logits')\n",
        "\n",
        "        self.build()\n",
        "\n",
        "\n",
        "    def replace_logits(self, num_classes):\n",
        "        self._num_classes = num_classes\n",
        "        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n",
        "                             kernel_shape=[1, 1, 1],\n",
        "                             padding=0,\n",
        "                             activation_fn=None,\n",
        "                             use_batch_norm=False,\n",
        "                             use_bias=True,\n",
        "                             name='logits')\n",
        "\n",
        "    def set_dropout(self, dropout):\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def build(self):\n",
        "        for k in self.end_points.keys():\n",
        "            self.add_module(k, self.end_points[k])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for end_point in self.VALID_ENDPOINTS:\n",
        "            if end_point in self.end_points:\n",
        "                x = self._modules[end_point](x) # use _modules to work with dataparallel\n",
        "\n",
        "        x = self.logits(self.dropout(self.avg_pool(x)))\n",
        "        if self._spatial_squeeze:\n",
        "            logits = x.squeeze(3).squeeze(3)  # tensor B x C x T\n",
        "\n",
        "        # avgpooling along temporal dimension\n",
        "        logits = torch.mean(logits, 2)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        for end_point in self.VALID_ENDPOINTS:\n",
        "            if end_point in self.end_points:\n",
        "                x = self._modules[end_point](x)\n",
        "        return self.avg_pool(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPaF2gv8JHnb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5XnBJLNfq_c"
      },
      "source": [
        "## Model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pGM0UMdFftJq"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn.init import normal, constant\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "class TSN(nn.Module):\n",
        "    def __init__(self, num_class, num_segments, modality,\n",
        "                 base_model='resnet101', new_length=None,\n",
        "                 consensus_type='avg', before_softmax=True,\n",
        "                 dropout=0.8, partial_bn=True, use_three_input_channels=False, pretrained_model=None):\n",
        "        super(TSN, self).__init__()\n",
        "        self.arch = base_model\n",
        "        self.modality = modality\n",
        "        self.num_segments = num_segments\n",
        "        self.reshape = True\n",
        "        self.before_softmax = before_softmax\n",
        "        self.dropout = dropout\n",
        "        self.consensus_type = consensus_type\n",
        "        if not before_softmax and consensus_type != 'avg':\n",
        "            raise ValueError(\"Only avg consensus can be used after Softmax\")\n",
        "\n",
        "        if new_length is None:\n",
        "            self.new_length = 1 if modality == \"RGB\" else 5\n",
        "        else:\n",
        "            self.new_length = new_length  # number of consecutive frames contained in a snippet\n",
        "\n",
        "        self.use_three_input_channels = use_three_input_channels\n",
        "\n",
        "        print((\"\"\"Initializing TSN with base model: {}.\n",
        "                TSN Configurations:\n",
        "                input_modality:     {}\n",
        "                num_segments:       {}\n",
        "                new_length:         {}\n",
        "                consensus_module:   {}\n",
        "                dropout_ratio:      {}\n",
        "        \"\"\".format(base_model, self.modality, self.num_segments, self.new_length, consensus_type, self.dropout)))\n",
        "\n",
        "        self._prepare_base_model(base_model, pretrained_model)\n",
        "\n",
        "        if not self.is_3D_architecture:\n",
        "            if base_model != 'Pretrained-Inception-v3':\n",
        "                self._prepare_tsn(num_class)\n",
        "\n",
        "                if self.modality == 'Flow':\n",
        "                    print(\"Converting the ImageNet model to a flow init model\")\n",
        "                    self.base_model = self._construct_flow_model(self.base_model)\n",
        "                    print(\"Done. Flow model ready...\")\n",
        "                elif self.modality == 'RGBDiff':\n",
        "                    print(\"Converting the ImageNet model to RGB+Diff init model\")\n",
        "                    self.base_model = self._construct_diff_model(self.base_model)\n",
        "                    print(\"Done. RGBDiff model ready.\")\n",
        "            else:\n",
        "                if self.modality == 'Flow':\n",
        "                    print(\"Converting the ImageNet model to a flow init model\")\n",
        "                    self.base_model = self._construct_flow_model(self.base_model)\n",
        "                    print(\"Done. Flow model ready...\")\n",
        "                elif self.modality == 'RGBDiff':\n",
        "                    print(\"Converting the ImageNet model to RGB+Diff init model\")\n",
        "                    self.base_model = self._construct_diff_model(self.base_model)\n",
        "                    print(\"Done. RGBDiff model ready.\")\n",
        "\n",
        "                if pretrained_model is not None:\n",
        "                    print('loading pretrained model weights from {}'.format(pretrained_model))\n",
        "                    state_dict = torch.load(pretrained_model)\n",
        "                    for k, v in state_dict.items():\n",
        "                        state_dict[k] = torch.squeeze(v, dim=0)\n",
        "                    self.base_model.load_state_dict(state_dict)\n",
        "                self._prepare_tsn(num_class)\n",
        "        else:\n",
        "            self._prepare_tsn(num_class)\n",
        "\n",
        "        self.consensus = ConsensusModule(consensus_type)\n",
        "\n",
        "        if not self.before_softmax:\n",
        "            self.softmax = nn.Softmax()\n",
        "\n",
        "        self._enable_pbn = partial_bn\n",
        "        if partial_bn:\n",
        "            self.partialBN(True)\n",
        "\n",
        "    def _prepare_tsn(self, num_class):\n",
        "        if self.arch == 'Inception3D':\n",
        "            self.base_model.set_dropout(self.dropout)\n",
        "            self.base_model.replace_logits(num_class)\n",
        "            self.new_fc = None\n",
        "        else:\n",
        "            if self.arch == 'Pretrained-Inception-v3':\n",
        "\n",
        "                setattr(self.base_model, 'top_cls_drop', nn.Dropout(p=self.dropout))\n",
        "                feature_dim = getattr(self.base_model, self.base_model.last_layer_name).in_features\n",
        "                setattr(self.base_model, self.base_model.last_layer_name, nn.Linear(feature_dim, num_class))\n",
        "                self.new_fc = None\n",
        "            elif self.arch == 'alexnet':\n",
        "                feature_dim = self.base_model.classifier_layers[self.base_model.last_fc_key].in_features\n",
        "                self.base_model.classifier_layers[self.base_model.last_fc_key] = nn.Dropout(p=self.dropout)\n",
        "                self.new_fc = nn.Linear(feature_dim, num_class)\n",
        "            else:\n",
        "                feature_dim = getattr(self.base_model, self.base_model.last_layer_name).in_features\n",
        "                if self.dropout == 0:\n",
        "                    setattr(self.base_model, self.base_model.last_layer_name, nn.Linear(feature_dim, num_class))\n",
        "                    self.new_fc = None\n",
        "                else:\n",
        "                    setattr(self.base_model, self.base_model.last_layer_name, nn.Dropout(p=self.dropout))\n",
        "                    self.new_fc = nn.Linear(feature_dim, num_class)\n",
        "\n",
        "            std = 0.001\n",
        "            if self.new_fc is None:\n",
        "                normal(getattr(self.base_model, self.base_model.last_layer_name).weight, 0, std)\n",
        "                constant(getattr(self.base_model, self.base_model.last_layer_name).bias, 0)\n",
        "            else:\n",
        "                normal(self.new_fc.weight, 0, std)\n",
        "                constant(self.new_fc.bias, 0)\n",
        "\n",
        "    def _prepare_base_model(self, base_model, pretrained_model=None):\n",
        "        if base_model == 'Inception3D':\n",
        "            if self.modality == 'RGB' or self.use_three_input_channels:\n",
        "                self.base_model = InceptionI3d(num_classes=400, in_channels=3,\n",
        "                                               dropout_keep_prob=self.dropout)\n",
        "            else:\n",
        "                assert (self.modality == 'Flow')\n",
        "                self.base_model = InceptionI3d(num_classes=400, in_channels=2,\n",
        "                                               dropout_keep_prob=self.dropout)\n",
        "\n",
        "            if pretrained_model is not None:\n",
        "                print('loading pretrained model weights from {}'.format(pretrained_model))\n",
        "                state_dict = torch.load(pretrained_model)\n",
        "                self.base_model.load_state_dict(state_dict)\n",
        "\n",
        "            self.input_size = 224\n",
        "            self.input_mean = [0.485, 0.456, 0.406]\n",
        "            self.input_std = [0.229, 0.224, 0.225]\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [0.5]\n",
        "                self.input_std = [np.mean(self.input_std)]\n",
        "        elif base_model == 'Pretrained-Inception-v3':\n",
        "          model_path = '/content/drive/MyDrive/ColabNotebooks/videos_imra/bninception/inceptionv3.yaml'\n",
        "          if os.path.isfile(model_path):\n",
        "            print('Model Path:', model_path)\n",
        "            self.base_model = InceptionV3(model_path=model_path, weight_url=None)\n",
        "          else:\n",
        "            print('Model file does not exist:', model_path)\n",
        "\n",
        "            self.base_model.last_layer_name = 'fc_action'\n",
        "            self.input_size = 299\n",
        "            self.input_mean = [0.5]\n",
        "            self.input_std = [0.5]\n",
        "        elif base_model == '3D-Resnet-34':\n",
        "            import resnet\n",
        "            shortcut_type = 'A'\n",
        "            sample_size = 112\n",
        "            sample_duration = 16\n",
        "\n",
        "            self.base_model = resnet.resnet34(\n",
        "                num_classes=400,\n",
        "                shortcut_type=shortcut_type,\n",
        "                sample_size=sample_size,\n",
        "                sample_duration=sample_duration)\n",
        "            self.base_model.last_layer_name = 'fc'\n",
        "            self.input_size = train_opts.sample_size\n",
        "            self.input_mean = [0.485, 0.456, 0.406]\n",
        "            self.input_std = [0.229, 0.224, 0.225]\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [0.5]\n",
        "                self.input_std = [np.mean(self.input_std)]\n",
        "\n",
        "            if pretrained_model is not None:\n",
        "                print('loading pretrained model weights from {}'.format(pretrained_model))\n",
        "                pretrain = torch.load(pretrained_model)\n",
        "                assert pretrain['arch'] == \"resnet-34\"\n",
        "                base_dict = {'.'.join(k.split('.')[1:]): v for k, v in list(pretrain['state_dict'].items())}\n",
        "                self.base_model.load_state_dict(base_dict)\n",
        "        elif base_model == \"alexnet\":\n",
        "            self.base_model = getattr(torchvision.models, base_model)(True)\n",
        "            self.base_model.last_layer_name = None\n",
        "            self.base_model.classifier_layers = getattr(getattr(self.base_model, '_modules')['classifier'], '_modules')\n",
        "            self.base_model.last_fc_key = '6'\n",
        "\n",
        "            self.input_size = 224\n",
        "            self.input_mean = [0.485, 0.456, 0.406]\n",
        "            self.input_std = [0.229, 0.224, 0.225]\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [0.5]\n",
        "                self.input_std = [np.mean(self.input_std)]\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                self.input_mean = [0.485, 0.456, 0.406] + [0] * 3 * self.new_length\n",
        "                self.input_std = self.input_std + [np.mean(self.input_std) * 2] * 3 * self.new_length\n",
        "        elif 'resnet' in base_model or 'vgg' in base_model:\n",
        "            self.base_model = getattr(torchvision.models, base_model)(True)\n",
        "            self.base_model.last_layer_name = 'fc'\n",
        "            self.input_size = 224\n",
        "            self.input_mean = [0.485, 0.456, 0.406]\n",
        "            self.input_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [0.5]\n",
        "                self.input_std = [np.mean(self.input_std)]\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                self.input_mean = [0.485, 0.456, 0.406] + [0] * 3 * self.new_length\n",
        "                self.input_std = self.input_std + [np.mean(self.input_std) * 2] * 3 * self.new_length\n",
        "        elif base_model == 'BNInception':\n",
        "            import tf_model_zoo  # clone tf_model_zoo repository for this to work!\n",
        "                                 #  (see original repository at https://github.com/yjxiong/tsn-pytorch)\n",
        "            self.base_model = getattr(tf_model_zoo, base_model)()\n",
        "            self.base_model.last_layer_name = 'fc'\n",
        "            self.input_size = 224\n",
        "            self.input_mean = [104, 117, 128]\n",
        "            self.input_std = [1]\n",
        "\n",
        "            if self.modality == 'Flow':\n",
        "                self.input_mean = [128]\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                self.input_mean = self.input_mean * (1 + self.new_length)\n",
        "\n",
        "        elif 'inception' in base_model:\n",
        "            print(base_model)\n",
        "            import tf_model_zoo\n",
        "            self.base_model = getattr(tf_model_zoo, base_model)()\n",
        "            self.base_model.last_layer_name = 'classif'\n",
        "            self.input_size = 299\n",
        "            self.input_mean = [0.5]\n",
        "            self.input_std = [0.5]\n",
        "        else:\n",
        "            raise ValueError('Unknown base model: {}'.format(base_model))\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        \"\"\"\n",
        "        Override the default train() to freeze the BN parameters\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        super(TSN, self).train(mode)\n",
        "        count = 0\n",
        "        if self._enable_pbn:\n",
        "            # print(\"Freezing BatchNorm2D except the first one.\")\n",
        "            for m in self.base_model.modules():\n",
        "                if isinstance(m, nn.BatchNorm2d):\n",
        "                    count += 1\n",
        "                    if count >= (2 if self._enable_pbn else 1):\n",
        "                        m.eval()\n",
        "\n",
        "                        # shutdown update in frozen mode\n",
        "                        m.weight.requires_grad = False\n",
        "                        m.bias.requires_grad = False\n",
        "\n",
        "    def partialBN(self, enable):\n",
        "        self._enable_pbn = enable\n",
        "\n",
        "    def get_optim_policies(self):\n",
        "        first_conv_weight = []\n",
        "        first_conv_bias = []\n",
        "        normal_weight = []\n",
        "        normal_bias = []\n",
        "        bn = []\n",
        "\n",
        "        conv_cnt = 0\n",
        "        bn_cnt = 0\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Conv1d):\n",
        "                ps = list(m.parameters())\n",
        "                conv_cnt += 1\n",
        "                if conv_cnt == 1:\n",
        "                    first_conv_weight.append(ps[0])\n",
        "                    if len(ps) == 2:\n",
        "                        first_conv_bias.append(ps[1])\n",
        "                else:\n",
        "                    normal_weight.append(ps[0])\n",
        "                    if len(ps) == 2:\n",
        "                        normal_bias.append(ps[1])\n",
        "            elif isinstance(m, torch.nn.Linear):\n",
        "                ps = list(m.parameters())\n",
        "                normal_weight.append(ps[0])\n",
        "                if len(ps) == 2:\n",
        "                    normal_bias.append(ps[1])\n",
        "\n",
        "            elif isinstance(m, torch.nn.BatchNorm1d):\n",
        "                bn.extend(list(m.parameters()))\n",
        "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
        "                bn_cnt += 1\n",
        "                # later BN's are frozen\n",
        "                if not self._enable_pbn or bn_cnt == 1:\n",
        "                    bn.extend(list(m.parameters()))\n",
        "            elif len(m._modules) == 0:\n",
        "                if len(list(m.parameters())) > 0:\n",
        "                    raise ValueError(\"New atomic module type: {}. Need to give it a learning policy\".format(type(m)))\n",
        "\n",
        "        return [\n",
        "            {'params': first_conv_weight, 'lr_mult': 5 if self.modality == 'Flow' else 1, 'decay_mult': 1,\n",
        "             'name': \"first_conv_weight\"},\n",
        "            {'params': first_conv_bias, 'lr_mult': 10 if self.modality == 'Flow' else 2, 'decay_mult': 0,\n",
        "             'name': \"first_conv_bias\"},\n",
        "            {'params': normal_weight, 'lr_mult': 1, 'decay_mult': 1,\n",
        "             'name': \"normal_weight\"},\n",
        "            {'params': normal_bias, 'lr_mult': 2, 'decay_mult': 0,\n",
        "             'name': \"normal_bias\"},\n",
        "            {'params': bn, 'lr_mult': 1, 'decay_mult': 0,\n",
        "             'name': \"BN scale/shift\"},\n",
        "        ]\n",
        "\n",
        "    def forward(self, input):\n",
        "        sample_len = (3 if self.modality == \"RGB\" else 2) * self.new_length\n",
        "\n",
        "        if self.modality == 'RGBDiff':\n",
        "            sample_len = 3 * self.new_length\n",
        "            input = self._get_diff(input)\n",
        "\n",
        "        if self.is_3D_architecture:\n",
        "            input = input.view((-1,) + input.size()[-4:])\n",
        "        else:\n",
        "            input = input.view((-1, sample_len) + input.size()[-2:])\n",
        "\n",
        "        base_out = self.base_model(input)\n",
        "\n",
        "        if self.new_fc is not None:\n",
        "            base_out = self.new_fc(base_out)\n",
        "\n",
        "        if not self.before_softmax:\n",
        "            base_out = self.softmax(base_out)\n",
        "        if self.reshape:\n",
        "            base_out = base_out.view((-1, self.num_segments) + base_out.size()[1:])\n",
        "\n",
        "        output = self.consensus(base_out)\n",
        "        return output.squeeze(1)\n",
        "\n",
        "    def _get_diff(self, input, keep_rgb=False):\n",
        "        input_c = 3 if self.modality in [\"RGB\", \"RGBDiff\"] else 2\n",
        "        input_view = input.view((-1, self.num_segments, self.new_length + 1, input_c,) + input.size()[2:])\n",
        "        if keep_rgb:\n",
        "            new_data = input_view.clone()\n",
        "        else:\n",
        "            new_data = input_view[:, :, 1:, :, :, :].clone()\n",
        "\n",
        "        for x in reversed(list(range(1, self.new_length + 1))):\n",
        "            if keep_rgb:\n",
        "                new_data[:, :, x, :, :, :] = input_view[:, :, x, :, :, :] - input_view[:, :, x - 1, :, :, :]\n",
        "            else:\n",
        "                new_data[:, :, x - 1, :, :, :] = input_view[:, :, x, :, :, :] - input_view[:, :, x - 1, :, :, :]\n",
        "\n",
        "        return new_data\n",
        "\n",
        "    def _construct_flow_model(self, base_model):\n",
        "        # modify the convolution layers\n",
        "        # Torch models are usually defined in a hierarchical way.\n",
        "        # nn.modules.children() return all sub modules in a DFS manner\n",
        "        modules = list(self.base_model.modules())\n",
        "        first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n",
        "        conv_layer = modules[first_conv_idx]\n",
        "        container = modules[first_conv_idx - 1]\n",
        "\n",
        "        # modify parameters, assume the first blob contains the convolution kernels\n",
        "        params = [x.clone() for x in conv_layer.parameters()]\n",
        "        kernel_size = params[0].size()\n",
        "        new_kernel_size = kernel_size[:1] + (2 * self.new_length, ) + kernel_size[2:]\n",
        "        new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n",
        "\n",
        "        new_conv = nn.Conv2d(2 * self.new_length, conv_layer.out_channels,\n",
        "                             conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n",
        "                             bias=True if len(params) == 2 else False)\n",
        "        new_conv.weight.data = new_kernels\n",
        "        if len(params) == 2:\n",
        "            new_conv.bias.data = params[1].data # add bias if neccessary\n",
        "        layer_name = list(container.state_dict().keys())[0][:-7] # remove .weight suffix to get the layer name\n",
        "\n",
        "        # replace the first convlution layer\n",
        "        setattr(container, layer_name, new_conv)\n",
        "        return base_model\n",
        "\n",
        "    def _construct_diff_model(self, base_model, keep_rgb=False):\n",
        "        # modify the convolution layers\n",
        "        # Torch models are usually defined in a hierarchical way.\n",
        "        # nn.modules.children() return all sub modules in a DFS manner\n",
        "        modules = list(self.base_model.modules())\n",
        "        first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n",
        "        conv_layer = modules[first_conv_idx]\n",
        "        container = modules[first_conv_idx - 1]\n",
        "\n",
        "        # modify parameters, assume the first blob contains the convolution kernels\n",
        "        params = [x.clone() for x in conv_layer.parameters()]\n",
        "        kernel_size = params[0].size()\n",
        "        if not keep_rgb:\n",
        "            new_kernel_size = kernel_size[:1] + (3 * self.new_length,) + kernel_size[2:]\n",
        "            new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n",
        "        else:\n",
        "            new_kernel_size = kernel_size[:1] + (3 * self.new_length,) + kernel_size[2:]\n",
        "            new_kernels = torch.cat((params[0].data, params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()),\n",
        "                                    1)\n",
        "            new_kernel_size = kernel_size[:1] + (3 + 3 * self.new_length,) + kernel_size[2:]\n",
        "\n",
        "        new_conv = nn.Conv2d(new_kernel_size[1], conv_layer.out_channels,\n",
        "                             conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n",
        "                             bias=True if len(params) == 2 else False)\n",
        "        new_conv.weight.data = new_kernels\n",
        "        if len(params) == 2:\n",
        "            new_conv.bias.data = params[1].data  # add bias if neccessary\n",
        "        layer_name = list(container.state_dict().keys())[0][:-7]  # remove .weight suffix to get the layer name\n",
        "\n",
        "        # replace the first convolution layer\n",
        "        setattr(container, layer_name, new_conv)\n",
        "        return base_model\n",
        "\n",
        "    @property\n",
        "    def crop_size(self):\n",
        "        return self.input_size\n",
        "\n",
        "    @property\n",
        "    def scale_size(self):\n",
        "        return self.input_size * 256 // 224\n",
        "\n",
        "    @property\n",
        "    def is_3D_architecture(self):\n",
        "        return \"3d\" in self.arch or \"3D\" in self.arch\n",
        "\n",
        "    def get_augmentation(self, do_horizontal_flip=True):\n",
        "        if do_horizontal_flip:\n",
        "            if self.modality == 'RGB':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75, .66]),\n",
        "                                                       GroupRandomHorizontalFlip(is_flow=False)])\n",
        "            elif self.modality == 'Flow':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75]),\n",
        "                                                       GroupRandomHorizontalFlip(is_flow=True)])\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75]),\n",
        "                                                       GroupRandomHorizontalFlip(is_flow=False)])\n",
        "        else:\n",
        "            if self.modality == 'RGB':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75, .66])])\n",
        "            elif self.modality == 'Flow':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75])])\n",
        "            elif self.modality == 'RGBDiff':\n",
        "                return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75])])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62jFMZ1HQavl"
      },
      "source": [
        "# Data Loder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EESrAtUICBCr"
      },
      "source": [
        "## Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "g8zrkeePB8qB"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import random\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import numbers\n",
        "import math\n",
        "import torch\n",
        "\n",
        "\n",
        "class GroupRandomCrop(object):\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        w, h = img_group[0].size\n",
        "        th, tw = self.size\n",
        "\n",
        "        out_images = list()\n",
        "\n",
        "        x1 = random.randint(0, w - tw)\n",
        "        y1 = random.randint(0, h - th)\n",
        "\n",
        "        for img in img_group:\n",
        "            assert(img.size[0] == w and img.size[1] == h)\n",
        "            if w == tw and h == th:\n",
        "                out_images.append(img)\n",
        "            else:\n",
        "                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n",
        "\n",
        "        return out_images\n",
        "\n",
        "\n",
        "class GroupCenterCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.worker = torchvision.transforms.CenterCrop(size)\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        return [self.worker(img) for img in img_group]\n",
        "\n",
        "\n",
        "class GroupRandomHorizontalFlip(object):\n",
        "    \"\"\"Randomly horizontally flips the given PIL.Image with a probability of 0.5\"\"\"\n",
        "    def __init__(self, is_flow=False):\n",
        "        self.is_flow = is_flow\n",
        "\n",
        "    def __call__(self, img_group, is_flow=False):\n",
        "        v = random.random()\n",
        "        if v < 0.5:\n",
        "            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n",
        "            if self.is_flow:\n",
        "                for i in range(0, len(ret), 2):\n",
        "                    ret[i] = ImageOps.invert(ret[i])  # invert flow pixel values when flipping\n",
        "            return ret\n",
        "        else:\n",
        "            return img_group\n",
        "\n",
        "\n",
        "class GroupNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        rep_mean = self.mean * (tensor.size()[0]//len(self.mean))\n",
        "        rep_std = self.std * (tensor.size()[0]//len(self.std))\n",
        "\n",
        "        # TODO: make efficient\n",
        "        for t, m, s in zip(tensor, rep_mean, rep_std):\n",
        "            t.sub_(m).div_(s)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class GroupScale(object):\n",
        "    \"\"\" Rescales the input PIL.Image to the given 'size'.\n",
        "    'size' will be the size of the smaller edge.\n",
        "    For example, if height > width, then image will be\n",
        "    rescaled to (size * height / width, size)\n",
        "    size: size of the smaller edge\n",
        "    interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
        "        self.worker = torchvision.transforms.Resize(size, interpolation)\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        return [self.worker(img) for img in img_group]\n",
        "\n",
        "\n",
        "class GroupOverSample(object):\n",
        "    \"\"\"Optionally scale, then for each of five crop positions (fixed offsets): crop all images and append them to\n",
        "    the resulting list, also append their flipped versions\"\"\"\n",
        "    def __init__(self, crop_size, scale_size=None):\n",
        "        self.crop_size = crop_size if not isinstance(crop_size, int) else (crop_size, crop_size)\n",
        "\n",
        "        if scale_size is not None:\n",
        "            self.scale_worker = GroupScale(scale_size)\n",
        "        else:\n",
        "            self.scale_worker = None\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        if self.scale_worker is not None:\n",
        "            img_group = self.scale_worker(img_group)\n",
        "\n",
        "        image_w, image_h = img_group[0].size\n",
        "        crop_w, crop_h = self.crop_size\n",
        "\n",
        "        offsets = GroupMultiScaleCrop.fill_fix_offset(False, image_w, image_h, crop_w, crop_h)\n",
        "        oversample_group = list()\n",
        "        for o_w, o_h in offsets:\n",
        "            normal_group = list()\n",
        "            flip_group = list()\n",
        "            for i, img in enumerate(img_group):\n",
        "                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n",
        "                normal_group.append(crop)\n",
        "                flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "                if img.mode == 'L' and i % 2 == 0:\n",
        "                    flip_group.append(ImageOps.invert(flip_crop))\n",
        "                else:\n",
        "                    flip_group.append(flip_crop)\n",
        "\n",
        "            oversample_group.extend(normal_group)\n",
        "            oversample_group.extend(flip_group)\n",
        "        return oversample_group\n",
        "\n",
        "\n",
        "class GroupMultiScaleCrop(object):\n",
        "    \"\"\"Crop then resize. Crop size is determined randomly based on scales & max_distort. Crop position is determined\n",
        "    randomly or may be a random one of several fixed choices\"\"\"\n",
        "    def __init__(self, input_size, scales=None, max_distort=1, fix_crop=True, more_fix_crop=True):\n",
        "        self.scales = scales if scales is not None else [1, .875, .75, .66]\n",
        "        self.max_distort = max_distort\n",
        "        self.fix_crop = fix_crop\n",
        "        self.more_fix_crop = more_fix_crop\n",
        "        self.input_size = input_size if not isinstance(input_size, int) else [input_size, input_size]\n",
        "        self.interpolation = Image.BILINEAR\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "\n",
        "        im_size = img_group[0].size\n",
        "\n",
        "        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size(im_size)\n",
        "        crop_img_group = [img.crop((offset_w, offset_h, offset_w + crop_w, offset_h + crop_h)) for img in img_group]\n",
        "        ret_img_group = [img.resize((self.input_size[0], self.input_size[1]), self.interpolation)\n",
        "                         for img in crop_img_group]\n",
        "        return ret_img_group\n",
        "\n",
        "    def _sample_crop_size(self, im_size):\n",
        "        image_w, image_h = im_size[0], im_size[1]\n",
        "\n",
        "        # find a crop size\n",
        "        base_size = min(image_w, image_h)\n",
        "        crop_sizes = [int(base_size * x) for x in self.scales]\n",
        "        crop_h = [self.input_size[1] if abs(x - self.input_size[1]) < 3 else x for x in crop_sizes]\n",
        "        crop_w = [self.input_size[0] if abs(x - self.input_size[0]) < 3 else x for x in crop_sizes]\n",
        "\n",
        "        pairs = []\n",
        "        for i, h in enumerate(crop_h):\n",
        "            for j, w in enumerate(crop_w):\n",
        "                if abs(i - j) <= self.max_distort:\n",
        "                    pairs.append((w, h))\n",
        "\n",
        "        crop_pair = random.choice(pairs)\n",
        "        if not self.fix_crop:\n",
        "            w_offset = random.randint(0, image_w - crop_pair[0])\n",
        "            h_offset = random.randint(0, image_h - crop_pair[1])\n",
        "        else:\n",
        "            w_offset, h_offset = self._sample_fix_offset(image_w, image_h, crop_pair[0], crop_pair[1])\n",
        "\n",
        "        return crop_pair[0], crop_pair[1], w_offset, h_offset\n",
        "\n",
        "    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):\n",
        "        offsets = self.fill_fix_offset(self.more_fix_crop, image_w, image_h, crop_w, crop_h)\n",
        "        return random.choice(offsets)\n",
        "\n",
        "    @staticmethod\n",
        "    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):\n",
        "        \"\"\"Choices of cropping an image of the given size (crop_w, crop_h) from the original image\"\"\"\n",
        "        w_step = (image_w - crop_w) // 4\n",
        "        h_step = (image_h - crop_h) // 4\n",
        "\n",
        "        ret = list()\n",
        "        ret.append((0, 0))  # upper left\n",
        "        ret.append((4 * w_step, 0))  # upper right\n",
        "        ret.append((0, 4 * h_step))  # lower left\n",
        "        ret.append((4 * w_step, 4 * h_step))  # lower right\n",
        "        ret.append((2 * w_step, 2 * h_step))  # center\n",
        "\n",
        "        if more_fix_crop:\n",
        "            ret.append((0, 2 * h_step))  # center left\n",
        "            ret.append((4 * w_step, 2 * h_step))  # center right\n",
        "            ret.append((2 * w_step, 4 * h_step))  # lower center\n",
        "            ret.append((2 * w_step, 0 * h_step))  # upper center\n",
        "\n",
        "            ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n",
        "            ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n",
        "            ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n",
        "            ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "class GroupRandomSizedCrop(object):\n",
        "    \"\"\"Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the original size\n",
        "    and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio (then resize)\n",
        "    This is popularly used to train the Inception networks\n",
        "    size: size of the smaller edge\n",
        "    interpolation: Default: PIL.Image.BILINEAR\n",
        "    \"\"\"\n",
        "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
        "        self.size = size\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        for attempt in range(10):\n",
        "            area = img_group[0].size[0] * img_group[0].size[1]\n",
        "            target_area = random.uniform(0.08, 1.0) * area\n",
        "            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n",
        "\n",
        "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
        "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
        "\n",
        "            if random.random() < 0.5:\n",
        "                w, h = h, w\n",
        "\n",
        "            if w <= img_group[0].size[0] and h <= img_group[0].size[1]:\n",
        "                x1 = random.randint(0, img_group[0].size[0] - w)\n",
        "                y1 = random.randint(0, img_group[0].size[1] - h)\n",
        "                found = True\n",
        "                break\n",
        "        else:\n",
        "            found = False\n",
        "            x1 = 0\n",
        "            y1 = 0\n",
        "\n",
        "        if found:\n",
        "            out_group = list()\n",
        "            for img in img_group:\n",
        "                img = img.crop((x1, y1, x1 + w, y1 + h))\n",
        "                assert(img.size == (w, h))\n",
        "                out_group.append(img.resize((self.size, self.size), self.interpolation))\n",
        "            return out_group\n",
        "        else:\n",
        "            # Fallback\n",
        "            scale = GroupScale(self.size, interpolation=self.interpolation)\n",
        "            crop = GroupRandomCrop(self.size)\n",
        "            return crop(scale(img_group))\n",
        "\n",
        "\n",
        "class Stack(object):\n",
        "\n",
        "    def __init__(self, roll=False):\n",
        "        self.roll = roll\n",
        "\n",
        "    def __call__(self, img_group):\n",
        "        if img_group[0].mode == 'L':\n",
        "            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n",
        "        elif img_group[0].mode == 'RGB':\n",
        "            if self.roll:\n",
        "                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n",
        "            else:\n",
        "                return np.concatenate(img_group, axis=2)\n",
        "\n",
        "\n",
        "class ToTorchFormatTensor(object):\n",
        "    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n",
        "    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \"\"\"\n",
        "    def __init__(self, div=True):\n",
        "        self.div = div\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        if isinstance(pic, np.ndarray):\n",
        "            # handle numpy array\n",
        "            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n",
        "        else:\n",
        "            # handle PIL Image\n",
        "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
        "            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n",
        "            # put it from HWC to CHW format\n",
        "            # yikes, this transpose takes 80% of the loading time/CPU\n",
        "            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
        "        return img.float().div(255) if self.div else img.float()\n",
        "\n",
        "\n",
        "class IdentityTransform(object):\n",
        "\n",
        "    def __call__(self, data):\n",
        "        return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqZLND_2CGSE"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dp3yGpi1-xny"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "import torch\n",
        "from torchvision.transforms import ToTensor, Compose\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "\n",
        "\n",
        "class VideoRecord(object):\n",
        "    def __init__(self, row):\n",
        "        self._data = row\n",
        "        self.frame_count = 0\n",
        "\n",
        "    @property\n",
        "    def trial(self):\n",
        "        return self._data[0]\n",
        "\n",
        "    @property\n",
        "    def num_frames(self):  # number of frames if sampled at full temporal resolution (30 fps)\n",
        "        return int(self._data[1])\n",
        "\n",
        "    @property\n",
        "    def root_path(self):\n",
        "      return self._data[2]\n",
        "\n",
        "    @property\n",
        "    def label(self):\n",
        "        return int(self._data[3])\n",
        "\n",
        "class TSNDataSet(data.Dataset):\n",
        "    def __init__(self, list_of_list_files,\n",
        "                 num_segments=3, new_length=1, modality='RGB',\n",
        "                 image_tmpl='img_{:05d}.jpg', transform=None, normalize=None,\n",
        "                 random_shift=True, test_mode=False,\n",
        "                 video_sampling_step=3,\n",
        "                 return_3D_tensor=False, return_three_channels=False,\n",
        "                 preload_to_RAM=False, return_trial_id=False):\n",
        "\n",
        "\n",
        "        self.list_of_list_files = list_of_list_files\n",
        "        self.num_segments = num_segments\n",
        "        self.new_length = new_length  # number of consecutive frames contained in a snippet\n",
        "        self.modality = modality\n",
        "        self.image_tmpl = image_tmpl\n",
        "        self.transform = transform\n",
        "        self.normalize = normalize\n",
        "        self.random_shift = random_shift\n",
        "        self.test_mode = test_mode\n",
        "\n",
        "        self.video_sampling_step = video_sampling_step\n",
        "        self.return_3D_tensor = return_3D_tensor\n",
        "        self.return_three_channels = return_three_channels\n",
        "        self.preload_to_RAM = preload_to_RAM\n",
        "        self.return_trial_id = return_trial_id\n",
        "\n",
        "        if self.modality == 'RGBDiff':\n",
        "            self.new_length += 1# Diff needs one more image to calculate diff\n",
        "\n",
        "        self._parse_list_files()\n",
        "\n",
        "        if self.preload_to_RAM:\n",
        "            self._preload_images()\n",
        "\n",
        "    def _load_image(self, directory, idx):\n",
        "        if self.modality == 'RGB' or self.modality == 'RGBDiff':\n",
        "            return [Image.open(os.path.join(directory, self.image_tmpl.format(idx + 1))).convert('RGB')]\n",
        "            # extracted images are numbered from 1 to N (instead of 0 to N-1)\n",
        "        elif self.modality == 'Flow':\n",
        "            x_img = Image.open(os.path.join(directory, self.image_tmpl.format('x', idx + 1))).convert('L')\n",
        "            y_img = Image.open(os.path.join(directory, self.image_tmpl.format('y', idx + 1))).convert('L')\n",
        "\n",
        "            return [x_img, y_img]\n",
        "\n",
        "    def _parse_list_files(self):\n",
        "        self.video_list = []\n",
        "        for list_file in self.list_of_list_files:\n",
        "            video_list = [VideoRecord(x.strip().split(',')) for x in open(list_file)]\n",
        "            self.video_list += video_list\n",
        "        for record in self.video_list:\n",
        "            frame_count = record.num_frames // self.video_sampling_step\n",
        "            try:\n",
        "                # check whether last frame is there (sometimes gets lost during the extraction process)\n",
        "                self._load_image(os.path.join(record.root_path), frame_count - 1)\n",
        "            except FileNotFoundError:\n",
        "                frame_count = frame_count - 1\n",
        "            record.frame_count = frame_count\n",
        "\n",
        "    def _preload_images(self):\n",
        "        self.image_data = {}\n",
        "        for record in self.video_list:\n",
        "            print(\"Loading images for {}...\".format(record.trial))\n",
        "            images = []\n",
        "            img_dir = os.path.join(record.root_path)\n",
        "            for p in range(0, record.frame_count):\n",
        "                images.extend(self._load_image(img_dir, p))\n",
        "            self.image_data[record.trial] = images\n",
        "\n",
        "    def _sample_indices(self, record):\n",
        "        \"\"\"\n",
        "\n",
        "        :param record: VideoRecord\n",
        "        :return: list\n",
        "        \"\"\"\n",
        "        average_duration = (record.frame_count - self.new_length + 1) // self.num_segments\n",
        "        if average_duration > 0:\n",
        "            offsets = np.multiply(list(range(self.num_segments)), average_duration) + randint(average_duration, size=self.num_segments)\n",
        "        elif record.frame_count > self.num_segments:\n",
        "            offsets = np.sort(randint(record.frame_count - self.new_length + 1, size=self.num_segments))\n",
        "        else:\n",
        "            offsets = np.zeros((self.num_segments,))\n",
        "        return offsets\n",
        "\n",
        "    def _get_val_indices(self, record):\n",
        "        if record.frame_count > self.num_segments + self.new_length - 1:\n",
        "            tick = (record.frame_count - self.new_length + 1) / float(self.num_segments)\n",
        "            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n",
        "        else:\n",
        "            offsets = np.zeros((self.num_segments,))\n",
        "        return offsets\n",
        "\n",
        "    def _get_test_indices(self, record):\n",
        "        tick = (record.frame_count - self.new_length + 1) / float(self.num_segments)\n",
        "        offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n",
        "        return offsets\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        record = self.video_list[index]\n",
        "\n",
        "        if not self.test_mode:\n",
        "            segment_indices = self._sample_indices(record) if self.random_shift else self._get_val_indices(record)\n",
        "        else:\n",
        "            segment_indices = self._get_test_indices(record)\n",
        "\n",
        "        return self.get(record, segment_indices)\n",
        "\n",
        "    def _get_snippet(self, record, seg_ind):\n",
        "        snippet = list()\n",
        "        p = int(seg_ind)\n",
        "        for _ in range(self.new_length):\n",
        "            if self.preload_to_RAM:\n",
        "                if self.modality == 'RGB' or self.modality == 'RGBDiff':\n",
        "                    seg_imgs = self.image_data[record.trial][p: p + 1]\n",
        "                elif self.modality == 'Flow':\n",
        "                    idx = p * 2\n",
        "                    seg_imgs = self.image_data[record.trial][idx: idx + 2]\n",
        "            else:\n",
        "                img_dir = os.path.join(record.root_path)\n",
        "                seg_imgs = self._load_image(img_dir, p)\n",
        "            snippet.extend(seg_imgs)\n",
        "            if p < (record.frame_count - 1):\n",
        "                p += 1\n",
        "        return snippet\n",
        "\n",
        "    def get(self, record, indices):\n",
        "\n",
        "        images = list()\n",
        "        for seg_ind in indices:\n",
        "            images.extend(self._get_snippet(record, seg_ind))\n",
        "\n",
        "        if self.return_3D_tensor:\n",
        "            images = self.transform(images)\n",
        "            images = [ToTensor()(img) for img in images]\n",
        "            if self.modality == 'RGB':\n",
        "                images = torch.stack(images, 0)\n",
        "            elif self.modality == 'Flow':\n",
        "                _images = []\n",
        "                if self.return_three_channels:\n",
        "                    for i in range(len(images) // 2):\n",
        "                        image_dummy = (images[i] + images[i + 1]) / 2\n",
        "                        _images.append(torch.cat([images[i], images[i + 1], image_dummy], 0))\n",
        "                else:\n",
        "                    for i in range(len(images) // 2):\n",
        "                        _images.append(torch.cat([images[i], images[i + 1]], 0))\n",
        "                images = torch.stack(_images, 0)\n",
        "            images = self.normalize(images)\n",
        "            images = images.view(((-1, self.new_length) + images.size()[-3:]))\n",
        "            images = images.permute(0, 2, 1, 3, 4)\n",
        "            process_data = images\n",
        "        else:\n",
        "            transform = Compose([\n",
        "                self.transform,\n",
        "                Stack(roll=False),\n",
        "                ToTensor(),\n",
        "                self.normalize,\n",
        "            ])\n",
        "            process_data = transform(images)\n",
        "\n",
        "        target = record.label\n",
        "\n",
        "        if self.return_trial_id:\n",
        "            trial_id = record.trial.split('_')[-2]\n",
        "            print(\"Trial ID:\", trial_id)\n",
        "            return trial_id, process_data, target\n",
        "        else:\n",
        "            return process_data, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "siCC2DqgiAp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your transformations\n",
        "\n",
        "\n",
        "consensus_type = 'avg'\n",
        "model = TSN(2, 10, 'RGB', base_model='Inception3D', new_length=64,\n",
        "            consensus_type='avg', before_softmax=True, dropout=0.7, partial_bn=False,\n",
        "            use_three_input_channels=False, pretrained_model='/content/drive/MyDrive/ColabNotebooks/videos_imra/rgb_imagenet.pt')\n",
        "\n",
        "\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.base_model.logits.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.base_model.Mixed_5c.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.base_model.Mixed_5b.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "normalize = GroupNormalize(model.input_mean, model.input_std)\n",
        "train_augmentation = model.get_augmentation(True)\n"
      ],
      "metadata": {
        "id": "Nlo7pV3F0fgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "099762b5-9c3c-47ab-808a-ab1c1870e0d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing TSN with base model: Inception3D.\n",
            "                TSN Configurations:\n",
            "                input_modality:     RGB\n",
            "                num_segments:       10\n",
            "                new_length:         64\n",
            "                consensus_module:   avg\n",
            "                dropout_ratio:      0.7\n",
            "        \n",
            "loading pretrained model weights from /content/drive/MyDrive/ColabNotebooks/videos_imra/rgb_imagenet.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training datagenearion"
      ],
      "metadata": {
        "id": "M3DSjp-wh7xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import tempfile\n",
        "\n",
        "# Define the path to your original CSV file\n",
        "original_csv_file = '/content/drive/MyDrive/ColabNotebooks/videos_imra/video_results.csv'\n",
        "\n",
        "# Read the original CSV file into a DataFrame\n",
        "original_df = pd.read_csv(original_csv_file)\n",
        "\n",
        "# Define the directory where you want to save the pickle files\n",
        "pickle_files_dir = '/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/'\n",
        "\n",
        "# Create an empty list to store the file paths of the train_set objects\n",
        "train_set_file_paths = []\n",
        "\n",
        "# Iterate through each row (video) in the original CSV\n",
        "for video_index, video_row in original_df.iterrows():\n",
        "    # Create a temporary CSV file for the current video\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv') as temp_csv:\n",
        "        video_df = pd.DataFrame([video_row], columns=original_df.columns)\n",
        "        video_df.to_csv(temp_csv, index=False, header=False)\n",
        "        temp_csv_path = temp_csv.name\n",
        "\n",
        "    # Create the corresponding train_set for the current video using the temporary CSV\n",
        "    train_set = TSNDataSet(\n",
        "        list_of_list_files=[temp_csv_path],\n",
        "        num_segments=10,\n",
        "        new_length=64,\n",
        "        modality='RGB',\n",
        "        image_tmpl='frame_{:05d}.jpg',\n",
        "        transform=train_augmentation,\n",
        "        normalize=normalize,\n",
        "        random_shift=True,\n",
        "        test_mode=False,\n",
        "        video_sampling_step=3,\n",
        "        return_3D_tensor=model.is_3D_architecture,\n",
        "        return_three_channels=False,\n",
        "        preload_to_RAM=True\n",
        "    )\n",
        "\n",
        "    # Save the train_set object to a pickle file\n",
        "    train_set_filename = os.path.join(pickle_files_dir, f'train_set_{video_index}.pkl')\n",
        "    with open(train_set_filename, 'wb') as file:\n",
        "        pickle.dump(train_set, file)\n",
        "        print(f'Train set for video {video_index} saved as {train_set_filename}')\n",
        "\n",
        "    # Append the file path to the list\n",
        "    train_set_file_paths.append(train_set_filename)\n",
        "\n",
        "    # Clean up: Delete the temporary CSV file\n",
        "    os.remove(temp_csv_path)\n",
        "\n",
        "# Save the list of train_set file paths to a single pickle file\n",
        "all_train_set_files_filename = os.path.join(pickle_files_dir, 'all_train_set_files.pkl')\n",
        "with open(all_train_set_files_filename, 'wb') as file:\n",
        "    pickle.dump(train_set_file_paths, file)\n",
        "    print(f'File paths of all train sets saved as {all_train_set_files_filename}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnVJyhJbaQHK",
        "outputId": "12dd2cf1-bf4a-41cc-80c3-061ca6d6f48e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images for Novice_dV_006_interupted...\n",
            "Train set for video 0 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_0.pkl\n",
            "Loading images for Novice_dv_007_suture...\n",
            "Train set for video 1 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_1.pkl\n",
            "Loading images for Novice_dV_007_interupted...\n",
            "Train set for video 2 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_2.pkl\n",
            "Loading images for Expert_dV_010_suture...\n",
            "Train set for video 3 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_3.pkl\n",
            "Loading images for Expert_dV_010_interupted...\n",
            "Train set for video 4 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_4.pkl\n",
            "Loading images for Novice_dV_012_suture...\n",
            "Train set for video 5 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_5.pkl\n",
            "Loading images for Novice_dV_012_interupted...\n",
            "Train set for video 6 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_6.pkl\n",
            "Loading images for Novice_dV_012_suture...\n",
            "Train set for video 7 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_7.pkl\n",
            "Loading images for Novice_dV_012_interupted...\n",
            "Train set for video 8 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_8.pkl\n",
            "Loading images for Novice_dV_020_suture...\n",
            "Train set for video 9 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_9.pkl\n",
            "Loading images for Novice_dV_020_interupted...\n",
            "Train set for video 10 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_10.pkl\n",
            "Loading images for Novice_dV_022_suture...\n",
            "Train set for video 11 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_11.pkl\n",
            "Loading images for Novice_dV_022_interupted...\n",
            "Train set for video 12 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_12.pkl\n",
            "Loading images for Novice_dV_024_suture...\n",
            "Train set for video 13 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_13.pkl\n",
            "Loading images for Novice_dV_024_interupted...\n",
            "Train set for video 14 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_14.pkl\n",
            "Loading images for Novice_dV_025_suture...\n",
            "Train set for video 15 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_15.pkl\n",
            "Loading images for Novice_dV_025_interupted...\n",
            "Train set for video 16 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_16.pkl\n",
            "Loading images for Novice_dV_027_suture...\n",
            "Train set for video 17 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_17.pkl\n",
            "Loading images for Novice_dV_027_interupted...\n",
            "Train set for video 18 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_18.pkl\n",
            "Loading images for Novice_dV_031_suture...\n",
            "Train set for video 19 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_19.pkl\n",
            "Loading images for Expert_dV_051_suture...\n",
            "Train set for video 20 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_20.pkl\n",
            "Loading images for Expert_dV_2_051_interupted...\n",
            "Train set for video 21 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_21.pkl\n",
            "Loading images for Expert_dV_1_051_interupted...\n",
            "Train set for video 22 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_22.pkl\n",
            "File paths of all train sets saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/all_train_set_files.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "9Tdjv8Heh3Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import pickle\n",
        "import gc\n",
        "import gc\n",
        "import psutil\n",
        "import sklearn\n",
        "\n",
        "targets = [0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1]\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(targets), y=targets)\n",
        "\n",
        "# Function to train the model\n",
        "def run(model, train_set):\n",
        "    criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32))\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "\n",
        "    for epoch in range(60):\n",
        "        print(\"epoch:\", epoch)\n",
        "        train_loader = DataLoader(train_set, batch_size=2, shuffle=True, num_workers=8, pin_memory=True)\n",
        "        train_loss = AverageMeter()\n",
        "        train_acc = AverageMeter()\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            print(f\"Batch {batch_idx}\")\n",
        "            optimizer.zero_grad()\n",
        "            batch_size = target.size(0)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # Add L2 regularization loss\n",
        "            l2_reg = torch.tensor(0.0, requires_grad=True)\n",
        "            for param in model.parameters():\n",
        "                l2_reg += torch.norm(param, p=2)\n",
        "            loss += 1e-5 * l2_reg\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss.update(loss.item(), batch_size)\n",
        "            _output = torch.nn.Softmax(dim=1)(output)\n",
        "            _, predicted = torch.max(_output.data, 1)\n",
        "            acc = (predicted == target).sum().item() / batch_size\n",
        "            train_acc.update(acc, batch_size)\n",
        "\n",
        "        print(f\"Epoch {epoch}: Train loss: {train_loss.avg:.4f}, Train acc: {train_acc.avg:.3f}\")\n",
        "\n",
        "    if train_set == train_sets[-1]:\n",
        "        model_file = os.path.join('/content/drive/MyDrive/ColabNotebooks/videos_imra/models_new', 'last_model.pth.tar')\n",
        "    else:\n",
        "        video_index = train_sets.index(train_set)\n",
        "        model_file = os.path.join('/content/drive/MyDrive/ColabNotebooks/videos_imra/models_new', f'model_{video_index}.pth.tar')\n",
        "    # Save the model\n",
        "\n",
        "    state = {\n",
        "        'epoch': epoch +1,\n",
        "        'arch': 1,\n",
        "        'state_dict': model.state_dict(),\n",
        "    }\n",
        "    torch.save(state, model_file)\n",
        "    print(f\"Saved model to {model_file}\")\n",
        "\n",
        "# Load the train_set objects from pickle files\n",
        "pickle_dir = '/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected'\n",
        "pickle_files = [f for f in os.listdir(pickle_dir) if os.path.isfile(os.path.join(pickle_dir, f))]\n",
        "num_videos = len(pickle_files)\n",
        "\n",
        "train_sets = []\n",
        "for video_index in range(num_videos-1):\n",
        "\n",
        "    train_set_filename = f'{pickle_dir}/train_set_{video_index}.pkl'\n",
        "    print(train_set_filename)\n",
        "    with open(train_set_filename, 'rb') as file:\n",
        "        train_set = pickle.load(file)\n",
        "        train_sets.append(train_set)\n",
        "\n",
        "# Set the memory threshold\n",
        "memory_threshold_gb = 10\n",
        "\n",
        "def check_memory_threshold(threshold_gb):\n",
        "    memory_usage = psutil.virtual_memory()\n",
        "    return memory_usage.used / (1024**3) < threshold_gb\n",
        "\n",
        "\n",
        "# Run the training process for each video\n",
        "for train_set in train_sets:\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    while not check_memory_threshold(memory_threshold_gb):\n",
        "        gc.collect()\n",
        "        print(\"Memory usage exceeds the threshold. Waiting...\")\n",
        "        time.sleep(10)  # Wait for 60 seconds before checking again\n",
        "\n",
        "    print()\n",
        "    run(model, train_set)\n"
      ],
      "metadata": {
        "id": "5Z-kxyFw5Hdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2bd7fe0-e698-4a8e-d5fb-5855fd1a51fe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_0.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_1.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_2.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_3.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_4.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_5.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_6.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_7.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_8.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_9.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_10.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_11.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_12.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_13.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_14.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_15.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_16.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_17.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_18.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_19.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_20.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_21.pkl\n",
            "/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_corrected/train_set_22.pkl\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.5430, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.5723, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.5790, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.5403, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.5566, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.5423, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.5377, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.5483, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.5378, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.5504, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.5265, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.5300, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.5445, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.4894, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.5130, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.5225, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.5168, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.5347, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.5090, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.5042, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.4986, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.4869, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.5027, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.5150, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.4751, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.4923, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.5072, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.4697, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.4853, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.5000, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.4883, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.4790, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.4753, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.4874, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.4678, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.4761, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.4542, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.4768, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.4681, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.4691, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.4480, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.4394, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.4472, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.4232, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.4357, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.4486, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.4245, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.4196, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.4383, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.4546, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.4178, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.4362, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.4418, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.4009, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.4298, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.4068, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.4117, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.4138, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.4207, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.4128, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_0.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.4364, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.4243, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.4567, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.4280, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.4182, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.4265, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.4255, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.4278, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.4071, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.4119, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.4083, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.4281, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.4439, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.4383, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.3946, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.4180, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.4142, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.3958, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.4016, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.3956, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.4008, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.3765, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.3743, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.3867, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.4167, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.3817, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.3906, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.3675, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.3824, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.3534, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.3716, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.3762, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.3797, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.3623, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.3652, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.3834, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.3550, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.3577, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.3527, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.3587, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.3588, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.3768, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.3639, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.3734, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.3569, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.3487, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.3534, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.3563, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.3191, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.3425, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.3339, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.3506, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.3320, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.3127, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.3296, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.3357, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.3232, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.3147, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.3276, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.3236, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_1.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.3042, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.3075, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.3175, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.3066, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.3134, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.3080, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.3067, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.3046, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.3081, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.2894, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.2858, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.2950, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.2829, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.2810, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.2937, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.2805, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.2816, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.2940, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.2851, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.2747, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.2690, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.2824, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.2782, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.2725, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.2758, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.2667, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.2655, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.2795, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.2693, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.2534, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.2578, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.2745, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.2571, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.2649, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.2632, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.2607, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.2491, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.2620, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.2637, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.2593, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.2689, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.2364, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.2544, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.2479, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.2623, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.2396, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.2395, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.2441, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.2361, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.2351, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.2279, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.2468, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.2286, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.2409, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.2282, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.2381, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.2268, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.2412, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.2261, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.2219, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_2.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 1.7362, Train acc: 0.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 1.7217, Train acc: 0.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 1.7137, Train acc: 0.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 1.6917, Train acc: 0.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 1.7210, Train acc: 0.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 1.7199, Train acc: 0.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 1.6708, Train acc: 0.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 1.6679, Train acc: 0.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 1.6612, Train acc: 0.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 1.6877, Train acc: 0.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 1.6874, Train acc: 0.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 1.6708, Train acc: 0.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 1.6432, Train acc: 0.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 1.6637, Train acc: 0.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 1.6386, Train acc: 0.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 1.6143, Train acc: 0.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 1.5883, Train acc: 0.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 1.6668, Train acc: 0.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 1.6299, Train acc: 0.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 1.6422, Train acc: 0.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 1.5212, Train acc: 0.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 1.5823, Train acc: 0.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 1.5783, Train acc: 0.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 1.5684, Train acc: 0.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 1.5723, Train acc: 0.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 1.5969, Train acc: 0.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 1.5574, Train acc: 0.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 1.5619, Train acc: 0.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 1.5315, Train acc: 0.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 1.5579, Train acc: 0.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 1.5516, Train acc: 0.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 1.5845, Train acc: 0.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 1.5120, Train acc: 0.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 1.4624, Train acc: 0.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 1.5124, Train acc: 0.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 1.6412, Train acc: 0.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 1.4824, Train acc: 0.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 1.5515, Train acc: 0.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 1.5073, Train acc: 0.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 1.4523, Train acc: 0.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 1.4539, Train acc: 0.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 1.4509, Train acc: 0.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 1.5166, Train acc: 0.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 1.4851, Train acc: 0.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 1.4127, Train acc: 0.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 1.4242, Train acc: 0.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 1.4801, Train acc: 0.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 1.4397, Train acc: 0.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 1.4756, Train acc: 0.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 1.4051, Train acc: 0.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 1.4368, Train acc: 0.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 1.4126, Train acc: 0.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 1.4012, Train acc: 0.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 1.4066, Train acc: 0.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 1.4201, Train acc: 0.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 1.3918, Train acc: 0.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 1.3681, Train acc: 0.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 1.4024, Train acc: 0.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 1.4031, Train acc: 0.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 1.3618, Train acc: 0.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_3.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 1.4568, Train acc: 0.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 1.3948, Train acc: 0.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 1.3319, Train acc: 0.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 1.4068, Train acc: 0.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 1.4169, Train acc: 0.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 1.3988, Train acc: 0.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 1.4365, Train acc: 0.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 1.4096, Train acc: 0.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 1.3623, Train acc: 0.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 1.3620, Train acc: 0.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 1.3719, Train acc: 0.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 1.3974, Train acc: 0.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 1.3441, Train acc: 0.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 1.3364, Train acc: 0.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 1.2974, Train acc: 0.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 1.3390, Train acc: 0.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 1.3448, Train acc: 0.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 1.2936, Train acc: 0.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 1.3655, Train acc: 0.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 1.2476, Train acc: 0.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 1.3035, Train acc: 0.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 1.2753, Train acc: 0.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 1.2811, Train acc: 0.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 1.3044, Train acc: 0.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 1.2875, Train acc: 0.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 1.2816, Train acc: 0.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 1.2858, Train acc: 0.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 1.2907, Train acc: 0.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 1.2540, Train acc: 0.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 1.2629, Train acc: 0.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 1.2387, Train acc: 0.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 1.2240, Train acc: 0.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 1.2596, Train acc: 0.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 1.1925, Train acc: 0.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 1.2567, Train acc: 0.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 1.2531, Train acc: 0.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 1.2377, Train acc: 0.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 1.1967, Train acc: 0.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 1.2375, Train acc: 0.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 1.1892, Train acc: 0.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 1.1931, Train acc: 0.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 1.1391, Train acc: 0.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 1.1246, Train acc: 0.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 1.2135, Train acc: 0.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 1.2185, Train acc: 0.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 1.1945, Train acc: 0.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 1.1088, Train acc: 0.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 1.1596, Train acc: 0.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 1.1695, Train acc: 0.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 1.1690, Train acc: 0.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 1.1870, Train acc: 0.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 1.1467, Train acc: 0.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 1.1426, Train acc: 0.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 1.1415, Train acc: 0.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 1.0513, Train acc: 0.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 1.1263, Train acc: 0.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 1.1325, Train acc: 0.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 1.1539, Train acc: 0.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 1.1033, Train acc: 0.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 1.0938, Train acc: 0.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_4.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.4442, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.4362, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.4404, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.4543, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.4494, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.4585, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.4228, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.4389, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.4146, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.4514, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.4630, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.4334, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.4118, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.4152, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.4187, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.4129, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.4067, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.4165, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.4159, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.4191, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.4117, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.3932, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.4147, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.4072, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.4067, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.3920, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.3852, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.4235, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.3827, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.3891, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.3897, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.3792, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.3799, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.3924, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.3771, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.3832, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.3777, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.3790, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.3980, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.3644, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.3804, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.3682, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.3669, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.3545, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.3359, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.3675, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.3409, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.3732, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.3548, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.3427, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.3231, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.3338, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.3440, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.3387, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.3469, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.3344, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.3413, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.3249, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.3440, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.3424, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_5.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.3390, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.3079, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.3163, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.3191, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.3321, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.3339, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.3053, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.3142, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.3153, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.3188, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.3167, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.2998, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.3258, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.3039, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.3030, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.3146, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.2909, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.2964, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.3105, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.3102, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.3085, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.2989, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.3003, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.2927, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.2855, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.2844, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.2819, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.2753, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.2973, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.2783, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.2695, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.2873, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.2774, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.2734, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.2727, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.2665, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.2629, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.2624, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.2719, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.2781, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.2944, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.2661, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.2556, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.2617, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.2572, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.2636, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.2602, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.2582, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.2557, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.2573, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.2609, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.2445, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.2369, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.2462, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.2399, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.2460, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.2694, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.2390, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.2292, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.2384, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_6.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.2413, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.2510, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.2468, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.2405, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.2396, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.2370, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.2355, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.2358, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.2210, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.2145, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.2280, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.2382, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.2255, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.2285, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.2183, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.2305, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.2227, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.2215, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.2227, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.2342, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.2201, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.2230, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.2210, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.2094, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.2181, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.2141, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.2068, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.2095, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.2099, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.2093, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.1999, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.2001, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.2015, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.1948, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.2084, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.1939, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.1977, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.1953, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.1942, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.1919, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.1917, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.1796, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.1874, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.1948, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.1864, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.1964, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.2048, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.1950, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.1978, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.1960, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.1793, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.1882, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.1821, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.1852, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.1801, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.1878, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.1806, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.1697, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.1826, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.1688, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_7.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.1818, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.1798, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.1802, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.1685, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.1632, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.1699, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.1723, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.1763, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.1701, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.1717, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.1679, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.1771, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.1687, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.1677, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.1649, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.1639, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.1606, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.1627, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.1569, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.1591, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.1547, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.1566, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.1526, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.1512, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.1669, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.1619, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.1638, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.1553, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.1536, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.1555, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.1666, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.1622, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.1511, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.1523, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.1528, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.1583, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.1552, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.1568, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.1483, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.1361, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.1423, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.1340, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.1499, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.1521, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.1478, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.1342, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.1512, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.1513, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.1402, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.1337, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.1351, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.1386, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.1377, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.1394, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.1396, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.1419, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.1409, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.1290, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.1318, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.1354, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_8.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.1384, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.1317, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.1446, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.1336, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.1292, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.1263, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.1335, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.1222, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.1275, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.1279, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.1244, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.1209, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.1272, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.1263, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.1264, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.1299, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.1271, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.1183, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.1213, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.1251, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.1174, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.1245, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.1171, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.1162, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.1146, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.1139, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.1202, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.1121, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.1179, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.1087, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.1185, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.1157, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.1111, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.1096, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.1069, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.1214, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.1101, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.1117, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.1124, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.1096, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.1124, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.1104, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.1091, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.1077, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.1073, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.1055, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.1065, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.1008, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.1040, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.1070, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.1097, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.1029, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.1033, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.1011, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.1068, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0989, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.1018, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0969, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0993, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0980, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_9.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0970, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.1060, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.1022, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.1009, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.1038, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0975, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.1097, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0997, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0945, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.1055, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0993, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0959, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.1054, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0953, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0977, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.1008, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0978, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0962, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0907, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0938, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0978, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0927, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0898, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0890, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0950, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0906, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0916, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0911, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0910, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0860, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0940, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0895, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0885, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0869, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0866, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0877, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0915, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0878, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0880, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0860, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0880, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0883, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0863, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0898, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0908, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0862, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0841, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0871, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0840, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0801, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0810, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0818, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0879, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0852, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0811, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0863, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0816, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0852, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0825, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0821, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_10.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0902, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0866, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0861, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0856, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0857, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0843, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0811, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0848, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0842, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0836, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0783, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0810, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0850, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0842, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0821, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0833, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0881, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0799, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0820, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0819, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0799, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0822, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0765, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0778, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0805, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0866, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0758, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0780, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0810, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0793, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0750, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0765, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0776, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0767, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0769, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0784, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0764, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0723, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0762, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0757, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0716, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0702, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0718, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0742, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0733, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0721, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0747, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0729, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0764, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0714, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0717, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0742, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0713, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0705, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0725, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0714, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0724, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0701, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0696, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0697, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_11.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0730, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0827, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0801, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0743, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0733, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0780, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0778, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0768, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0705, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0733, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0695, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0693, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0711, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0731, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0669, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0794, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0704, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0760, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0754, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0739, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0721, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0651, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0662, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0665, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0662, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0678, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0792, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0655, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0714, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0655, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0745, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0669, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0639, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0673, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0651, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0708, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0664, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0688, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0656, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0637, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0661, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0658, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0637, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0626, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0658, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0680, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0659, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0635, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0646, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0663, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0625, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0654, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0640, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0620, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0655, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0609, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0605, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0630, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0607, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0598, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_12.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0601, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0625, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0602, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0582, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0642, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0612, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0583, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0605, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0588, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0588, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0607, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0617, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0584, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0593, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0597, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0611, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0572, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0604, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0572, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0565, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0600, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0573, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0574, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0567, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0595, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0619, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0563, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0568, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0551, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0574, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0559, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0567, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0555, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0569, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0587, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0557, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0589, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0580, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0542, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0558, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0570, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0557, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0572, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0550, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0539, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0564, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0528, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0529, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0549, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0554, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0550, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0545, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0549, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0538, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0514, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0549, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0571, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0573, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0519, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0538, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_13.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0536, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0537, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0548, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0529, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0538, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0539, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0548, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0547, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0530, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0530, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0547, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0523, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0529, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0525, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0522, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0529, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0529, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0527, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0528, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0507, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0524, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0504, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0528, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0519, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0500, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0517, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0530, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0504, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0511, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0513, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0511, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0516, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0509, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0522, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0529, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0494, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0496, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0511, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0516, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0501, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0505, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0514, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0514, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0494, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0498, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0506, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0499, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0507, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0479, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0500, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0494, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0507, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0492, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0503, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0498, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0481, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0492, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0488, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0481, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0480, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_14.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0492, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0477, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0491, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0488, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0501, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0487, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0482, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0495, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0489, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0494, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0480, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0475, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0469, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0481, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0482, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0474, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0473, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0482, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0474, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0485, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0466, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0459, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0473, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0481, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0465, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0461, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0467, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0470, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0477, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0479, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0466, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0472, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0481, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0464, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0470, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0477, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0463, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0459, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0470, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0462, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0471, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0453, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0468, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0461, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0460, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0463, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0462, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0462, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0453, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0463, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0462, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0446, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0458, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0451, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0448, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0470, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0452, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0450, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0445, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0445, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_15.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0441, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0434, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0443, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0443, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0451, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0447, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0449, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0443, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0442, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0442, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0448, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0447, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0438, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0441, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0442, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0440, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0434, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0432, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0432, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0445, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0439, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0434, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0432, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0442, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0449, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0425, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0427, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0433, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0422, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0427, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0427, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0433, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0428, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0431, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0426, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0432, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0424, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0426, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0422, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0423, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0436, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0423, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0425, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0422, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0421, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0418, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0429, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0428, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0421, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0419, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0421, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0424, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0419, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0418, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0427, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0415, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0416, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0421, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0419, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0411, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_16.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0427, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0419, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0421, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0419, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0416, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0422, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0419, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0425, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0431, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0419, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0417, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0412, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0412, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0416, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0417, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0426, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0419, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0419, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0413, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0426, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0420, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0414, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0409, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0411, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0414, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0413, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0410, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0410, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0417, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0414, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0412, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0409, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0415, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0418, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0406, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0411, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0412, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0412, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0410, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0410, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0414, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0408, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0414, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0415, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0408, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0410, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0404, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0407, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0400, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0402, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0403, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0404, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0401, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0403, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0405, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0408, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0401, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0409, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0403, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0404, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_17.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0401, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0407, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0408, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0405, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0405, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0404, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0402, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0400, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0398, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0401, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0401, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0404, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0402, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0403, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0403, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0401, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0401, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0402, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0397, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0399, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0403, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0398, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0401, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0400, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0394, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0395, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0396, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0397, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0389, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0393, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0395, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0392, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0396, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0393, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0394, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0394, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0399, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0396, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0393, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0394, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0390, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0389, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0389, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0391, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0386, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0395, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0391, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0387, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0394, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0391, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0387, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0391, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0388, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0388, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0392, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0387, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0390, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0389, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0393, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0386, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_18.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 0.0394, Train acc: 1.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 0.0400, Train acc: 1.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 0.0392, Train acc: 1.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 0.0391, Train acc: 1.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 0.0393, Train acc: 1.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 0.0392, Train acc: 1.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 0.0392, Train acc: 1.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 0.0391, Train acc: 1.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 0.0390, Train acc: 1.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 0.0391, Train acc: 1.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 0.0397, Train acc: 1.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 0.0389, Train acc: 1.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 0.0389, Train acc: 1.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 0.0396, Train acc: 1.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 0.0397, Train acc: 1.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 0.0387, Train acc: 1.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 0.0390, Train acc: 1.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 0.0391, Train acc: 1.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 0.0390, Train acc: 1.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 0.0390, Train acc: 1.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 0.0388, Train acc: 1.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 0.0384, Train acc: 1.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 0.0387, Train acc: 1.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 0.0387, Train acc: 1.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 0.0384, Train acc: 1.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 0.0393, Train acc: 1.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 0.0387, Train acc: 1.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 0.0383, Train acc: 1.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 0.0386, Train acc: 1.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 0.0384, Train acc: 1.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 0.0391, Train acc: 1.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 0.0383, Train acc: 1.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 0.0389, Train acc: 1.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 0.0382, Train acc: 1.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 0.0385, Train acc: 1.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 0.0383, Train acc: 1.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 0.0380, Train acc: 1.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 0.0380, Train acc: 1.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 0.0387, Train acc: 1.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 0.0382, Train acc: 1.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 0.0393, Train acc: 1.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 0.0383, Train acc: 1.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 0.0383, Train acc: 1.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 0.0381, Train acc: 1.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 0.0381, Train acc: 1.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 0.0386, Train acc: 1.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 0.0382, Train acc: 1.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 0.0388, Train acc: 1.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 0.0381, Train acc: 1.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 0.0387, Train acc: 1.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 0.0383, Train acc: 1.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 0.0380, Train acc: 1.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 0.0379, Train acc: 1.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 0.0380, Train acc: 1.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 0.0379, Train acc: 1.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 0.0383, Train acc: 1.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 0.0382, Train acc: 1.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 0.0381, Train acc: 1.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 0.0381, Train acc: 1.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 0.0384, Train acc: 1.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_19.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 5.7057, Train acc: 0.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 5.5901, Train acc: 0.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 5.7594, Train acc: 0.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 5.7767, Train acc: 0.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 5.6871, Train acc: 0.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 5.8150, Train acc: 0.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 5.6398, Train acc: 0.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 5.7628, Train acc: 0.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 5.7070, Train acc: 0.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 5.6518, Train acc: 0.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 5.6383, Train acc: 0.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 5.6906, Train acc: 0.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 5.6030, Train acc: 0.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 5.7324, Train acc: 0.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 5.6973, Train acc: 0.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 5.5647, Train acc: 0.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 5.3759, Train acc: 0.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 5.5540, Train acc: 0.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 5.5948, Train acc: 0.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 5.6035, Train acc: 0.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 5.5724, Train acc: 0.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 5.6825, Train acc: 0.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 5.4717, Train acc: 0.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 5.5336, Train acc: 0.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 5.6196, Train acc: 0.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 5.5818, Train acc: 0.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 5.6877, Train acc: 0.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 5.4977, Train acc: 0.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 5.5943, Train acc: 0.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 5.5124, Train acc: 0.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 5.6446, Train acc: 0.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 5.4944, Train acc: 0.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 5.5669, Train acc: 0.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 5.5918, Train acc: 0.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 5.4500, Train acc: 0.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 5.4215, Train acc: 0.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 5.3408, Train acc: 0.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 5.3843, Train acc: 0.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 5.3186, Train acc: 0.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 5.4149, Train acc: 0.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 5.5368, Train acc: 0.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 5.5414, Train acc: 0.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 5.4996, Train acc: 0.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 5.2454, Train acc: 0.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 5.4321, Train acc: 0.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 5.4186, Train acc: 0.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 5.2404, Train acc: 0.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 5.3047, Train acc: 0.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 5.3265, Train acc: 0.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 5.5209, Train acc: 0.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 5.3197, Train acc: 0.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 5.3438, Train acc: 0.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 5.3075, Train acc: 0.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 5.2312, Train acc: 0.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 5.2607, Train acc: 0.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 5.3694, Train acc: 0.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 5.3816, Train acc: 0.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 5.3865, Train acc: 0.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 5.2929, Train acc: 0.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 5.2044, Train acc: 0.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_20.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 5.1578, Train acc: 0.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 5.3770, Train acc: 0.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 5.1889, Train acc: 0.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 5.3171, Train acc: 0.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 5.2248, Train acc: 0.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 5.2793, Train acc: 0.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 5.2187, Train acc: 0.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 5.2055, Train acc: 0.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 5.2708, Train acc: 0.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 5.1658, Train acc: 0.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 5.2790, Train acc: 0.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 5.2349, Train acc: 0.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 5.2567, Train acc: 0.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 5.1885, Train acc: 0.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 5.1869, Train acc: 0.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 5.0975, Train acc: 0.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 5.2531, Train acc: 0.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 5.1430, Train acc: 0.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 5.1039, Train acc: 0.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 5.2455, Train acc: 0.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 5.1681, Train acc: 0.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 5.2283, Train acc: 0.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 5.0638, Train acc: 0.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 5.1901, Train acc: 0.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 5.1295, Train acc: 0.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 5.1121, Train acc: 0.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 5.1174, Train acc: 0.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 5.0475, Train acc: 0.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 5.1163, Train acc: 0.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 5.0679, Train acc: 0.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 5.1600, Train acc: 0.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 5.0773, Train acc: 0.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 5.1029, Train acc: 0.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 4.8563, Train acc: 0.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 4.9763, Train acc: 0.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 5.0586, Train acc: 0.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 5.1091, Train acc: 0.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 5.0514, Train acc: 0.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 5.0287, Train acc: 0.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 4.9219, Train acc: 0.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 5.0247, Train acc: 0.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 5.0190, Train acc: 0.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 4.9934, Train acc: 0.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 4.9574, Train acc: 0.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 4.9150, Train acc: 0.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 4.9828, Train acc: 0.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 4.7591, Train acc: 0.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 4.9408, Train acc: 0.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 4.9709, Train acc: 0.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 4.7697, Train acc: 0.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 4.9152, Train acc: 0.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 4.9424, Train acc: 0.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 4.8539, Train acc: 0.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 4.9681, Train acc: 0.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 4.8697, Train acc: 0.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 4.4658, Train acc: 0.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 4.5639, Train acc: 0.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 4.7642, Train acc: 0.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 4.7386, Train acc: 0.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 4.6991, Train acc: 0.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/model_21.pth.tar\n",
            "\n",
            "epoch: 0\n",
            "Batch 0\n",
            "Epoch 0: Train loss: 4.9461, Train acc: 0.000\n",
            "epoch: 1\n",
            "Batch 0\n",
            "Epoch 1: Train loss: 4.9510, Train acc: 0.000\n",
            "epoch: 2\n",
            "Batch 0\n",
            "Epoch 2: Train loss: 4.8625, Train acc: 0.000\n",
            "epoch: 3\n",
            "Batch 0\n",
            "Epoch 3: Train loss: 4.8003, Train acc: 0.000\n",
            "epoch: 4\n",
            "Batch 0\n",
            "Epoch 4: Train loss: 4.8707, Train acc: 0.000\n",
            "epoch: 5\n",
            "Batch 0\n",
            "Epoch 5: Train loss: 4.8932, Train acc: 0.000\n",
            "epoch: 6\n",
            "Batch 0\n",
            "Epoch 6: Train loss: 4.9066, Train acc: 0.000\n",
            "epoch: 7\n",
            "Batch 0\n",
            "Epoch 7: Train loss: 4.6906, Train acc: 0.000\n",
            "epoch: 8\n",
            "Batch 0\n",
            "Epoch 8: Train loss: 4.9319, Train acc: 0.000\n",
            "epoch: 9\n",
            "Batch 0\n",
            "Epoch 9: Train loss: 4.8459, Train acc: 0.000\n",
            "epoch: 10\n",
            "Batch 0\n",
            "Epoch 10: Train loss: 4.8390, Train acc: 0.000\n",
            "epoch: 11\n",
            "Batch 0\n",
            "Epoch 11: Train loss: 4.7663, Train acc: 0.000\n",
            "epoch: 12\n",
            "Batch 0\n",
            "Epoch 12: Train loss: 4.7323, Train acc: 0.000\n",
            "epoch: 13\n",
            "Batch 0\n",
            "Epoch 13: Train loss: 4.9152, Train acc: 0.000\n",
            "epoch: 14\n",
            "Batch 0\n",
            "Epoch 14: Train loss: 4.8566, Train acc: 0.000\n",
            "epoch: 15\n",
            "Batch 0\n",
            "Epoch 15: Train loss: 4.8123, Train acc: 0.000\n",
            "epoch: 16\n",
            "Batch 0\n",
            "Epoch 16: Train loss: 4.8083, Train acc: 0.000\n",
            "epoch: 17\n",
            "Batch 0\n",
            "Epoch 17: Train loss: 4.7413, Train acc: 0.000\n",
            "epoch: 18\n",
            "Batch 0\n",
            "Epoch 18: Train loss: 4.7325, Train acc: 0.000\n",
            "epoch: 19\n",
            "Batch 0\n",
            "Epoch 19: Train loss: 4.8067, Train acc: 0.000\n",
            "epoch: 20\n",
            "Batch 0\n",
            "Epoch 20: Train loss: 4.7634, Train acc: 0.000\n",
            "epoch: 21\n",
            "Batch 0\n",
            "Epoch 21: Train loss: 4.8709, Train acc: 0.000\n",
            "epoch: 22\n",
            "Batch 0\n",
            "Epoch 22: Train loss: 4.6855, Train acc: 0.000\n",
            "epoch: 23\n",
            "Batch 0\n",
            "Epoch 23: Train loss: 4.7923, Train acc: 0.000\n",
            "epoch: 24\n",
            "Batch 0\n",
            "Epoch 24: Train loss: 4.7219, Train acc: 0.000\n",
            "epoch: 25\n",
            "Batch 0\n",
            "Epoch 25: Train loss: 4.8217, Train acc: 0.000\n",
            "epoch: 26\n",
            "Batch 0\n",
            "Epoch 26: Train loss: 4.6757, Train acc: 0.000\n",
            "epoch: 27\n",
            "Batch 0\n",
            "Epoch 27: Train loss: 4.6765, Train acc: 0.000\n",
            "epoch: 28\n",
            "Batch 0\n",
            "Epoch 28: Train loss: 4.7717, Train acc: 0.000\n",
            "epoch: 29\n",
            "Batch 0\n",
            "Epoch 29: Train loss: 4.6706, Train acc: 0.000\n",
            "epoch: 30\n",
            "Batch 0\n",
            "Epoch 30: Train loss: 4.7147, Train acc: 0.000\n",
            "epoch: 31\n",
            "Batch 0\n",
            "Epoch 31: Train loss: 4.6768, Train acc: 0.000\n",
            "epoch: 32\n",
            "Batch 0\n",
            "Epoch 32: Train loss: 4.7427, Train acc: 0.000\n",
            "epoch: 33\n",
            "Batch 0\n",
            "Epoch 33: Train loss: 4.5983, Train acc: 0.000\n",
            "epoch: 34\n",
            "Batch 0\n",
            "Epoch 34: Train loss: 4.5768, Train acc: 0.000\n",
            "epoch: 35\n",
            "Batch 0\n",
            "Epoch 35: Train loss: 4.5658, Train acc: 0.000\n",
            "epoch: 36\n",
            "Batch 0\n",
            "Epoch 36: Train loss: 4.5222, Train acc: 0.000\n",
            "epoch: 37\n",
            "Batch 0\n",
            "Epoch 37: Train loss: 4.6294, Train acc: 0.000\n",
            "epoch: 38\n",
            "Batch 0\n",
            "Epoch 38: Train loss: 4.6412, Train acc: 0.000\n",
            "epoch: 39\n",
            "Batch 0\n",
            "Epoch 39: Train loss: 4.4906, Train acc: 0.000\n",
            "epoch: 40\n",
            "Batch 0\n",
            "Epoch 40: Train loss: 4.5899, Train acc: 0.000\n",
            "epoch: 41\n",
            "Batch 0\n",
            "Epoch 41: Train loss: 4.5543, Train acc: 0.000\n",
            "epoch: 42\n",
            "Batch 0\n",
            "Epoch 42: Train loss: 4.6171, Train acc: 0.000\n",
            "epoch: 43\n",
            "Batch 0\n",
            "Epoch 43: Train loss: 4.6149, Train acc: 0.000\n",
            "epoch: 44\n",
            "Batch 0\n",
            "Epoch 44: Train loss: 4.5975, Train acc: 0.000\n",
            "epoch: 45\n",
            "Batch 0\n",
            "Epoch 45: Train loss: 4.5025, Train acc: 0.000\n",
            "epoch: 46\n",
            "Batch 0\n",
            "Epoch 46: Train loss: 4.4801, Train acc: 0.000\n",
            "epoch: 47\n",
            "Batch 0\n",
            "Epoch 47: Train loss: 4.5341, Train acc: 0.000\n",
            "epoch: 48\n",
            "Batch 0\n",
            "Epoch 48: Train loss: 4.5241, Train acc: 0.000\n",
            "epoch: 49\n",
            "Batch 0\n",
            "Epoch 49: Train loss: 4.5034, Train acc: 0.000\n",
            "epoch: 50\n",
            "Batch 0\n",
            "Epoch 50: Train loss: 4.5520, Train acc: 0.000\n",
            "epoch: 51\n",
            "Batch 0\n",
            "Epoch 51: Train loss: 4.5313, Train acc: 0.000\n",
            "epoch: 52\n",
            "Batch 0\n",
            "Epoch 52: Train loss: 4.6060, Train acc: 0.000\n",
            "epoch: 53\n",
            "Batch 0\n",
            "Epoch 53: Train loss: 4.5099, Train acc: 0.000\n",
            "epoch: 54\n",
            "Batch 0\n",
            "Epoch 54: Train loss: 4.3392, Train acc: 0.000\n",
            "epoch: 55\n",
            "Batch 0\n",
            "Epoch 55: Train loss: 4.4398, Train acc: 0.000\n",
            "epoch: 56\n",
            "Batch 0\n",
            "Epoch 56: Train loss: 4.4285, Train acc: 0.000\n",
            "epoch: 57\n",
            "Batch 0\n",
            "Epoch 57: Train loss: 4.4644, Train acc: 0.000\n",
            "epoch: 58\n",
            "Batch 0\n",
            "Epoch 58: Train loss: 4.4460, Train acc: 0.000\n",
            "epoch: 59\n",
            "Batch 0\n",
            "Epoch 59: Train loss: 4.4142, Train acc: 0.000\n",
            "Saved model to /content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/last_model.pth.tar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "U-Qe0XkwgLeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = TSN(2, 1,  # num_segments = 1 to predict each snippet independently\n",
        "              'RGB', base_model='Inception3D', new_length=64,\n",
        "              consensus_type='avg', before_softmax=True, dropout=0.0, partial_bn=False,\n",
        "              use_three_input_channels=False)\n",
        "model_file_path = '/content/drive/MyDrive/ColabNotebooks/videos_imra/models_new/last_model.pth.tar'\n",
        "if os.path.exists(model_file_path):\n",
        "    print('yes')\n",
        "    checkpoint = torch.load(model_file_path)\n",
        "    net.load_state_dict(checkpoint['state_dict'],strict=False)\n",
        "\n",
        "\n",
        "normalize = GroupNormalize(net.input_mean, net.input_std)\n",
        "cropping = torchvision.transforms.Compose([GroupScale(net.scale_size), GroupCenterCrop(net.input_size)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D5mC12rTd5a",
        "outputId": "65b7ebee-1601-4be4-846f-eb29bdddcad1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing TSN with base model: Inception3D.\n",
            "                TSN Configurations:\n",
            "                input_modality:     RGB\n",
            "                num_segments:       1\n",
            "                new_length:         64\n",
            "                consensus_module:   avg\n",
            "                dropout_ratio:      0.0\n",
            "        \n",
            "yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.is_3D_architecture"
      ],
      "metadata": {
        "id": "GuGmpnGtpH11",
        "outputId": "bc09c017-71db-46e3-da05-80ebf9cd7285",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Data set"
      ],
      "metadata": {
        "id": "PbV_suB7S5nP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import tempfile\n",
        "\n",
        "# Define the path to your original CSV file\n",
        "original_csv_file = '/content/drive/MyDrive/ColabNotebooks/videos_imra/test_data.csv'\n",
        "\n",
        "# Read the original CSV file into a DataFrame\n",
        "original_df = pd.read_csv(original_csv_file , header=None)\n",
        "\n",
        "# Define the directory where you want to save the pickle files\n",
        "pickle_files_dir = '/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_test/'\n",
        "\n",
        "# Create an empty list to store the file paths of the train_set objects\n",
        "test_set_file_paths = []\n",
        "\n",
        "# Iterate through each row (video) in the original CSV\n",
        "for video_index, video_row in original_df.iterrows():\n",
        "    # Create a temporary CSV file for the current video\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv') as temp_csv:\n",
        "        video_df = pd.DataFrame([video_row], columns=original_df.columns)\n",
        "        video_df.to_csv(temp_csv, index=False, header=False)\n",
        "        temp_csv_path = temp_csv.name\n",
        "\n",
        "    # Create the corresponding train_set for the current video using the temporary CSV\n",
        "    test_set = TSNDataSet(\n",
        "        list_of_list_files=[temp_csv_path],\n",
        "        num_segments=25,\n",
        "        new_length=64,\n",
        "        modality='RGB',\n",
        "        image_tmpl='frame_{:05d}.jpg',\n",
        "        transform=cropping,\n",
        "        normalize=normalize,\n",
        "        random_shift=False,\n",
        "        test_mode=True,\n",
        "        video_sampling_step=3,\n",
        "        return_3D_tensor= net.is_3D_architecture,\n",
        "        return_three_channels=False,\n",
        "        preload_to_RAM=True,\n",
        "        return_trial_id=True\n",
        "    )\n",
        "\n",
        "    # Save the train_set object to a pickle file\n",
        "    test_set_filename = os.path.join(pickle_files_dir, f'test_set_{video_index}.pkl')\n",
        "    with open(test_set_filename, 'wb') as file:\n",
        "        pickle.dump(test_set, file)\n",
        "        print(f'Test set for video {video_index} saved as {test_set_filename}')\n",
        "\n",
        "    # Append the file path to the list\n",
        "    test_set_file_paths.append(test_set_filename)\n",
        "\n",
        "    # Clean up: Delete the temporary CSV file\n",
        "    os.remove(temp_csv_path)\n",
        "\n",
        "# Save the list of train_set file paths to a single pickle file\n",
        "all_test_set_files_filename = os.path.join(pickle_files_dir, 'all_test_set_files.pkl')\n",
        "with open(all_train_set_files_filename, 'wb') as file:\n",
        "    pickle.dump(test_set_file_paths, file)\n",
        "    print(f'File paths of all train sets saved as {all_test_set_files_filename}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "TbMKZeOOOCj4",
        "outputId": "d886ea7d-61c4-484c-9480-beaa7fdb1282"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images for Novice_dV_001_suture...\n",
            "Test set for video 0 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_test/test_set_0.pkl\n",
            "Loading images for Novice_dV_001_interupted...\n",
            "Test set for video 1 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_test/test_set_1.pkl\n",
            "Loading images for Expert_dV_008_suture...\n",
            "Test set for video 2 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_test/test_set_2.pkl\n",
            "Loading images for Expert_dV_008_interupted...\n",
            "Test set for video 3 saved as /content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_test/test_set_3.pkl\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-a46b8f33180b>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Save the list of train_set file paths to a single pickle file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mall_test_set_files_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_files_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all_test_set_files.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_set_files_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_file_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'File paths of all train sets saved as {all_test_set_files_filename}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'all_train_set_files_filename' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predection"
      ],
      "metadata": {
        "id": "UHCPc2QJfiV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import psutil\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# Define a testing function\n",
        "def test(model, test_set):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize a list to store prediction results\n",
        "    results = []\n",
        "\n",
        "    # Create a DataLoader for the test data\n",
        "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=8)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (trail_id,data, target) in enumerate(test_loader):\n",
        "            # Make predictions using the model\n",
        "            output = model(data)\n",
        "            pred = torch.mean(output, 0).squeeze()\n",
        "            pred = torch.nn.Softmax()(pred)\n",
        "            pred = pred.cpu().numpy()\n",
        "            results.append([trail_id, np.argmax(pred), target.item()])\n",
        "\n",
        "\n",
        "    return results\n",
        "\n",
        "# Directory containing pickle files with test data\n",
        "test_pickle_dir = '/content/drive/MyDrive/ColabNotebooks/videos_imra/pickelfiles_test'\n",
        "\n",
        "# Load test data from pickle files\n",
        "test_sets = []\n",
        "for test_set_filename in os.listdir(test_pickle_dir):\n",
        "    if test_set_filename.endswith('.pkl'):\n",
        "        print(test_set_filename)\n",
        "        with open(os.path.join(test_pickle_dir, test_set_filename), 'rb') as file:\n",
        "            test_set = pickle.load(file)\n",
        "            test_sets.append(test_set)\n",
        "\n",
        "# Load your model\n",
        "model = net  # Replace with the actual model class and loading logic\n",
        "\n",
        "memory_threshold_gb = 10\n",
        "\n",
        "def check_memory_threshold(threshold_gb):\n",
        "    memory_usage = psutil.virtual_memory()\n",
        "    return memory_usage.used / (1024**3) < threshold_gb\n",
        "\n",
        "# Perform testing and get prediction results\n",
        "all_results = []\n",
        "\n",
        "for test_set in test_sets:\n",
        "    gc.collect()\n",
        "\n",
        "    while not check_memory_threshold(memory_threshold_gb):\n",
        "        gc.collect()\n",
        "        print(\"Memory usage exceeds the threshold. Waiting...\")\n",
        "        time.sleep(10)  # Wait for 10 seconds before checking again\n",
        "    results = test(model, test_set)\n",
        "    all_results.extend(results)\n",
        "    print(all_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtHRYQ00fgGc",
        "outputId": "ace26396-4283-4a37-f4b1-b079c418f52a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_set_3.pkl\n",
            "test_set_1.pkl\n",
            "test_set_0.pkl\n",
            "test_set_2.pkl\n",
            "Trial ID: 008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-fe6f2c885511>:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  pred = torch.nn.Softmax()(pred)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('008',), 0, 1]]\n",
            "Trial ID: 001\n",
            "[[('008',), 0, 1], [('001',), 0, 0]]\n",
            "Trial ID: 001\n",
            "[[('008',), 0, 1], [('001',), 0, 0], [('001',), 0, 0]]\n",
            "Trial ID: 008\n",
            "[[('008',), 0, 1], [('001',), 0, 0], [('001',), 0, 0], [('008',), 0, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V4To8PXvfhlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extra code for TPU and other code don't need to see"
      ],
      "metadata": {
        "id": "VoglR2kN1bEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file = '/content/drive/MyDrive/ColabNotebooks/videos_imra/video_results.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Get the paths from the third column\n",
        "paths = df.iloc[:, 2]\n",
        "\n",
        "# Loop through the paths and count the files in each folder\n",
        "for index, path in enumerate(paths):\n",
        "    folder_path = os.path.join('/content/drive/MyDrive/ColabNotebooks/videos_imra', path)\n",
        "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
        "        file_count = len(os.listdir(folder_path))\n",
        "        print(f\"Folder: {folder_path}, File Count: {file_count}\")\n",
        "        # Update the value in the second column (at index 2)\n",
        "        df.iat[index, 1] = file_count\n",
        "    else:\n",
        "        print(f\"Folder does not exist: {folder_path}\")\n",
        "\n",
        "# Save the updated DataFrame to the CSV file\n",
        "df.to_csv(csv_file, index=False)"
      ],
      "metadata": {
        "id": "B6U46ynUDUmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## THis is the main code just save this fro later"
      ],
      "metadata": {
        "id": "tPP0B0BFODBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Function to train the model\n",
        "def run(model, train_set):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "\n",
        "    for epoch in range(0, 60):  # Changed to 60 epochs\n",
        "        print(\"epoch:\", epoch)\n",
        "        train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=8, pin_memory=True)\n",
        "        train_loss = AverageMeter()\n",
        "        train_acc = AverageMeter()\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            print(f\"Batch {batch_idx}\")\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss.update(loss.item(), batch_size)\n",
        "            _output = torch.nn.Softmax(dim=1)(output)\n",
        "            _, predicted = torch.max(_output.data, 1)\n",
        "            acc = (predicted == target).sum().item() / batch_size\n",
        "            train_acc.update(acc, batch_size)\n",
        "\n",
        "        print(f\"Epoch {epoch}: Train loss: {train_loss.avg:.4f}, Train acc: {train_acc.avg:.3f}\")\n",
        "\n",
        "    # Save the model\n",
        "    if train_set == train_sets[-1]:\n",
        "        model_file = os.path.join('/content/drive/MyDrive/ColabNotebooks/videos_imra/models', 'last_model.pth.tar')\n",
        "    else:\n",
        "        video_index = train_sets.index(train_set)\n",
        "        model_file = os.path.join('/content/drive/MyDrive/ColabNotebooks/videos_imra/models', f'model_{video_index}.pth.tar')\n",
        "\n",
        "    state = {\n",
        "        'epoch': epoch + 1,\n",
        "        'arch': 1,\n",
        "        'state_dict': model.state_dict(),\n",
        "    }\n",
        "    torch.save(state, model_file)\n",
        "    print(f\"Saved model to {model_file}\")\n",
        "\n",
        "# Load the train_set objects from pickle files\n",
        "pickle_dir = '/content/drive/MyDrive/ColabNotebooks/videos_imra/picklefile'\n",
        "\n",
        "pickle_files = [f for f in os.listdir(pickle_files_dir) if os.path.isfile(os.path.join(pickle_files_dir, f))]\n",
        "\n",
        "num_videos = len(pickle_files)  # Replace with the actual number of videos\n",
        "print(num_videos)\n",
        "train_sets = []\n",
        "for video_index in range(22,num_videos):\n",
        "    train_set_filename = f'{pickle_dir}/train_set_{video_index}.pkl'\n",
        "    with open(train_set_filename, 'rb') as file:\n",
        "        train_set = pickle.load(file)\n",
        "        train_sets.append(train_set)\n",
        "\n",
        "# Initialize your model (replace with your actual model)\n",
        "\n",
        "# Run the training process\n",
        "for train_set in train_sets:\n",
        "    run(model, train_set)\n"
      ],
      "metadata": {
        "id": "j9ZHjcgj5Hg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VutLd_xW5HkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hbpgmd-a5Hnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "\n",
        "def run(model,train_set):\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(train_set,\n",
        "                                                                      num_replicas = xm.xrt_world_size(),\n",
        "                                                                      rank         = xm.get_ordinal(),\n",
        "                                                                      shuffle      = True)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_set,\n",
        "                                                batch_size  = 64,\n",
        "                                                sampler     = train_sampler,\n",
        "                                                num_workers = 4,\n",
        "                                                pin_memory  = True)\n",
        "\n",
        "  mx    = xmp.MpModelWrapper(model)\n",
        "  device = xm.xla_device()\n",
        "  model  = mx.to(device)\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  #criterion = torch.nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=(0.00001*xm.xrt_world_size()))\n",
        "  para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "\n",
        "  for epoch in range(0, 120):\n",
        "          train_loss = AverageMeter()\n",
        "          train_acc = AverageMeter()\n",
        "          model.train()\n",
        "          for batch_idx, batch in enumerate(para_loader):\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              data, target = batch\n",
        "              batch_size = target.size(0)\n",
        "              data = data.to(device)\n",
        "              target = target.to(device)\n",
        "\n",
        "              output = model(data)\n",
        "              loss = criterion(output, target)\n",
        "              loss.backward()\n",
        "              xm.optimizer_step(optimizer, barrier = True)\n",
        "\n",
        "              train_loss.update(loss.item(), batch_size)\n",
        "              _output = torch.nn.Softmax(dim=1)(output)\n",
        "              #criterion = torch.nn.BCEWithLogitsLoss()\n",
        "              _, predicted = torch.max(_output.data, 1)\n",
        "              acc = (predicted == target).sum().item() / batch_size\n",
        "              train_acc.update(acc, batch_size)\n",
        "\n",
        "          if (epoch + 1) %  epoch == 120 - 1:  # eval\n",
        "              xm.master_print(\"Epoch {}: Train loss: {train_loss.avg:.4f} Train acc: {train_acc.avg:.3f} \".format(\n",
        "                  epoch, train_loss=train_loss, train_acc=train_acc))\n",
        "\n",
        "          if (epoch + 1) % 10 == 0 or epoch == 1999 - 1:  # save\n",
        "              name = \"model_\" + str(epoch)\n",
        "              model_file = os.path.join('/content/drive/MyDrive/ColabNotebooks/videos_imra/models', name + \".pth.tar\")\n",
        "              state = {'epoch': epoch + 1,\n",
        "                      'arch': 1,\n",
        "                      'state_dict': model.state_dict(),\n",
        "                      }\n",
        "              torch.save(state, model_file)\n",
        "              xm.master_print(\"Saved model to \" + model_file)"
      ],
      "metadata": {
        "id": "Hs_uMReaZOXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "MptIxFScZF2t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}